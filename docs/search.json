[
  {
    "objectID": "posts/finalproj/src/start_end_join.html",
    "href": "posts/finalproj/src/start_end_join.html",
    "title": "Data Gathering and Wrangling",
    "section": "",
    "text": "The purpose of this script is to define functions which convert data collected by the python script running on mapping.capital, which are returned as nested JSON files, to geoJSON which can be used for analysis. This file is set up for a dataset of all scooter locations, collected every 15 minutes, from 0600 Eastern to 1000 Eastern on May 1, 2022. The second function defined in this document creates a long dataframe which is a timeseries of scooter locations over time.\n\nvendors = list(\"link\",\"lime\",\"spin\")\n\nThis code block defines a method to convert the nested JSON returned by scooter vendors to a non-nested geoJSON which can be read by sf.\n\njson2geoJSON &lt;- function(vendor){\n  files &lt;- list.files(path = paste(\"../data/monday/\", vendor, \"/morning/json/\", sep = \"\"), pattern=\"*.json\", full.names=TRUE, recursive=FALSE) #list files in directory\n    lapply(files, function(x) {\n      current_data &lt;- fromJSON(txt = x) # load file\n      current_tibble &lt;- as_tibble(current_data$data$bikes) #convert to tibble\n      current_tibble$timestamp_utc &lt;- as_datetime(current_data$last_updated, tz = Sys.timezone()) #create timestamp column\n      current_sf &lt;- st_as_sf(current_tibble, coords = c(\"lon\",\"lat\"), crs = 4326) #coerce to sf\n      if (!file.exists(paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\",\n                                       current_data$last_updated, \"_\", vendor, \".geoJSON\"))){\n      st_write(current_sf, dsn = paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\",\n                                       current_data$last_updated, \"_\", vendor, \".geoJSON\", sep = \"\")\n               , append = FALSE) #write as geoJSON\n      }\n})\n}\n\n\nfor (v in vendors){\n  json2geoJSON(v)\n} #loop through each of link, lime, spin\n\nThis section of code defines a function which creates a timeseries for each scooter and adds a vendor column which can be grouped by in following scripts.\n\n\n\n\n\n\nImportant\n\n\n\nNote 2024-10-11: I cannot emphasize enough how much you SHOULD NOT USE GLOBAL ASSIGNMENT (&lt;&lt;-) in a function. This was some of the first R code I‚Äôd ever written, and I‚Äôm leaving it for posterity. However, there are MUCH better ways to do this.\n\n\n\nload_timeseries &lt;- function(vendor){\n  files &lt;- list.files(path = paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\", sep = \"\"), pattern=\"*.geoJSON\", full.names=TRUE, recursive=FALSE) #load files from geoJSON directory\n  list_df &lt;&lt;- vector(mode = \"list\") #empty list\n  for(fn in files){\n    tmp &lt;- st_read(fn) #read each file in geoJSON dir\n    list_df[[which(fn == files)]] &lt;&lt;- tmp #append to list_df\n  }\n  test_sf &lt;&lt;- bind_rows(list_df) #make long df\n  test_sf$vendor &lt;&lt;- vendor #create vendor column\n  test_sf &lt;&lt;- distinct(test_sf) #script adds multiples, need to debug. hacky solution here\n}\n\n\nload_timeseries(\"link\")\nlink_data &lt;- test_sf\nif (!file.exists(\"../results/link_mon_am.gpkg\")){\n  st_write(link_data, dsn = paste0(\"../results/link_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}\n\n\nload_timeseries(\"lime\")\nlime_data &lt;- test_sf\nif (!file.exists(\"../results/lime_mon_am.gpkg\")){\n  st_write(lime_data, dsn = paste0(\"../results/lime_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}\n\n\nload_timeseries(\"spin\")\nspin_data &lt;- test_sf\nif (!file.exists(\"../results/spin_mon_am.gpkg\")){\n  st_write(spin_data, dsn = paste0(\"../results/spin_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}"
  },
  {
    "objectID": "posts/finalproj/src/start_end_join.html#setup",
    "href": "posts/finalproj/src/start_end_join.html#setup",
    "title": "Data Gathering and Wrangling",
    "section": "",
    "text": "The purpose of this script is to define functions which convert data collected by the python script running on mapping.capital, which are returned as nested JSON files, to geoJSON which can be used for analysis. This file is set up for a dataset of all scooter locations, collected every 15 minutes, from 0600 Eastern to 1000 Eastern on May 1, 2022. The second function defined in this document creates a long dataframe which is a timeseries of scooter locations over time.\n\nvendors = list(\"link\",\"lime\",\"spin\")\n\nThis code block defines a method to convert the nested JSON returned by scooter vendors to a non-nested geoJSON which can be read by sf.\n\njson2geoJSON &lt;- function(vendor){\n  files &lt;- list.files(path = paste(\"../data/monday/\", vendor, \"/morning/json/\", sep = \"\"), pattern=\"*.json\", full.names=TRUE, recursive=FALSE) #list files in directory\n    lapply(files, function(x) {\n      current_data &lt;- fromJSON(txt = x) # load file\n      current_tibble &lt;- as_tibble(current_data$data$bikes) #convert to tibble\n      current_tibble$timestamp_utc &lt;- as_datetime(current_data$last_updated, tz = Sys.timezone()) #create timestamp column\n      current_sf &lt;- st_as_sf(current_tibble, coords = c(\"lon\",\"lat\"), crs = 4326) #coerce to sf\n      if (!file.exists(paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\",\n                                       current_data$last_updated, \"_\", vendor, \".geoJSON\"))){\n      st_write(current_sf, dsn = paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\",\n                                       current_data$last_updated, \"_\", vendor, \".geoJSON\", sep = \"\")\n               , append = FALSE) #write as geoJSON\n      }\n})\n}\n\n\nfor (v in vendors){\n  json2geoJSON(v)\n} #loop through each of link, lime, spin\n\nThis section of code defines a function which creates a timeseries for each scooter and adds a vendor column which can be grouped by in following scripts.\n\n\n\n\n\n\nImportant\n\n\n\nNote 2024-10-11: I cannot emphasize enough how much you SHOULD NOT USE GLOBAL ASSIGNMENT (&lt;&lt;-) in a function. This was some of the first R code I‚Äôd ever written, and I‚Äôm leaving it for posterity. However, there are MUCH better ways to do this.\n\n\n\nload_timeseries &lt;- function(vendor){\n  files &lt;- list.files(path = paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\", sep = \"\"), pattern=\"*.geoJSON\", full.names=TRUE, recursive=FALSE) #load files from geoJSON directory\n  list_df &lt;&lt;- vector(mode = \"list\") #empty list\n  for(fn in files){\n    tmp &lt;- st_read(fn) #read each file in geoJSON dir\n    list_df[[which(fn == files)]] &lt;&lt;- tmp #append to list_df\n  }\n  test_sf &lt;&lt;- bind_rows(list_df) #make long df\n  test_sf$vendor &lt;&lt;- vendor #create vendor column\n  test_sf &lt;&lt;- distinct(test_sf) #script adds multiples, need to debug. hacky solution here\n}\n\n\nload_timeseries(\"link\")\nlink_data &lt;- test_sf\nif (!file.exists(\"../results/link_mon_am.gpkg\")){\n  st_write(link_data, dsn = paste0(\"../results/link_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}\n\n\nload_timeseries(\"lime\")\nlime_data &lt;- test_sf\nif (!file.exists(\"../results/lime_mon_am.gpkg\")){\n  st_write(lime_data, dsn = paste0(\"../results/lime_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}\n\n\nload_timeseries(\"spin\")\nspin_data &lt;- test_sf\nif (!file.exists(\"../results/spin_mon_am.gpkg\")){\n  st_write(spin_data, dsn = paste0(\"../results/spin_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}"
  },
  {
    "objectID": "posts/finalproj/src/scooter_analysis.html",
    "href": "posts/finalproj/src/scooter_analysis.html",
    "title": "scooter analysis",
    "section": "",
    "text": "This script relies on 3 geopackage outputs from the previous script. It creates a long dataframe, inclusive of all three vendors, groups them by permanent, unique bike ID, and defines a trip as a movement of 50 meters between timestamps, in an attempt to adjust for GPS variability.\n\nlink_data &lt;- st_read(\"../results/link_mon_am.gpkg\")\n\nReading layer `link_mon_am' from data source \n  `D:\\website\\oonuttlet.github.io\\posts\\finalproj\\results\\link_mon_am.gpkg' \n  using driver `GPKG'\nSimple feature collection with 71931 features and 7 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -79.7663 ymin: 33.87113 xmax: -75.11482 ymax: 40.16036\nGeodetic CRS:  WGS 84\n\nlime_data &lt;- st_read(\"../results/spin_mon_am.gpkg\")\n\nReading layer `spin_mon_am' from data source \n  `D:\\website\\oonuttlet.github.io\\posts\\finalproj\\results\\spin_mon_am.gpkg' \n  using driver `GPKG'\nSimple feature collection with 16584 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -76.71196 ymin: 39.22984 xmax: -76.53002 ymax: 39.37177\nGeodetic CRS:  WGS 84\n\nspin_data &lt;- st_read(\"../results/spin_mon_am.gpkg\")\n\nReading layer `spin_mon_am' from data source \n  `D:\\website\\oonuttlet.github.io\\posts\\finalproj\\results\\spin_mon_am.gpkg' \n  using driver `GPKG'\nSimple feature collection with 16584 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -76.71196 ymin: 39.22984 xmax: -76.53002 ymax: 39.37177\nGeodetic CRS:  WGS 84\n\nscooters_raw = bind_rows(link_data, lime_data, spin_data) %&gt;%\n  st_transform(crs = 3857)\nif(!file.exists(\"../results/scooters_raw.gpkg\")){\n  st_write(scooters_raw, dsn = \"../results/scooters_raw.gpkg\")\n}\n\nFirst, we‚Äôll filter out the disabled bike observations, and split the bikes into their own lists in order to apply a function over each bike.\n\nscooters_split &lt;- scooters_raw %&gt;%\n  distinct() %&gt;%\n  filter(is_disabled == 0) %&gt;%\n  group_by(vendor, bike_id) %&gt;%\n  group_split()\n\nThis function defines what constitutes a trip. It is slow. When it‚Äôs done, each list element, representing a scooter, will have a field delineating if a given time-interval was part of a trip.\n\nscooters_trip &lt;- lapply(scooters_split, function(df){\n  df %&gt;% mutate(dist_prev = units::drop_units(st_distance(geom, lag(geom), by_element = TRUE)),\n                dist_next = units::drop_units(st_distance(geom, lead(geom), by_element = TRUE)),\n                time_id = row_number(), #this is what allows us to order points for QGIS analysis\n                movement_id = paste(bike_id, \"_\", row_number(), sep = \"\"), #perhaps redundant, but easy solution for moving between R and QGIS\n                trip = case_when(\n                  dist_prev &gt; 50 | dist_next &gt; 50 ~ 1, #define trip based on distance column\n                  TRUE ~ 0))\n})\n\nNote that this is some of the earliest R code I‚Äôd ever written. I‚Äôm leaving it alone for posterity‚Äôs sake, but I‚Äôm not sure why I split it into groups and re-bound it again.\n\ntrip_long &lt;- bind_rows(scooters_trip) %&gt;%\n  filter(trip == 1) #filter by only trip points\ntrip_split &lt;- trip_long %&gt;% #split again by trips\n  group_by(bike_id) %&gt;%\n  group_split()\n\n\n#trip_split_id &lt;- lapply(trip_split, function(df){\n#  df %&gt;% mutate(time_id = row_number())\n#})\ntrip_id_long &lt;-bind_rows(trip_split) #bind into trips\nif(!file.exists(\"../results/trip_id_long.gpkg\")){\n  st_write(trip_id_long, \"../results/trip_id_long.gpkg\", append = FALSE)\n}"
  },
  {
    "objectID": "posts/finalproj/src/scooter_analysis.html#setup",
    "href": "posts/finalproj/src/scooter_analysis.html#setup",
    "title": "scooter analysis",
    "section": "",
    "text": "This script relies on 3 geopackage outputs from the previous script. It creates a long dataframe, inclusive of all three vendors, groups them by permanent, unique bike ID, and defines a trip as a movement of 50 meters between timestamps, in an attempt to adjust for GPS variability.\n\nlink_data &lt;- st_read(\"../results/link_mon_am.gpkg\")\n\nReading layer `link_mon_am' from data source \n  `D:\\website\\oonuttlet.github.io\\posts\\finalproj\\results\\link_mon_am.gpkg' \n  using driver `GPKG'\nSimple feature collection with 71931 features and 7 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -79.7663 ymin: 33.87113 xmax: -75.11482 ymax: 40.16036\nGeodetic CRS:  WGS 84\n\nlime_data &lt;- st_read(\"../results/spin_mon_am.gpkg\")\n\nReading layer `spin_mon_am' from data source \n  `D:\\website\\oonuttlet.github.io\\posts\\finalproj\\results\\spin_mon_am.gpkg' \n  using driver `GPKG'\nSimple feature collection with 16584 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -76.71196 ymin: 39.22984 xmax: -76.53002 ymax: 39.37177\nGeodetic CRS:  WGS 84\n\nspin_data &lt;- st_read(\"../results/spin_mon_am.gpkg\")\n\nReading layer `spin_mon_am' from data source \n  `D:\\website\\oonuttlet.github.io\\posts\\finalproj\\results\\spin_mon_am.gpkg' \n  using driver `GPKG'\nSimple feature collection with 16584 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -76.71196 ymin: 39.22984 xmax: -76.53002 ymax: 39.37177\nGeodetic CRS:  WGS 84\n\nscooters_raw = bind_rows(link_data, lime_data, spin_data) %&gt;%\n  st_transform(crs = 3857)\nif(!file.exists(\"../results/scooters_raw.gpkg\")){\n  st_write(scooters_raw, dsn = \"../results/scooters_raw.gpkg\")\n}\n\nFirst, we‚Äôll filter out the disabled bike observations, and split the bikes into their own lists in order to apply a function over each bike.\n\nscooters_split &lt;- scooters_raw %&gt;%\n  distinct() %&gt;%\n  filter(is_disabled == 0) %&gt;%\n  group_by(vendor, bike_id) %&gt;%\n  group_split()\n\nThis function defines what constitutes a trip. It is slow. When it‚Äôs done, each list element, representing a scooter, will have a field delineating if a given time-interval was part of a trip.\n\nscooters_trip &lt;- lapply(scooters_split, function(df){\n  df %&gt;% mutate(dist_prev = units::drop_units(st_distance(geom, lag(geom), by_element = TRUE)),\n                dist_next = units::drop_units(st_distance(geom, lead(geom), by_element = TRUE)),\n                time_id = row_number(), #this is what allows us to order points for QGIS analysis\n                movement_id = paste(bike_id, \"_\", row_number(), sep = \"\"), #perhaps redundant, but easy solution for moving between R and QGIS\n                trip = case_when(\n                  dist_prev &gt; 50 | dist_next &gt; 50 ~ 1, #define trip based on distance column\n                  TRUE ~ 0))\n})\n\nNote that this is some of the earliest R code I‚Äôd ever written. I‚Äôm leaving it alone for posterity‚Äôs sake, but I‚Äôm not sure why I split it into groups and re-bound it again.\n\ntrip_long &lt;- bind_rows(scooters_trip) %&gt;%\n  filter(trip == 1) #filter by only trip points\ntrip_split &lt;- trip_long %&gt;% #split again by trips\n  group_by(bike_id) %&gt;%\n  group_split()\n\n\n#trip_split_id &lt;- lapply(trip_split, function(df){\n#  df %&gt;% mutate(time_id = row_number())\n#})\ntrip_id_long &lt;-bind_rows(trip_split) #bind into trips\nif(!file.exists(\"../results/trip_id_long.gpkg\")){\n  st_write(trip_id_long, \"../results/trip_id_long.gpkg\", append = FALSE)\n}"
  },
  {
    "objectID": "posts/finalproj/src/hexagons.html",
    "href": "posts/finalproj/src/hexagons.html",
    "title": "Flow Mapping",
    "section": "",
    "text": "This script uses outputs from the previous 3 Rmd files to create a flow map based on the mapdeck package, and bins start and end points by hexes. This workflow is not currently automated for other datasets, but this is a minimal working example to show that it can be expanded to other metro areas.\n\nflow_lines &lt;- readRDS(\"G:/My Drive/GES486/final_proj/results/flow_lines.RDS\") %&gt;%\n  st_transform(crs = 3857)\nbaltimore_bound = counties(state = \"MD\", cb = TRUE) %&gt;%\n  filter(str_detect(GEOID, \"24510|24005\")) %&gt;%\n  st_transform(crs = 3857)\n\n\nbalt_hex &lt;- st_make_grid(baltimore_bound, #create 2000m hex grid\n                         cellsize = c(1600,1600),\n                         what = \"polygons\",\n                         square = FALSE,\n                         crs = 3857\n                         ) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(hex_id = paste(\"hex_\", row_number(), sep = \"\"))\nbalt_hex.intersects &lt;- st_intersects(st_union(baltimore_bound), balt_hex)\nbalt_hex.subset &lt;- balt_hex[balt_hex.intersects[[1]],]\nbalt_centroid &lt;- st_centroid(balt_hex.subset) %&gt;% st_transform(4326) #dataframe of centroids of each hex\nbaltimore_bound &lt;- baltimore_bound %&gt;% st_transform(4326)\ntm_shape(baltimore_bound)+\n  tm_polygons(col = \"#bdbdbd\", border.col = \"black\", lwd = 2)+\n  tm_shape(balt_hex)+\n  tm_borders()\n\n\n\n\n\n\n\n\nThis next block of code is a mess, but I couldn‚Äôt get dplyr and sf to cooperate with some of these dataframes, which contained multiple geometry columns. This can be cleaned up (and probably will be), but for now, this works for the data that I have.\n\nbalt_hex &lt;- balt_hex %&gt;% st_transform(4326)\nbalt_hex.subset &lt;- balt_hex.subset %&gt;% st_transform(4326)\nflow_lines_sf &lt;- st_as_sf(flow_lines)\nflow_lines_sf &lt;- st_transform(flow_lines_sf, crs = 4326)\nflow_lines_sf &lt;- st_set_geometry(flow_lines_sf, flow_lines_sf$start_geom) %&gt;% st_transform(4326)\nstart_points_join &lt;- st_join(flow_lines_sf, balt_hex.subset) #spatial join of start points (set start points as active geometry in previous step)\nflow_lines_sf &lt;- st_set_geometry(flow_lines_sf, flow_lines_sf$end_geom) %&gt;% st_transform(4326)\nend_points_join &lt;- st_join(flow_lines_sf, balt_hex.subset) #in similar fashion, spatial join of end points\nflow_lines_sf$start_hex_id &lt;- start_points_join$hex_id\nflow_lines_sf$end_hex_id &lt;- end_points_join$hex_id\nflow_lines_sf &lt;- left_join(st_drop_geometry(flow_lines_sf), balt_centroid, by = c(\"start_hex_id\" = \"hex_id\")) #join centroid of endpoint hex to row\nflow_lines_sf &lt;- rename(flow_lines_sf, start_centroid = x)\nflow_lines_sf &lt;- left_join(flow_lines_sf, balt_centroid, by = c(\"end_hex_id\" = \"hex_id\")) #likewise for end\nflow_lines_sf &lt;- rename(flow_lines_sf, end_centroid = x)\nflow_lines_sf &lt;- st_sf(flow_lines_sf) %&gt;% st_transform(4326)\nflow_lines_arc &lt;- rename(count(flow_lines_sf, start_hex_id, end_hex_id), wgt = n)\nflow_lines_arc &lt;- flow_lines_arc %&gt;% mutate(scale_weight = (3*wgt))\nflow_lines_arc &lt;- left_join(st_drop_geometry(flow_lines_arc), balt_centroid, by = c(\"end_hex_id\" = \"hex_id\"))\nflow_lines_arc &lt;- left_join(flow_lines_arc, balt_centroid, by = c(\"start_hex_id\" = \"hex_id\"))\nflow_lines_arc &lt;- flow_lines_arc %&gt;% rename(end_centroid = x.x, start_centroid = x.y) %&gt;% st_sf()\nflow_lines_arc &lt;- flow_lines_arc %&gt;% st_transform(4326)\n\nThe following methodology, to count start and end points within hexes, is adapted from Matt Herman‚Äôs blog post detailing the counting of trees within NYC census geographies.\n\nstart_hex_count &lt;- count(as_tibble(start_points_join), hex_id) %&gt;% rename(start_in_hex = n)\nend_hex_count &lt;- count(as_tibble(end_points_join), hex_id) %&gt;% rename(end_in_hex = n)\nbalt_hex.subset &lt;- left_join(balt_hex.subset, start_hex_count, by = c(\"hex_id\" = \"hex_id\"))\nbalt_hex.subset &lt;- left_join(balt_hex.subset, end_hex_count, by = c(\"hex_id\" = \"hex_id\"))\nbalt_hex.subset &lt;- balt_hex.subset %&gt;% replace(is.na(.), 0)\nbalt_hex.subset &lt;- balt_hex.subset %&gt;% mutate(total_endpoint = start_in_hex + end_in_hex) \nbalt_hex.subset &lt;- balt_hex.subset %&gt;% st_transform(4326)\n\nThree flow maps can be generated from this data: one symbolized with start points per hex, one with end points per hex, and one with total endpoints per hex.\n\nflow_lines_arc %&gt;%\n  mapdeck(token = Sys.getenv(\"MAPBOX_TOKEN\")) %&gt;%\n  add_arc(origin = \"start_centroid\",\n          destination = \"end_centroid\",\n          stroke_from = \"#000000\",\n          stroke_to = \"#000000\",\n          stroke_width = \"scale_weight\",\n          update_view = TRUE) %&gt;%\n  add_sf(data = balt_hex.subset,\n         fill_colour = \"start_in_hex\",\n         fill_opacity = 180,\n         legend = TRUE\n         )\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\n\n\n\n\n\nflow_lines_arc %&gt;%\n  mapdeck(token = Sys.getenv(\"MAPBOX_TOKEN\")) %&gt;%\n  add_arc(origin = \"start_centroid\",\n          destination = \"end_centroid\",\n          stroke_from = \"#000000\",\n          stroke_to = \"#000000\",\n          stroke_width = \"scale_weight\",\n          update_view = TRUE) %&gt;%\n  add_sf(data = balt_hex.subset,\n         fill_colour = \"end_in_hex\",\n         fill_opacity = 180,\n         legend = TRUE\n         )\n\n\n\n\n\n\nflow_lines_arc %&gt;%\n  mapdeck(token = Sys.getenv(\"MAPBOX_TOKEN\")) %&gt;%\n  add_arc(origin = \"start_centroid\",\n          destination = \"end_centroid\",\n          stroke_from = \"#000000\",\n          stroke_to = \"#000000\",\n          stroke_width = \"scale_weight\",\n          update_view = TRUE) %&gt;%\n  add_sf(data = balt_hex.subset,\n         fill_colour = \"total_endpoint\",\n         fill_opacity = 180,\n         legend = TRUE\n         )\n\n\n\n\n\n\nif(!file.exists(\"../results/flow_lines_arc.RDS\")){\n  saveRDS(flow_lines_arc, file = \"../results/flow_lines_arc.RDS\")\n}\nif(!file.exists(\"../results/balt_hex.RDS\")){\n  saveRDS(balt_hex, file = \"../results/balt_hex.RDS\")\n}"
  },
  {
    "objectID": "posts/finalproj/src/hexagons.html#setup",
    "href": "posts/finalproj/src/hexagons.html#setup",
    "title": "Flow Mapping",
    "section": "",
    "text": "This script uses outputs from the previous 3 Rmd files to create a flow map based on the mapdeck package, and bins start and end points by hexes. This workflow is not currently automated for other datasets, but this is a minimal working example to show that it can be expanded to other metro areas.\n\nflow_lines &lt;- readRDS(\"G:/My Drive/GES486/final_proj/results/flow_lines.RDS\") %&gt;%\n  st_transform(crs = 3857)\nbaltimore_bound = counties(state = \"MD\", cb = TRUE) %&gt;%\n  filter(str_detect(GEOID, \"24510|24005\")) %&gt;%\n  st_transform(crs = 3857)\n\n\nbalt_hex &lt;- st_make_grid(baltimore_bound, #create 2000m hex grid\n                         cellsize = c(1600,1600),\n                         what = \"polygons\",\n                         square = FALSE,\n                         crs = 3857\n                         ) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(hex_id = paste(\"hex_\", row_number(), sep = \"\"))\nbalt_hex.intersects &lt;- st_intersects(st_union(baltimore_bound), balt_hex)\nbalt_hex.subset &lt;- balt_hex[balt_hex.intersects[[1]],]\nbalt_centroid &lt;- st_centroid(balt_hex.subset) %&gt;% st_transform(4326) #dataframe of centroids of each hex\nbaltimore_bound &lt;- baltimore_bound %&gt;% st_transform(4326)\ntm_shape(baltimore_bound)+\n  tm_polygons(col = \"#bdbdbd\", border.col = \"black\", lwd = 2)+\n  tm_shape(balt_hex)+\n  tm_borders()\n\n\n\n\n\n\n\n\nThis next block of code is a mess, but I couldn‚Äôt get dplyr and sf to cooperate with some of these dataframes, which contained multiple geometry columns. This can be cleaned up (and probably will be), but for now, this works for the data that I have.\n\nbalt_hex &lt;- balt_hex %&gt;% st_transform(4326)\nbalt_hex.subset &lt;- balt_hex.subset %&gt;% st_transform(4326)\nflow_lines_sf &lt;- st_as_sf(flow_lines)\nflow_lines_sf &lt;- st_transform(flow_lines_sf, crs = 4326)\nflow_lines_sf &lt;- st_set_geometry(flow_lines_sf, flow_lines_sf$start_geom) %&gt;% st_transform(4326)\nstart_points_join &lt;- st_join(flow_lines_sf, balt_hex.subset) #spatial join of start points (set start points as active geometry in previous step)\nflow_lines_sf &lt;- st_set_geometry(flow_lines_sf, flow_lines_sf$end_geom) %&gt;% st_transform(4326)\nend_points_join &lt;- st_join(flow_lines_sf, balt_hex.subset) #in similar fashion, spatial join of end points\nflow_lines_sf$start_hex_id &lt;- start_points_join$hex_id\nflow_lines_sf$end_hex_id &lt;- end_points_join$hex_id\nflow_lines_sf &lt;- left_join(st_drop_geometry(flow_lines_sf), balt_centroid, by = c(\"start_hex_id\" = \"hex_id\")) #join centroid of endpoint hex to row\nflow_lines_sf &lt;- rename(flow_lines_sf, start_centroid = x)\nflow_lines_sf &lt;- left_join(flow_lines_sf, balt_centroid, by = c(\"end_hex_id\" = \"hex_id\")) #likewise for end\nflow_lines_sf &lt;- rename(flow_lines_sf, end_centroid = x)\nflow_lines_sf &lt;- st_sf(flow_lines_sf) %&gt;% st_transform(4326)\nflow_lines_arc &lt;- rename(count(flow_lines_sf, start_hex_id, end_hex_id), wgt = n)\nflow_lines_arc &lt;- flow_lines_arc %&gt;% mutate(scale_weight = (3*wgt))\nflow_lines_arc &lt;- left_join(st_drop_geometry(flow_lines_arc), balt_centroid, by = c(\"end_hex_id\" = \"hex_id\"))\nflow_lines_arc &lt;- left_join(flow_lines_arc, balt_centroid, by = c(\"start_hex_id\" = \"hex_id\"))\nflow_lines_arc &lt;- flow_lines_arc %&gt;% rename(end_centroid = x.x, start_centroid = x.y) %&gt;% st_sf()\nflow_lines_arc &lt;- flow_lines_arc %&gt;% st_transform(4326)\n\nThe following methodology, to count start and end points within hexes, is adapted from Matt Herman‚Äôs blog post detailing the counting of trees within NYC census geographies.\n\nstart_hex_count &lt;- count(as_tibble(start_points_join), hex_id) %&gt;% rename(start_in_hex = n)\nend_hex_count &lt;- count(as_tibble(end_points_join), hex_id) %&gt;% rename(end_in_hex = n)\nbalt_hex.subset &lt;- left_join(balt_hex.subset, start_hex_count, by = c(\"hex_id\" = \"hex_id\"))\nbalt_hex.subset &lt;- left_join(balt_hex.subset, end_hex_count, by = c(\"hex_id\" = \"hex_id\"))\nbalt_hex.subset &lt;- balt_hex.subset %&gt;% replace(is.na(.), 0)\nbalt_hex.subset &lt;- balt_hex.subset %&gt;% mutate(total_endpoint = start_in_hex + end_in_hex) \nbalt_hex.subset &lt;- balt_hex.subset %&gt;% st_transform(4326)\n\nThree flow maps can be generated from this data: one symbolized with start points per hex, one with end points per hex, and one with total endpoints per hex.\n\nflow_lines_arc %&gt;%\n  mapdeck(token = Sys.getenv(\"MAPBOX_TOKEN\")) %&gt;%\n  add_arc(origin = \"start_centroid\",\n          destination = \"end_centroid\",\n          stroke_from = \"#000000\",\n          stroke_to = \"#000000\",\n          stroke_width = \"scale_weight\",\n          update_view = TRUE) %&gt;%\n  add_sf(data = balt_hex.subset,\n         fill_colour = \"start_in_hex\",\n         fill_opacity = 180,\n         legend = TRUE\n         )\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\n\n\n\n\n\nflow_lines_arc %&gt;%\n  mapdeck(token = Sys.getenv(\"MAPBOX_TOKEN\")) %&gt;%\n  add_arc(origin = \"start_centroid\",\n          destination = \"end_centroid\",\n          stroke_from = \"#000000\",\n          stroke_to = \"#000000\",\n          stroke_width = \"scale_weight\",\n          update_view = TRUE) %&gt;%\n  add_sf(data = balt_hex.subset,\n         fill_colour = \"end_in_hex\",\n         fill_opacity = 180,\n         legend = TRUE\n         )\n\n\n\n\n\n\nflow_lines_arc %&gt;%\n  mapdeck(token = Sys.getenv(\"MAPBOX_TOKEN\")) %&gt;%\n  add_arc(origin = \"start_centroid\",\n          destination = \"end_centroid\",\n          stroke_from = \"#000000\",\n          stroke_to = \"#000000\",\n          stroke_width = \"scale_weight\",\n          update_view = TRUE) %&gt;%\n  add_sf(data = balt_hex.subset,\n         fill_colour = \"total_endpoint\",\n         fill_opacity = 180,\n         legend = TRUE\n         )\n\n\n\n\n\n\nif(!file.exists(\"../results/flow_lines_arc.RDS\")){\n  saveRDS(flow_lines_arc, file = \"../results/flow_lines_arc.RDS\")\n}\nif(!file.exists(\"../results/balt_hex.RDS\")){\n  saveRDS(balt_hex, file = \"../results/balt_hex.RDS\")\n}"
  },
  {
    "objectID": "posts/finalproj/index.html",
    "href": "posts/finalproj/index.html",
    "title": "Micromobility in Baltimore",
    "section": "",
    "text": "In 2019, Baltimore City officially adopted its Dockless Vehicle Program, granting permits to micromobility companies Link, Lime, and Spin. The goal of this program was to supplement existing public transit networks, to provide a sustainable alternative for small-scale commuting, and, according to the Baltimore DOT, to be ridden just ‚Äúfor fun!‚Äù\nAs the ‚Äòdockless‚Äô name implies, these scooters have nowhere to call home: they‚Äôre placed down by local employees of the scooter vendor, remain out for up to weeks at a time, then recollected for maintenance and recharging before being placed out again. Baltimore City lays out clear Deployment Zones and Deployment Equity Zones, in which a certain amount of scooters must remain for the vendors to continue operation in the city.\nThese zones are relatively small compared to the size of the city boundary: do they make scooter distribution truly equitable? Using the vendors‚Äô public API endpoints (another requirement for operation within the city), data was (and is being) collected on scooter locations every fifteen minutes. For this project, I only analyzed times between 6:00 am and 10:00 am for the week of May 1, 2022 and May 7, 2022; but the potential is there for much more detailed analysis.\nThe python script which queries the API endpoints for all three vendors is currently running on Mapping Capital, and all other analysis was done using R statistical software. The scripts I used to manipulate the data collected can be found here.\n\n\n\nFor this analysis, I‚Äôve coined a new unit: people-points. Represented by the total number of jobs as represented in LEHD data added with the total population in a given area, people-points can be used to find centers of transport and human activity as people commute to and from work and home. I‚Äôll be symbolizing my maps based on the number of scooters per person-point (in this case, per 1,000 people-points for better scaling) in order to pick out locations where the number of scooters is not proportional to the number of jobs and residents in an area.\nHere are the resulting maps, made in ggplot2 and arranged using patchwork:\n \nWe can see that, during a week of morning commuting (Monday - Monday), the areas with the most scooter trips per person-point are in Locust Point and the Inner Harbor. More specifically, the two yellowest polygons, which represent the hexes with the highest proportion of trips per person-point, contain the Under Armour main campus. Notably, areas with high percentages of BIPOC individuals are almost all dark purple, showing few numbers of scooter trips per person-point.\nAccording to Spin, they are ‚Äúcommitted to being the best possible partner for cities while building the safest, most equitable, and most sustainable mobility solution for the communities [they] serve‚Äù (emphasis added). I‚Äôd say that, whether intentional or not, the stark contrast of micromobility in marginalized communities compared to whiter communities demonstrates a veritable lack of equitable access, at least in Baltimore. Whether the vehicles are purposely dropped in whiter areas after charging, if they‚Äôre used to commute from these areas to Downtown but not back, or any other reason, calling the distribution of scooter trips and availability ‚Äúequitable‚Äù is downright laughable as things stand at the moment.\n\n\n\nAn important reason that cities (including Baltimore) adopt these platforms is to supplement existing transit networks by providing a method for an individual to quickly get to a transit stop before switching to the bus, train, or other method of transit. One interesting area of research in this regard could be to examine how many scooter trips have start and end points that mirror existing transit routes. Could people be using scooters instead of public transit? If so, why? And, as always, more research could be carried out with more data. Since the script collecting our data runs every 15 minutes until the server runs out of storage or the inevitable heat death of the universe (whichever comes first), we could theoretically run this same analysis for time periods of months or even years, given powerful enough hardware."
  },
  {
    "objectID": "posts/finalproj/index.html#introduction",
    "href": "posts/finalproj/index.html#introduction",
    "title": "Micromobility in Baltimore",
    "section": "",
    "text": "In 2019, Baltimore City officially adopted its Dockless Vehicle Program, granting permits to micromobility companies Link, Lime, and Spin. The goal of this program was to supplement existing public transit networks, to provide a sustainable alternative for small-scale commuting, and, according to the Baltimore DOT, to be ridden just ‚Äúfor fun!‚Äù\nAs the ‚Äòdockless‚Äô name implies, these scooters have nowhere to call home: they‚Äôre placed down by local employees of the scooter vendor, remain out for up to weeks at a time, then recollected for maintenance and recharging before being placed out again. Baltimore City lays out clear Deployment Zones and Deployment Equity Zones, in which a certain amount of scooters must remain for the vendors to continue operation in the city.\nThese zones are relatively small compared to the size of the city boundary: do they make scooter distribution truly equitable? Using the vendors‚Äô public API endpoints (another requirement for operation within the city), data was (and is being) collected on scooter locations every fifteen minutes. For this project, I only analyzed times between 6:00 am and 10:00 am for the week of May 1, 2022 and May 7, 2022; but the potential is there for much more detailed analysis.\nThe python script which queries the API endpoints for all three vendors is currently running on Mapping Capital, and all other analysis was done using R statistical software. The scripts I used to manipulate the data collected can be found here."
  },
  {
    "objectID": "posts/finalproj/index.html#results",
    "href": "posts/finalproj/index.html#results",
    "title": "Micromobility in Baltimore",
    "section": "",
    "text": "For this analysis, I‚Äôve coined a new unit: people-points. Represented by the total number of jobs as represented in LEHD data added with the total population in a given area, people-points can be used to find centers of transport and human activity as people commute to and from work and home. I‚Äôll be symbolizing my maps based on the number of scooters per person-point (in this case, per 1,000 people-points for better scaling) in order to pick out locations where the number of scooters is not proportional to the number of jobs and residents in an area.\nHere are the resulting maps, made in ggplot2 and arranged using patchwork:\n \nWe can see that, during a week of morning commuting (Monday - Monday), the areas with the most scooter trips per person-point are in Locust Point and the Inner Harbor. More specifically, the two yellowest polygons, which represent the hexes with the highest proportion of trips per person-point, contain the Under Armour main campus. Notably, areas with high percentages of BIPOC individuals are almost all dark purple, showing few numbers of scooter trips per person-point.\nAccording to Spin, they are ‚Äúcommitted to being the best possible partner for cities while building the safest, most equitable, and most sustainable mobility solution for the communities [they] serve‚Äù (emphasis added). I‚Äôd say that, whether intentional or not, the stark contrast of micromobility in marginalized communities compared to whiter communities demonstrates a veritable lack of equitable access, at least in Baltimore. Whether the vehicles are purposely dropped in whiter areas after charging, if they‚Äôre used to commute from these areas to Downtown but not back, or any other reason, calling the distribution of scooter trips and availability ‚Äúequitable‚Äù is downright laughable as things stand at the moment."
  },
  {
    "objectID": "posts/finalproj/index.html#further-research",
    "href": "posts/finalproj/index.html#further-research",
    "title": "Micromobility in Baltimore",
    "section": "",
    "text": "An important reason that cities (including Baltimore) adopt these platforms is to supplement existing transit networks by providing a method for an individual to quickly get to a transit stop before switching to the bus, train, or other method of transit. One interesting area of research in this regard could be to examine how many scooter trips have start and end points that mirror existing transit routes. Could people be using scooters instead of public transit? If so, why? And, as always, more research could be carried out with more data. Since the script collecting our data runs every 15 minutes until the server runs out of storage or the inevitable heat death of the universe (whichever comes first), we could theoretically run this same analysis for time periods of months or even years, given powerful enough hardware."
  },
  {
    "objectID": "posts/381_proj/src/ipums_gsl_tract_display.html",
    "href": "posts/381_proj/src/ipums_gsl_tract_display.html",
    "title": "GSL data display",
    "section": "",
    "text": "library(dplyr) \n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2) \nlibrary(gganimate) \nlibrary(stringr) \nlibrary(ggpubr)\n\nThis document handles the charts used in the original presentation. First, it reads data created in the previous script:\n\nvars_joined &lt;- readr::read_csv(\"../data/vars_joined.csv\") \n\nRows: 34 Columns: 5\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (5): year, tpop, area, area_km, tpop_mn\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvars_joined.playa &lt;- vars_joined %&gt;% \n  mutate(new_playa = lag(area_km)-area_km)\n\nIt creates some labels, in a very inefficient manner ‚Äì if I were to rewrite this code today, I‚Äôd use something like format(x, big.mark = \",\").\n\nvars_joined.label &lt;- vars_joined.playa %&gt;%\n  mutate(tpop_lab = paste0(\"Total population: \", str_sub(tpop, 1,1), \",\", str_sub(tpop, 2,4), \",\", str_sub(tpop, 5,7)), \n         area_lab = paste0(\"Water area: \", str_sub(round(area_km), 1,1), \",\", str_sub(round(area_km), 2,4), \" sq. km\"), \n         playa_lab = paste0(\"New playa area: \", round(new_playa), \" sq. km\"))\n\nFirst, we set up an empty set of axes on which we‚Äôll plot our data:\n\np1 &lt;- ggplot(data = vars_joined, aes(x = year, y = tpop_mn))+ \n  labs(title = \"Population of counties bordering the Great Salt Lake, Utah, 1986-2021\",\n       x = \"Year\", y = \"Population (millions)\")+\n  theme(panel.background = element_blank(),\n        axis.title.x = element_text(size = 16), \n        axis.title.y = element_text(size = 16, hjust = 0.4, vjust = 1), \n        axis.text.x = element_text(color = \"black\", size = 14), \n        axis.text.y = element_text(color = \"black\", size = 14), \n        plot.title = element_text(hjust = 0, vjust = 5, size = 18, face = \"bold\"), \n        legend.key = element_blank(), \n        legend.position = \"top\", \n        plot.margin = margin(1, 5, 0.1,0.1, 'cm')) \n\np1\n\n\n\n\n\n\n\n\nThen, we‚Äôll use those axes to create an animated chart showing change in population over time.\n\np2 &lt;- p1 + \n  geom_point(data = vars_joined.label, aes(x = year, y = tpop_mn), color = \"#ffb303\", size = 3)+ \n  geom_line(data = vars_joined.label, aes(x = year, y = tpop_mn), color = \"#ffb303\", size = 1)+ \n  geom_text(data = vars_joined.label, aes(x = year + .1, y = tpop_mn), \n            label = vars_joined.label$tpop_lab, \n            hjust = -0.1, \n            vjust = -1, \n            size = 4, \n            show.legend = FALSE)+\n  transition_reveal(year)+ \n  ease_aes('linear')+ \n  coord_cartesian(clip = 'off') \n\nanimate(p2, start_pause = 24, end_pause = 24)\n\n\nJust like before, we‚Äôll create an empty set of axes, this time for water area:\n\np3 &lt;- ggplot(data = vars_joined, aes(x = year, y = area_km))+\n  labs(title = \"Water area of the Great Salt Lake, 1986-2021\", x = \"Year\", y = \"Water area,\"~km^2)+\n  theme(panel.background = element_blank(), \n        axis.title.x = element_text(size = 16), \n        axis.title.y = element_text(size = 16, hjust = 0.4, vjust = 1), \n        axis.text.x = element_text(color = \"black\", size = 14), \n        axis.text.y = element_text(color = \"black\", size = 14), \n        plot.title = element_text(hjust = 0, vjust = 5, size = 18, face = \"bold\"), \n        legend.key = element_blank(), \n        legend.position = \"top\", \n        plot.margin = margin(1, 5, 0.1,0.1, 'cm')) \n\np3\n\n\n\n\n\n\n\n\nAnd, again like before, we‚Äôll animate over those axes.\n\np4 &lt;- p3 + \n  geom_point(data = vars_joined, aes(x = year, y = area_km), color = \"#03afff\", size = 3)+ \n  geom_line(data = vars_joined, aes(x = year, y = area_km), color = \"#03afff\", size = 1)+ \n  geom_text(data = vars_joined.label, aes(x = year + .1, y = area_km), \n            label = vars_joined.label$area_lab, \n            hjust = -0.1, \n            vjust = -1, \n            size = 4, \n            show.legend = FALSE)+\n  transition_reveal(year)+\n  ease_aes('linear')+\n  transition_reveal(year)+\n  coord_cartesian(clip = 'off')\n\nanimate(p4, start_pause = 24, end_pause = 24)\n\nif (!file.exists(\"../images/utah_gsl_area.gif\")){\n  anim_save(filename = \"../images/utah_gsl_area.gif\", \n            fps = 24, nframes = 207, \n            animation = p4, \n            start_pause = 24, \n            end_pause = 24, \n            height = 1080, \n            width = 1920, \n            res = 180, \n            renderer = gifski_renderer())\n}\n\n\nOne last time, this time creating a linear regression model:\n\np8 &lt;- ggplot(data = vars_joined.label, aes(x = tpop, y = area_km))+ \n  labs(title = \"Water area of the Great Salt Lake compared with population, 1986-2021\", x = \"Population of counties bordering the Great Salt Lake\", y = \"Water area,\"~km^2)+ \n  theme(panel.background = element_blank(), \n        axis.title.x = element_text(size = 16), \n        axis.title.y = element_text(size = 16, hjust = 0.5, vjust = 1), \n        axis.text.x = element_text(color = \"black\", size = 14), \n        axis.text.y = element_text(color = \"black\", size = 14), \n        plot.title = element_text(hjust = 0, vjust = 5, size = 18, face = \"bold\"), \n        legend.key = element_blank(), \n        legend.position = \"top\", \n        plot.margin = margin(1, 0.1, 0.1,0.1, 'cm')) \n\np8\n\n\n\n\n\n\n\n\n\np7 &lt;- p8+ geom_point(data = vars_joined, aes(x = tpop, y = area_km), color = \"black\", size = 2)+ \n  geom_smooth(method=lm, se = FALSE, color = \"#a82727\")+ \n  stat_regline_equation(label.x = 2300000, label.y = 6500, aes(label = ..eq.label..))+ \n  stat_regline_equation(label.x = 2400000, label.y = 6400, aes(label = ..rr.label..)) \n\np7\n\nif (!file.exists(\"../images/pop_vs_area.png\")){\n  ggsave(p7, filename = \"../images/pop_vs_area.png\", width = 1920, height = 1080, dpi = 180, units = 'px')\n}\n\n\n\nlinreg &lt;- lm(area_km ~ tpop, data = vars_joined)\n\nsummary(linreg)\n\n\nCall:\nlm(formula = area_km ~ tpop, data = vars_joined)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-719.1 -243.1  -26.2  262.2  494.2 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.879e+03  2.847e+02   27.67  &lt; 2e-16 ***\ntpop        -1.601e-03  1.489e-04  -10.75 3.73e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 322.5 on 32 degrees of freedom\nMultiple R-squared:  0.7832,    Adjusted R-squared:  0.7764 \nF-statistic: 115.6 on 1 and 32 DF,  p-value: 3.731e-12"
  },
  {
    "objectID": "posts/381_proj/src/index.html",
    "href": "posts/381_proj/src/index.html",
    "title": "harrison deford",
    "section": "",
    "text": "Download the R script used to calculate yearly population estimates.\nDownload the R script used to visualize the area and population estimates and generate the linear regression."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about harrison",
    "section": "",
    "text": "My name is Harrison DeFord, and this website serves as a collection of documents showing my journey as I learn more about space, place, and what we can learn from them.\nI primarily write code in R, which I began learning in early 2022. I‚Äôm a geographer by training, but am also particularly interested in riding my bike (slowly), transportation and development in urban areas, and how geography shapes these topics.\nI‚Äôm currently a senior data analyst as part of the BCSTAT team in the Baltimore County Executive Office, where I serve as a spatial data guru, leading the development of internal and external tools, including the Social Determinants of Health (SDOH) web experience and the Baltimore County Food Pantry Locator. Working in the public sector is immensely fulfilling, and BCSTAT‚Äôs unique position in the government structure means I have experience with all sorts of topics, including animal services, code enforcement, fire/EMS, tax credits, transportation, and more.\nI graduated from UMBC with a BS in Geography and Environmental Systems in December 2022, and have since returned as a graduate student pursuing my MS, again in Geography and Environmental Systems. At UMBC I‚Äôve had the opportunity to work with several professors in a lab environment.\n\nThe UMBC labs I‚Äôve worked with\n\n\n\n\n\n\n\n\nProfessor\nLaboratory\nResearch Topics\nDates\n\n\n\n\nDr.¬†Dillon Mahmoudi\nMapping Capital\nAir quality, Census data, micromobility, critical geography\nAug 2021 - Present\n\n\nDr.¬†Matthew Fagan\nEarth from Above\nRemote Sensing, Machine Learning, Reforestation\nAug 2022 - Mar 2022\n\n\n\nDuring my time at UMBC, I‚Äôve also had the chance to work with other organizations on various projects, including the Partnership for a Healthier America and UMBC‚Äôs IS department as a Data Science Scholar.\nIf you‚Äôre interested in working with me on a project, please reach out at hdeford1 [at] umbc [dot] edu. I currently don‚Äôt have much bandwidth, but am always willing to listen to new research ideas!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "harrison deford",
    "section": "",
    "text": "forklift certified; aspiring geographer\nüìç baltimore\n\nPORTFOLIO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicromobility in Baltimore\n\n\n\n\n\n\numbc\n\n\ntransportation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Great Shrinking Lake\n\n\n\n\n\n\numbc\n\n\nremote sensing\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/381_proj/index.html",
    "href": "posts/381_proj/index.html",
    "title": "The Great Shrinking Lake",
    "section": "",
    "text": "This presentation was given as my final project for my remote sensing course at UMBC. It uses R and Google Earth Engine to analyze the relationship between the changing water area of the Great Salt Lake since 1986 and the growing population of the Wasatch Front and the Utah Valley. You can view the animated timeseries of the lake here.\nThe report was limited in scope due to limited time to work: obviously, there are more factors contributing to a shrinking lake area than just population growth. However, in USGS‚Äôs 2015 Circular 1441 ranked Utah second of all states in per capita domestic water usage, so to say that the population of the Valley is not a major reason the Lake is losing water yearly would be undeniably false.\nLinear correlation tests run in R yielded a significantly negative correlation of y ~ 0.0016x and an R-squared value of 0.78 (p &lt;&lt; 0.001). In context, according to this model, for every 100,000 people that move to the Salt Lake area, 160 km2 of water surface are lost.\nThe R scripts for this report are available here and the Earth Engine code is available here\nNOTE: Google has removed Landsat Collection 1 data from GEE; this script will need to be modified before use (see this GEE article for more details)."
  },
  {
    "objectID": "posts/381_proj/index.html#final-project-for-ges-381-remote-sensing",
    "href": "posts/381_proj/index.html#final-project-for-ges-381-remote-sensing",
    "title": "The Great Shrinking Lake",
    "section": "",
    "text": "This presentation was given as my final project for my remote sensing course at UMBC. It uses R and Google Earth Engine to analyze the relationship between the changing water area of the Great Salt Lake since 1986 and the growing population of the Wasatch Front and the Utah Valley. You can view the animated timeseries of the lake here.\nThe report was limited in scope due to limited time to work: obviously, there are more factors contributing to a shrinking lake area than just population growth. However, in USGS‚Äôs 2015 Circular 1441 ranked Utah second of all states in per capita domestic water usage, so to say that the population of the Valley is not a major reason the Lake is losing water yearly would be undeniably false.\nLinear correlation tests run in R yielded a significantly negative correlation of y ~ 0.0016x and an R-squared value of 0.78 (p &lt;&lt; 0.001). In context, according to this model, for every 100,000 people that move to the Salt Lake area, 160 km2 of water surface are lost.\nThe R scripts for this report are available here and the Earth Engine code is available here\nNOTE: Google has removed Landsat Collection 1 data from GEE; this script will need to be modified before use (see this GEE article for more details)."
  },
  {
    "objectID": "posts/381_proj/src/ipums_gsl_tract.html",
    "href": "posts/381_proj/src/ipums_gsl_tract.html",
    "title": "IPUMS data collection and processing",
    "section": "",
    "text": "This document describes the process through which population estimates were made and related to water area values extracted from Google Earth.\n\nlibrary(ipumsr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidycensus)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(tidyverse)\n\nFirst, a CSV containing information from the 1980, 1990, 2000, and 2010 censuses, all normalized to 2010 geographies, is loaded using read_nhgis() from {ipumsr}, and only counties bordering the Great Salt Lake were kept.\n\nut_ts &lt;- read_nhgis(\"../data/nhgis0005_ts_geog2010_tract.csv\") %&gt;%\n  filter(STATE == \"Utah\") %&gt;%\n  filter(str_detect(COUNTY, \"Utah County|Salt Lake County|Davis|Weber|Box Elder|Tooele\")) %&gt;%\n  select(c(1,6,8:17)) %&gt;%\n  pivot_longer(cols = tidyselect::starts_with(\"CL8AA\"))%&gt;%\n  filter(name %in% c(\"CL8AA1990\",\"CL8AA2000\",\"CL8AA2010\",\"CL8AA2020\"))\n\nA linear regression model was generated for each census tract. This allows for the interpolation of population data between the 10-year census recurrence interval, before the ACS was begun in 2009.\nThe estimated annual populations of each tract were summarized to the level of the entire geography, then ACS 5-year data was used from 2009 onwards. These data sources were combined to yield continuous annual population estimates from 1986 to 2020.\n\nut_ts.pred &lt;- list()\nfor (i in 1:length(ut_model)){\n  ut_ts.pred[[i]] = data.frame(year = c(1986:2009), \n                               GISJOIN = first(ut_ts.split[[i]]$GISJOIN), \n                               COUNTYA = first(ut_ts.split[[i]]$COUNTYA))\n  ut_ts.pred[[i]]$tpop = round(predict(ut_model[[i]], newdata = ut_ts.pred[[i]]))\n} \nut_ts.pred &lt;- bind_rows(ut_ts.pred)\n\nut_ts.sum &lt;- ut_ts.pred %&gt;%\n  group_by(year)%&gt;%\n  mutate(tpop = sum(tpop)) %&gt;%\n  ungroup()%&gt;%\n  distinct(year, .keep_all = TRUE) %&gt;%\n  select(year, tpop)\n\nut_acs.list &lt;- lapply(c(2010:2020), function(y) get_acs(geography = \"county\", state = \"UT\", year = y, survey = \"acs5\", variables = \"B01001_001\", cache_table = TRUE) %&gt;% mutate(year = y)) %&gt;%\n  bind_rows()\nut_acs.filtered &lt;- ut_acs.list %&gt;% filter(str_detect(NAME, \"Utah County|Salt Lake County|Davis|Weber|Box Elder|Tooele\")) %&gt;%\n  group_by(year) %&gt;%\n  mutate(tpop = sum(estimate))%&gt;%\n  ungroup()%&gt;%\n  distinct(year, .keep_all = TRUE) %&gt;%\n  select(year, tpop)\n\nut_tpop &lt;- bind_rows(ut_ts.sum, ut_acs.filtered)\n\nFinally, the population estimates were joined to the water area values obtained from Google Earth Engine, and written to a .csv for use in further processing.\n\nyear_area_gsl &lt;- read_csv(\"../data/year_area_gsl.csv\")\nvars_joined &lt;- left_join(ut_tpop, year_area_gsl) %&gt;% mutate(area_km = (area/1e6), tpop_mn = (tpop/1e6))\nvars_joined &lt;- na.omit(vars_joined)\nwrite_csv(vars_joined, file = \"../data/vars_joined.csv\")"
  },
  {
    "objectID": "posts/center_of_pop/src/index.html",
    "href": "posts/center_of_pop/src/index.html",
    "title": "Centers of Population by Race/Ethnicity",
    "section": "",
    "text": "Every decennial census, the U.S. Census Bureau releases an updated version of the Mean Center of Population, which can be interpreted as the ‚Äúcenter of gravity‚Äù of the U.S. population. The goal of this project is to calculate the mean center of population per ethnic group captured in the decennial census.\nFor simplification, the groups of interest in this analysis are:\n\n\n\n\n\n\n\nGroup\nCensus Sub-groups\n\n\n\n\nHispanic\n‚ÄúHispanic or Latino‚Äù\n\n\nNH White\n‚ÄúNot Hispanic or Latino; White alone‚Äù\n\n\nNH African-American\n‚ÄúNot Hispanic or Latino; Black or African-American alone‚Äù\n\n\nNH American and Alaskan Native\n‚ÄúNot Hispanic or Latino; American Indian and Alaska Native alone‚Äù\n\n\nNH Asian\n‚ÄúNot Hispanic or Latino; Asian alone‚Äù\n\n\nNH Hawaiian and Pacific Islander\n‚ÄúNot Hispanic or Latino; Native Hawaiian and Other Pacific Islander alone‚Äù\n\n\nNH Other\n‚ÄúNot Hispanic or Latino; Some Other Race alone‚Äù\n\n\nNH Two or more\n‚ÄúNot Hispanic or Latino; Two or more races‚Äù\n\n\n\n\nLibraries\nFirst, we‚Äôll load required libraries:\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(showtext)\nlibrary(furrr)\nlibrary(data.table)\nlibrary(dtplyr)\noptions(tigris_use_cache = TRUE)\noptions(scipen = 999)\nextrafont::loadfonts(device = \"win\", quiet = T)\n\nrm(list = ls())\nyr = 2020\n\n\n\nParallelization\nSince we‚Äôre computing at the block level (11 million blocks) across 7 groups, we‚Äôll have a dataframe with over 70 million rows at its highest. I actually had to upgrade to 32GB of memory to run this analysis: we‚Äôre going to create a {furrr} multisession using all but one of our CPU threads. In my case, that‚Äôs 11 threads.\n\nfips_list &lt;- unique(fips_codes$state)[1:51]\nnum_cores &lt;- availableCores()-1\nplan(multisession, workers = num_cores)\n\nNow that our multisession is planned, we can start gathering our census data. The {tidycensus} package provides a fantastic interface to the USCB API, but at the block level we can only collect data one state at a time. However, by using future_map() to iterate over our list of state FIPS codes (fips_list) created earlier, we can gather 11 states worth of data simultaneously until we‚Äôve obtained population counts for all 50 states, then reduce() them into one very long dataframe.\n\n\nCensus Data Collection\nNote that we gather tabular data (b) and spatial data (c) separately, to be stored in two different files on our machine. This code block takes my machine 20-30 minutes to run ‚Äì luckily, once you‚Äôve run it once, you shouldn‚Äôt have to run it again thanks to the power of fwrite()!\n\n## Not run:\n\nif (yr &lt; 2020){\n  pop_vars &lt;- c(white = \"P005003\",\n                black = \"P005004\",\n                aian = \"P005005\",\n                asian = \"P005006\",\n                hipi = \"P005007\",\n                other = \"P005008\",\n                two_p = \"P005009\",\n                hisp = \"P005010\")\n} else {\n  pop_vars &lt;- c(white = \"P2_005N\",\n                black = \"P2_006N\",\n                aian = \"P2_007N\",\n                asian = \"P2_008N\",\n                hipi = \"P2_009N\",\n                other = \"P2_010N\",\n                two_p = \"P2_011N\",\n                hisp = \"P2_002N\")\n}\n\nb &lt;- reduce(future_map(fips_list, function(x){\n  get_decennial(geography = \"block\", \n                           variables = pop_vars,\n                           year = yr,\n                           state = x,\n                           output = \"tidy\",\n                           geometry = FALSE)}, .progress = TRUE), rbind)\n\nc &lt;- reduce(future_map(fips_list, function(x){blocks(state = x, year = yr) %&gt;%\n     st_drop_geometry() %&gt;%\n     mutate(across(.cols = starts_with(\"INTPTLON\"), .fns = as.numeric, .names = \"lon\")) %&gt;%\n     mutate(across(.cols = starts_with(\"INTPTLAT\"), .fns = as.numeric, .names = \"lat\")) %&gt;%\n     select(c(5,last_col(offset = 1), last_col()))}, .progress = TRUE), rbind)\n\nfuture:::ClusterRegistry(\"stop\")\n\nfwrite(as.data.table(b), paste0(\"../data/pop_data_\", yr, \".csv\"))\nfwrite(as.data.table(c), paste0(\"../data/latlon_\", yr, \".csv\"))\n\n## End(Not run)\n\n\n\nCalculations\nNow, we need to join our tables in order to weight the internal point of each block with its respective population. Note the usage of {dtplyr} here: I wanted to keep writing in tidyverse syntax while gaining the performance of {data.table}. What a fantastic package!\nThe formula used is derived from this document. Since block-level population centers aren‚Äôt available through {tigris}, we will use the ‚Äúinternal point‚Äù latitude and longitude. The internal point of a geography is often the centroid: if a geography is shaped such that the centroid would fall outside its boundary, the internal point is placed as close to the internal centroid of the geography as possible, preferably on land.\n\nb &lt;- lazy_dt(fread(paste0(\"../data/pop_data_\", yr, \".csv\")))\nc &lt;- lazy_dt(fread(paste0(\"../data/latlon_\", yr, \".csv\")))\no &lt;- b %&gt;%\n  pivot_wider(names_from = variable, values_from = value) %&gt;%\n  mutate(other = other + two_p) %&gt;%\n  select(-c(two_p)) %&gt;%\n  pivot_longer(cols = 3:9, names_to = \"variable\")\n\nj &lt;- left_join(o, c, by = c(\"GEOID\" = paste0(\"GEOID\", str_sub(yr, 3,4)))) %&gt;%\n  mutate(elat = value * lat,\n         elon = value * lon * cos(lat * (pi/180)),\n         denom = value*cos(lat * (pi/180)))\n\n\ng &lt;- j %&gt;% \n  group_by(variable) %&gt;%\n  summarize(tlat = sum(elat, na.rm = TRUE)/sum(value, na.rm = TRUE),\n            tlon = sum(elon, na.rm = TRUE)/sum(denom, na.rm = TRUE),\n            pop = sum(value))\n\ncc &lt;- as.data.table(j) %&gt;%\n  summarize(tlat = sum(elat, na.rm = TRUE)/sum(value, na.rm = TRUE),\n            tlon = sum(elon, na.rm = TRUE)/sum(denom, na.rm = TRUE),\n            pop = sum(value))\n\ng &lt;- as.data.table(g)\n\n\ns &lt;- g %&gt;%\n  st_as_sf(coords = c(\"tlon\",\"tlat\")) %&gt;% st_set_crs(4326) %&gt;% st_transform(6350)\n\ncc &lt;- cc %&gt;%\n  st_as_sf(coords = c(\"tlon\", \"tlat\")) %&gt;% st_set_crs(4326) %&gt;% st_transform(6350)\n\nlso &lt;- vector(mode = 'list', length = nrow(s))\n\nfor (row in 1:nrow(s)){\n  cs &lt;- data.frame(st_coordinates(cc)[1], st_coordinates(cc)[2]) %&gt;%\n  rename(p_x = 1,\n         p_y = 2) %&gt;%\n  mutate(seq = 1)\n  cs &lt;- cbind(g[,c(1,4)],cs)\n  \n  ss &lt;- s[row,] %&gt;%\n    rowwise() %&gt;%\n    mutate(p_x = unlist(geometry)[1],\n           p_y = unlist(geometry)[2]) %&gt;%\n    mutate(seq = 2) %&gt;%\n    st_drop_geometry() %&gt;%\n    ungroup()\n  \n  ap &lt;- rbind(ss,cs[row,]) %&gt;%\n    arrange(variable, seq)\n  \n  m &lt;- as.matrix(ap[order(ap$seq),\n                    c(\"p_x\",\"p_y\")])\n  \n  ls &lt;- st_linestring(m) %&gt;%\n    st_sfc() %&gt;%\n    st_sf() %&gt;%\n    mutate(variable = ap$variable[1],\n           pop = ap$pop[1]) %&gt;%\n    bind_cols()\n  \n  lso &lt;- lso %&gt;% bind_rows(ls)\n}\n\nlsp &lt;- lso %&gt;% st_set_crs(6350)\n\n\ns &lt;- s %&gt;% mutate(pop_pct = pop/sum(pop),\n                  log_pct = abs(log(pop_pct)),\n                  normalized_log_pct = 0.1 + (log_pct - max(log_pct)) / (min(log_pct) - max(log_pct)) * (0.7 - 0.1))\n\nplot(lsp[\"variable\"])\n\n\n\n\n\n\n\n\n\n\nPlotting\nFinally, all we have to do is plot our points! While I don‚Äôt have them sized relative to their populations, it would be very easy to do: all I‚Äôd have to do is replace the numeric value in tm_symbols() with the column name I wanted to graduate on.\n\nilh &lt;- palette(c(rgb(114,153,67, maxColorValue = 255),\n                 rgb(148,79,161, maxColorValue = 255),\n                 rgb(76,196,144, maxColorValue = 255),\n                 rgb(185,74,115, maxColorValue = 255),\n                 rgb(193,158,60, maxColorValue = 255),\n                 rgb(104,123,210, maxColorValue = 255),\n                 rgb(185,85,61, maxColorValue = 255)))\n\nplot_fips &lt;- unique(fips_codes$state_code)[1:51]\nplot_fips &lt;- plot_fips[!plot_fips %in% c(\"02\", \"15\", \"72\", \"78\")]\n\nus &lt;- states(cb = TRUE, year = yr) %&gt;% filter(if (yr == 2010) STATE %in% plot_fips else\n                                                 STATEFP %in% plot_fips) %&gt;%\n  st_transform(6350) #weird artifact in tigris means that column names don't match\n\nt &lt;- tm_shape(us, bbox = bb(us, ext = 1.1))+\n  tm_polygons(border.col = \"#aaaaaa\", lwd = 0.75, col = \"#5b5b5b\")+\n  tm_shape(lsp)+\n  tm_lines(col = \"variable\", palette = ilh, legend.col.show = FALSE, lwd = 1.5, legend.lwd.show = FALSE)+\n  tm_shape(s)+\n  tm_symbols(col = \"variable\", title.col = \"Race/Ethnicity\", size = \"normalized_log_pct\", border.col = \"#bdbdbd\", palette = ilh, border.lwd = 1, legend.col.show = FALSE, legend.size.show = FALSE)+\n  tm_shape(cc)+\n  tm_dots(col = \"#1286c4\", shape = 24, title = \"Total center of population\", size = 0.6, legend.show = FALSE, border.lwd = 1, border.col = \"#bdbdbd\")+\n  tm_add_legend(type = \"symbol\", \n    labels = c(\"American/Alaskan Native\", \"Asian\", \"African-American\", \"Hawaiian/Pacific Islander\", \"Hispanic\", \"Other/Two or more\", \"White\"),\n    col = ilh,\n    border.col = \"#bdbdbd\",\n    title = \"Ethnicity\",\n    size = 0.4)+\n  tm_add_legend(type = \"symbol\",\n                shape = 24,\n                col = \"#1286c4\",\n                size = 0.6,\n                border.col = \"#bdbdbd\",\n                labels = \"Total center of population\")+\n  tm_layout(main.title = \"Center of population by race/ethnicity\",\n            main.title.fontfamily = \"Manrope\",\n            main.title.fontface = 2,\n            main.title.size = 2,\n            bg.color = \"#3b3b3b\",\n            legend.outside = TRUE,\n            legend.text.color = \"#bdbdbd\", \n            legend.text.fontfamily = \"Manrope\", \n            legend.title.color = \"#bdbdbd\",\n            legend.title.fontface = 2,\n            legend.title.size = 1.5,\n            legend.title.fontfamily = \"Manrope\", \n            legend.text.size = 0.75,\n            legend.position = c(0,0.25),\n            outer.bg.color = \"#3b3b3b\",\n            frame = FALSE,\n            main.title.color = \"#bdbdbd\")+\n  tm_credits(paste0(\"Decennial census data, \", yr, \"\\nGraphic by Harrison DeFord (@oonuttlet)\"),\n             position = c(0,0.08),\n             col = \"#bdbdbd\",\n             fontfamily = \"Open Sans\",\n             size = 0.62)\n\nt\n\n\n\n\n\n\n\nif (!file.exists(paste0(\"../bin/center_of_pop_natl_\",yr,\"_intpt.png\"))){\n  tmap_save(t, paste0(\"../bin/center_of_pop_natl_\",yr,\"_intpt.png\"), dpi = 1200)\n}"
  },
  {
    "objectID": "posts/finalproj/src/flow_diagram.html",
    "href": "posts/finalproj/src/flow_diagram.html",
    "title": "Generating Flow Lines",
    "section": "",
    "text": "This script is meant to be run after QGIS 2.x.x is used to generate paths from the timeseries data generated in the previous script. The older version of QGIS is necessary to use the Points to Paths plugin, which allows for separate lines per vertex (important because we define each 15-minute interval as a distinct trip, even if movement was detected over several consecutive intervals). Within the plugin, bike IDs and row numbers were concatenated using field calculator in order to create a movement ID, which was used to define a trip as a movement of over 50 meters within 15 minutes to account for GPS variability on scooters.\n\nflow_lines &lt;- st_read(\"../results/trip_id_p2p.shp\")\n\nReading layer `trip_id_p2p' from data source \n  `D:\\website\\oonuttlet.github.io\\posts\\finalproj\\results\\trip_id_p2p.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 255 features and 7 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -8538362 ymin: 4755023 xmax: -8520945 ymax: 4774157\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ntrip_id_long &lt;- st_read(\"../results/trip_id_long.gpkg\")\n\nReading layer `trip_id_long' from data source \n  `D:\\website\\oonuttlet.github.io\\posts\\finalproj\\results\\trip_id_long.gpkg' \n  using driver `GPKG'\nSimple feature collection with 10251 features and 13 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8539536 ymin: 4754428 xmax: -8516307 ymax: 4775070\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nflow_lines_proj &lt;- flow_lines %&gt;% st_transform(4326)\n\nWe use lwgeom to define the start and endpoints of each of our flow lines, so we have a directionality for our trips\n\nflow_lines$start_geom &lt;- st_startpoint(flow_lines)\nflow_lines$end_geom &lt;- st_endpoint(flow_lines)\nflow_lines_proj$start_geom &lt;- st_startpoint(flow_lines_proj)\nflow_lines_proj$end_geom &lt;- st_endpoint(flow_lines_proj)\n\nSince almost all spatial file formats require only one geometry column, we write to an RDS file to preserve our geometries (for starts and ends, and in the next script for hexagon data as well).\n\nflow_lines &lt;- flow_lines %&gt;% mutate(dist = st_length(geometry))\nsummarise(flow_lines, mean = mean(dist))\n\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -8538362 ymin: 4755023 xmax: -8520945 ymax: 4774157\nProjected CRS: WGS 84 / Pseudo-Mercator\n          mean                       geometry\n1 1289.586 [m] MULTILINESTRING ((-8523392 ...\n\nif(!file.exists(\"../results/flow_lines.RDS\")){\n  saveRDS(object = flow_lines, file = \"../results/flow_lines.RDS\")\n}"
  },
  {
    "objectID": "posts/finalproj/src/flow_diagram.html#setup",
    "href": "posts/finalproj/src/flow_diagram.html#setup",
    "title": "Generating Flow Lines",
    "section": "",
    "text": "This script is meant to be run after QGIS 2.x.x is used to generate paths from the timeseries data generated in the previous script. The older version of QGIS is necessary to use the Points to Paths plugin, which allows for separate lines per vertex (important because we define each 15-minute interval as a distinct trip, even if movement was detected over several consecutive intervals). Within the plugin, bike IDs and row numbers were concatenated using field calculator in order to create a movement ID, which was used to define a trip as a movement of over 50 meters within 15 minutes to account for GPS variability on scooters.\n\nflow_lines &lt;- st_read(\"../results/trip_id_p2p.shp\")\n\nReading layer `trip_id_p2p' from data source \n  `D:\\website\\oonuttlet.github.io\\posts\\finalproj\\results\\trip_id_p2p.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 255 features and 7 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -8538362 ymin: 4755023 xmax: -8520945 ymax: 4774157\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ntrip_id_long &lt;- st_read(\"../results/trip_id_long.gpkg\")\n\nReading layer `trip_id_long' from data source \n  `D:\\website\\oonuttlet.github.io\\posts\\finalproj\\results\\trip_id_long.gpkg' \n  using driver `GPKG'\nSimple feature collection with 10251 features and 13 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8539536 ymin: 4754428 xmax: -8516307 ymax: 4775070\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nflow_lines_proj &lt;- flow_lines %&gt;% st_transform(4326)\n\nWe use lwgeom to define the start and endpoints of each of our flow lines, so we have a directionality for our trips\n\nflow_lines$start_geom &lt;- st_startpoint(flow_lines)\nflow_lines$end_geom &lt;- st_endpoint(flow_lines)\nflow_lines_proj$start_geom &lt;- st_startpoint(flow_lines_proj)\nflow_lines_proj$end_geom &lt;- st_endpoint(flow_lines_proj)\n\nSince almost all spatial file formats require only one geometry column, we write to an RDS file to preserve our geometries (for starts and ends, and in the next script for hexagon data as well).\n\nflow_lines &lt;- flow_lines %&gt;% mutate(dist = st_length(geometry))\nsummarise(flow_lines, mean = mean(dist))\n\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -8538362 ymin: 4755023 xmax: -8520945 ymax: 4774157\nProjected CRS: WGS 84 / Pseudo-Mercator\n          mean                       geometry\n1 1289.586 [m] MULTILINESTRING ((-8523392 ...\n\nif(!file.exists(\"../results/flow_lines.RDS\")){\n  saveRDS(object = flow_lines, file = \"../results/flow_lines.RDS\")\n}"
  },
  {
    "objectID": "posts/finalproj/src/index.html",
    "href": "posts/finalproj/src/index.html",
    "title": "harrison deford",
    "section": "",
    "text": "These three scripts are data preparation and manipulation scripts:\nScript 1\nScript 2\nScript 3\nThis final script produces HTML widgets, which are interactive in 3 dimensions, to smybolize scooter flows. It only uses data collected on May 1, 2022 (due to hardware restraints).\nScript 4\nThis HTML is what I presented in class, and incorporates elements from all of the above scripts.\nIt uses data collected the week of 05/01/2022 through 05/07/2022 to output maps on scooter locations compared to BIPOC population and job and population location."
  },
  {
    "objectID": "posts/finalproj/src/index.html#ges-486-final-project-scripts",
    "href": "posts/finalproj/src/index.html#ges-486-final-project-scripts",
    "title": "harrison deford",
    "section": "",
    "text": "These three scripts are data preparation and manipulation scripts:\nScript 1\nScript 2\nScript 3\nThis final script produces HTML widgets, which are interactive in 3 dimensions, to smybolize scooter flows. It only uses data collected on May 1, 2022 (due to hardware restraints).\nScript 4\nThis HTML is what I presented in class, and incorporates elements from all of the above scripts.\nIt uses data collected the week of 05/01/2022 through 05/07/2022 to output maps on scooter locations compared to BIPOC population and job and population location."
  },
  {
    "objectID": "posts/finalproj/src/scooter_locations.html",
    "href": "posts/finalproj/src/scooter_locations.html",
    "title": "Micromobility in Baltimore",
    "section": "",
    "text": "flow_lines_arc &lt;- st_sf(readRDS(\"../results/flow_lines_arc.RDS\"))\nbalt_hex &lt;- st_sf(readRDS(\"../results/balt_hex.RDS\"))\notm_lehd &lt;- st_read(\"../results/otm_5b749ca22bf64d9abe899ef50a619131/points_2019.shp\") %&gt;% st_transform(3857) %&gt;%\n  select(id, c000, geometry)\nscooters_raw &lt;- st_read(\"../results/scooters_raw.gpkg\") %&gt;% filter(is_disabled == 0)\n\n\nbaltimore_bg_income &lt;- get_acs(geography = \"block group\", \n                           variables = c(\"pop\" = \"B03002_001\", # Total\n                                         \"pop_nhwhite\" = \"B03002_003\", # NH White\n                                         \"pop_nhblack\" = \"B03002_004\", # NH Black\n                                         \"pop_nhamind\" = \"B03002_005\", # NH Am Ind\n                                         \"pop_nhasian\" = \"B03002_006\", # NH Asian\n                                         \"pop_nhhwnpi\" = \"B03002_007\", # NH Hawaiin/PI\n                                         \"pop_nhother\" = \"B03002_008\", # One Other\n                                         \"pop_nhtwomr\" = \"B03002_009\", # Two+\n                                         \"pop_hispltx\" = \"B03002_012\", # Hispanic/Latinx\n                                         \"hu_total\"  = \"B25001_001\", # Housing Units\n                                         \"hu_totocc\" = \"B25003_001\", # Housing Units - Occ\n                                         \"hu_totown\" = \"B25003_002\", # Housing Units - Owner Occ,\n                                         \"hu_totrnt\" = \"B25003_003\", # Housing Units - Renter Occ,\n                                         \"mhhi\" = \"B19013_001\"), #median household income\n                           \n                           year = 2019,\n                           survey = \"acs5\",\n                           state = c(24), \n                           county = c(510), \n                           geometry = TRUE, \n                           output = \"wide\") %&gt;% st_transform(3857)\n\nGetting data from the 2015-2019 5-year ACS\n\n\n\n# Computes the NH Asian Population\nbaltimore_bg_income$pop_nhasianXE &lt;- baltimore_bg_income$pop_nhasianE + baltimore_bg_income$pop_nhhwnpiE\n\n# Computes the NH \"Other\" Population\nbaltimore_bg_income$pop_nhotherXE &lt;- baltimore_bg_income$pop_nhamindE + baltimore_bg_income$pop_nhotherE + baltimore_bg_income$pop_nhtwomrE\n\n\nst_erase &lt;- function(x, y) {\n  st_difference(x, st_make_valid(st_union(y)))}\nbmore_water &lt;- area_water(\"MD\", c(510,005), class = \"sf\") %&gt;% st_transform(3857) %&gt;% filter(AWATER &gt; 20000)\n\nRetrieving data for the year 2022\n\nbmore_water &lt;- st_make_valid(st_buffer(bmore_water, 0)) # Fix topology\n\n\nbaltimore_bg_income &lt;- baltimore_bg_income %&gt;% st_transform(3857)\nbaltimore_bg_income &lt;- st_erase(baltimore_bg_income, bmore_water)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nbalt_hex &lt;- balt_hex %&gt;% st_transform(3857)\nbalt_bg_emp &lt;- st_join(otm_lehd, balt_hex)\nbalt_bg_emp &lt;- balt_bg_emp %&gt;% group_by(hex_id) %&gt;% summarise(sum_c000 = sum(c000))\nbalt_hex.intersects &lt;- st_intersects(st_union(baltimore_bg_income), balt_hex)\nbalt_hex.subset &lt;- balt_hex[balt_hex.intersects[[1]],]\nbmore_scooters_in_hex &lt;- st_join(balt_hex.subset, scooters_raw)\nbmore_scooter_hex_count &lt;- count(as_tibble(bmore_scooters_in_hex), hex_id)\nbalt_hex.subset &lt;- left_join(balt_hex.subset, bmore_scooter_hex_count)\n\nJoining with `by = join_by(hex_id)`\n\n\n\nar_validate(source = baltimore_bg_income, target = balt_hex.subset, varList = \"popE\", method = \"aw\", verbose = TRUE)\n\nbmore_grid_income &lt;- aw_interpolate(balt_hex.subset, tid = hex_id, source = baltimore_bg_income, sid = \"GEOID\", weight = \"sum\", output = \"sf\", extensive = c(\"popE\",\"hu_totalE\", \"pop_nhwhiteE\"))\nbmore_grid_emp_income &lt;- left_join(bmore_grid_income, st_drop_geometry(balt_bg_emp))\n\nJoining with `by = join_by(hex_id)`\n\nbmore_grid_emp_income[is.na(bmore_grid_emp_income)] &lt;- 0\nbmore_grid_emp_income &lt;- bmore_grid_emp_income %&gt;% rename(scooter_count = n)%&gt;% mutate(people_pt = popE + sum_c000, scooters_per_pt = (1000*scooter_count/18)/people_pt, pct_bipoc = (popE-pop_nhwhiteE)/popE) %&gt;% filter(people_pt &gt; 10)\nbmore_grid_emp_income[is.na(bmore_grid_emp_income)] &lt;- 0\nbmore_grid_emp_income &lt;- bmore_grid_emp_income %&gt;% st_transform(3857)\n\n\nbbox_new &lt;- st_bbox(bmore_grid_emp_income) # current bounding box\n\nxrange &lt;- bbox_new$xmax - bbox_new$xmin # range of x values\nyrange &lt;- bbox_new$ymax - bbox_new$ymin # range of y values\n\nbbox_new &lt;- bbox_new %&gt;%  # take the bounding box ...\n  st_as_sfc() # ... and make it a sf polygon\n\n\nggplot.pts &lt;- ggplot()+\n  geom_sf(balt_hex, fill = NA, mapping = aes())+\n  geom_sf(bmore_grid_emp_income, color = NA, mapping = aes(fill = people_pt))+\n  labs(title = \"Total person-points\", fill = \"Person-points\")+\n  geom_sf(st_union(baltimore_bg_income), fill = NA, color = \"black\", size = 0.8, mapping = aes())+\n  scale_fill_viridis_c(breaks = c(0, 20000, 40000))+\n  coord_sf(xlim = st_coordinates(bbox_new)[c(1,2),1], \n           ylim = st_coordinates(bbox_new)[c(2,3),2]) + \n  theme_void()+\n  theme(plot.title = element_text(hjust = 0.5), panel.border = element_rect(color = \"black\", fill = NA), legend.position = \"bottom\", plot.margin = margin(0,5,0,5))\n\nggplot.pct_bipoc &lt;- ggplot()+\n  geom_sf(balt_hex, fill = NA, mapping = aes())+\n  geom_sf(bmore_grid_emp_income, color = NA, mapping = aes(fill = pct_bipoc))+\n  labs(title = \"Percentage of BIPOC residents\", fill = \"% BIPOC\")+\n  geom_sf(st_union(baltimore_bg_income), fill = NA, color = \"black\", size = 0.8, mapping = aes())+\n  scale_fill_viridis_c()+\n  coord_sf(xlim = st_coordinates(bbox_new)[c(1,2),1], \n           ylim = st_coordinates(bbox_new)[c(2,3),2]) + \n  theme_void()+\n  theme(plot.title = element_text(hjust = 0.5), panel.border = element_rect(color = \"black\", fill = NA), legend.position = \"bottom\", plot.margin = margin(0,5,0,5))\n\nggplot.scooters_pt &lt;- ggplot()+\n  geom_sf(balt_hex, fill = NA, mapping = aes())+\n  geom_sf(bmore_grid_emp_income, color = NA, mapping = aes(fill = scooters_per_pt))+\n  labs(title = \"Scooters per 1000 person-points\", fill = \"Scooters per 1000 \\nperson-points\")+\n  geom_sf(st_union(baltimore_bg_income), fill = NA, color = \"black\", size = 0.8, mapping = aes())+\n  scale_fill_viridis_c()+\n  coord_sf(xlim = st_coordinates(bbox_new)[c(1,2),1], \n           ylim = st_coordinates(bbox_new)[c(2,3),2]) + \n  theme_void()+\n  theme(plot.title = element_text(hjust = 0.5), panel.border = element_rect(color = \"black\", fill = NA), legend.position = \"bottom\", plot.margin = margin(0,5,0,5))\n\n\nlayout1 &lt;- ggplot.scooters_pt|ggplot.pct_bipoc\nlayout1\n\n\n\n\n\n\n\n\n\nlayout2 &lt;- ggplot.scooters_pt | ggplot.pts\nlayout2"
  }
]