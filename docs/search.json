[
  {
    "objectID": "posts/finalproj/src/start_end_join.html",
    "href": "posts/finalproj/src/start_end_join.html",
    "title": "Data Gathering and Wrangling",
    "section": "",
    "text": "The purpose of this script is to define functions which convert data collected by the python script running on mapping.capital, which are returned as nested JSON files, to geoJSON which can be used for analysis. This file is set up for a dataset of all scooter locations, collected every 15 minutes, from 0600 Eastern to 1000 Eastern on May 1, 2022. The second function defined in this document creates a long dataframe which is a timeseries of scooter locations over time.\n\nvendors = list(\"link\",\"lime\",\"spin\")\n\nThis code block defines a method to convert the nested JSON returned by scooter vendors to a non-nested geoJSON which can be read by sf.\n\njson2geoJSON &lt;- function(vendor){\n  files &lt;- list.files(path = paste(\"../data/monday/\", vendor, \"/morning/json/\", sep = \"\"), pattern=\"*.json\", full.names=TRUE, recursive=FALSE) #list files in directory\n    lapply(files, function(x) {\n      current_data &lt;- fromJSON(txt = x) # load file\n      current_tibble &lt;- as_tibble(current_data$data$bikes) #convert to tibble\n      current_tibble$timestamp_utc &lt;- as_datetime(current_data$last_updated, tz = Sys.timezone()) #create timestamp column\n      current_sf &lt;- st_as_sf(current_tibble, coords = c(\"lon\",\"lat\"), crs = 4326) #coerce to sf\n      if (!file.exists(paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\",\n                                       current_data$last_updated, \"_\", vendor, \".geoJSON\"))){\n      st_write(current_sf, dsn = paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\",\n                                       current_data$last_updated, \"_\", vendor, \".geoJSON\", sep = \"\")\n               , append = FALSE) #write as geoJSON\n      }\n})\n}\n\n\nfor (v in vendors){\n  json2geoJSON(v)\n} #loop through each of link, lime, spin\n\nThis section of code defines a function which creates a timeseries for each scooter and adds a vendor column which can be grouped by in following scripts.\n\n\n\n\n\n\nImportant\n\n\n\nNote 2024-10-11: I cannot emphasize enough how much you SHOULD NOT USE GLOBAL ASSIGNMENT (&lt;&lt;-) in a function. This was some of the first R code I’d ever written, and I’m leaving it for posterity. However, there are MUCH better ways to do this.\n\n\n\nload_timeseries &lt;- function(vendor){\n  files &lt;- list.files(path = paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\", sep = \"\"), pattern=\"*.geoJSON\", full.names=TRUE, recursive=FALSE) #load files from geoJSON directory\n  list_df &lt;&lt;- vector(mode = \"list\") #empty list\n  for(fn in files){\n    tmp &lt;- st_read(fn) #read each file in geoJSON dir\n    list_df[[which(fn == files)]] &lt;&lt;- tmp #append to list_df\n  }\n  test_sf &lt;&lt;- bind_rows(list_df) #make long df\n  test_sf$vendor &lt;&lt;- vendor #create vendor column\n  test_sf &lt;&lt;- distinct(test_sf) #script adds multiples, need to debug. hacky solution here\n}\n\n\nload_timeseries(\"link\")\nlink_data &lt;- test_sf\nif (!file.exists(\"../results/link_mon_am.gpkg\")){\n  st_write(link_data, dsn = paste0(\"../results/link_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}\n\n\nload_timeseries(\"lime\")\nlime_data &lt;- test_sf\nif (!file.exists(\"../results/lime_mon_am.gpkg\")){\n  st_write(lime_data, dsn = paste0(\"../results/lime_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}\n\n\nload_timeseries(\"spin\")\nspin_data &lt;- test_sf\nif (!file.exists(\"../results/spin_mon_am.gpkg\")){\n  st_write(spin_data, dsn = paste0(\"../results/spin_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}"
  },
  {
    "objectID": "posts/finalproj/src/start_end_join.html#setup",
    "href": "posts/finalproj/src/start_end_join.html#setup",
    "title": "Data Gathering and Wrangling",
    "section": "",
    "text": "The purpose of this script is to define functions which convert data collected by the python script running on mapping.capital, which are returned as nested JSON files, to geoJSON which can be used for analysis. This file is set up for a dataset of all scooter locations, collected every 15 minutes, from 0600 Eastern to 1000 Eastern on May 1, 2022. The second function defined in this document creates a long dataframe which is a timeseries of scooter locations over time.\n\nvendors = list(\"link\",\"lime\",\"spin\")\n\nThis code block defines a method to convert the nested JSON returned by scooter vendors to a non-nested geoJSON which can be read by sf.\n\njson2geoJSON &lt;- function(vendor){\n  files &lt;- list.files(path = paste(\"../data/monday/\", vendor, \"/morning/json/\", sep = \"\"), pattern=\"*.json\", full.names=TRUE, recursive=FALSE) #list files in directory\n    lapply(files, function(x) {\n      current_data &lt;- fromJSON(txt = x) # load file\n      current_tibble &lt;- as_tibble(current_data$data$bikes) #convert to tibble\n      current_tibble$timestamp_utc &lt;- as_datetime(current_data$last_updated, tz = Sys.timezone()) #create timestamp column\n      current_sf &lt;- st_as_sf(current_tibble, coords = c(\"lon\",\"lat\"), crs = 4326) #coerce to sf\n      if (!file.exists(paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\",\n                                       current_data$last_updated, \"_\", vendor, \".geoJSON\"))){\n      st_write(current_sf, dsn = paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\",\n                                       current_data$last_updated, \"_\", vendor, \".geoJSON\", sep = \"\")\n               , append = FALSE) #write as geoJSON\n      }\n})\n}\n\n\nfor (v in vendors){\n  json2geoJSON(v)\n} #loop through each of link, lime, spin\n\nThis section of code defines a function which creates a timeseries for each scooter and adds a vendor column which can be grouped by in following scripts.\n\n\n\n\n\n\nImportant\n\n\n\nNote 2024-10-11: I cannot emphasize enough how much you SHOULD NOT USE GLOBAL ASSIGNMENT (&lt;&lt;-) in a function. This was some of the first R code I’d ever written, and I’m leaving it for posterity. However, there are MUCH better ways to do this.\n\n\n\nload_timeseries &lt;- function(vendor){\n  files &lt;- list.files(path = paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\", sep = \"\"), pattern=\"*.geoJSON\", full.names=TRUE, recursive=FALSE) #load files from geoJSON directory\n  list_df &lt;&lt;- vector(mode = \"list\") #empty list\n  for(fn in files){\n    tmp &lt;- st_read(fn) #read each file in geoJSON dir\n    list_df[[which(fn == files)]] &lt;&lt;- tmp #append to list_df\n  }\n  test_sf &lt;&lt;- bind_rows(list_df) #make long df\n  test_sf$vendor &lt;&lt;- vendor #create vendor column\n  test_sf &lt;&lt;- distinct(test_sf) #script adds multiples, need to debug. hacky solution here\n}\n\n\nload_timeseries(\"link\")\nlink_data &lt;- test_sf\nif (!file.exists(\"../results/link_mon_am.gpkg\")){\n  st_write(link_data, dsn = paste0(\"../results/link_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}\n\n\nload_timeseries(\"lime\")\nlime_data &lt;- test_sf\nif (!file.exists(\"../results/lime_mon_am.gpkg\")){\n  st_write(lime_data, dsn = paste0(\"../results/lime_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}\n\n\nload_timeseries(\"spin\")\nspin_data &lt;- test_sf\nif (!file.exists(\"../results/spin_mon_am.gpkg\")){\n  st_write(spin_data, dsn = paste0(\"../results/spin_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}"
  },
  {
    "objectID": "posts/finalproj/src/scooter_analysis.html",
    "href": "posts/finalproj/src/scooter_analysis.html",
    "title": "scooter analysis",
    "section": "",
    "text": "This script relies on 3 geopackage outputs from the previous script. It creates a long dataframe, inclusive of all three vendors, groups them by permanent, unique bike ID, and defines a trip as a movement of 50 meters between timestamps, in an attempt to adjust for GPS variability.\n\nlink_data &lt;- st_read(\"../results/link_mon_am.gpkg\")\nlime_data &lt;- st_read(\"../results/spin_mon_am.gpkg\")\nspin_data &lt;- st_read(\"../results/spin_mon_am.gpkg\")\nscooters_raw = bind_rows(link_data, lime_data, spin_data) %&gt;%\n  st_transform(crs = 3857)\nif(!file.exists(\"../results/scooters_raw.gpkg\")){\n  st_write(scooters_raw, dsn = \"../results/scooters_raw.gpkg\")\n}\n\nFirst, we’ll filter out the disabled bike observations, and split the bikes into their own lists in order to apply a function over each bike.\n\nscooters_split &lt;- scooters_raw %&gt;%\n  distinct() %&gt;%\n  filter(is_disabled == 0) %&gt;%\n  group_by(vendor, bike_id) %&gt;%\n  group_split()\n\nThis function defines what constitutes a trip. It is slow. When it’s done, each list element, representing a scooter, will have a field delineating if a given time-interval was part of a trip.\n\nscooters_trip &lt;- lapply(scooters_split, function(df){\n  df %&gt;% mutate(dist_prev = units::drop_units(st_distance(geom, lag(geom), by_element = TRUE)),\n                dist_next = units::drop_units(st_distance(geom, lead(geom), by_element = TRUE)),\n                time_id = row_number(), #this is what allows us to order points for QGIS analysis\n                movement_id = paste(bike_id, \"_\", row_number(), sep = \"\"), #perhaps redundant, but easy solution for moving between R and QGIS\n                trip = case_when(\n                  dist_prev &gt; 50 | dist_next &gt; 50 ~ 1, #define trip based on distance column\n                  TRUE ~ 0))\n})\n\nNote that this is some of the earliest R code I’d ever written. I’m leaving it alone for posterity’s sake, but I’m not sure why I split it into groups and re-bound it again.\n\ntrip_long &lt;- bind_rows(scooters_trip) %&gt;%\n  filter(trip == 1) #filter by only trip points\ntrip_split &lt;- trip_long %&gt;% #split again by trips\n  group_by(bike_id) %&gt;%\n  group_split()\n\n\n#trip_split_id &lt;- lapply(trip_split, function(df){\n#  df %&gt;% mutate(time_id = row_number())\n#})\ntrip_id_long &lt;-bind_rows(trip_split) #bind into trips\nif(!file.exists(\"../results/trip_id_long.gpkg\")){\n  st_write(trip_id_long, \"../results/trip_id_long.gpkg\", append = FALSE)\n}"
  },
  {
    "objectID": "posts/finalproj/src/scooter_analysis.html#setup",
    "href": "posts/finalproj/src/scooter_analysis.html#setup",
    "title": "scooter analysis",
    "section": "",
    "text": "This script relies on 3 geopackage outputs from the previous script. It creates a long dataframe, inclusive of all three vendors, groups them by permanent, unique bike ID, and defines a trip as a movement of 50 meters between timestamps, in an attempt to adjust for GPS variability.\n\nlink_data &lt;- st_read(\"../results/link_mon_am.gpkg\")\nlime_data &lt;- st_read(\"../results/spin_mon_am.gpkg\")\nspin_data &lt;- st_read(\"../results/spin_mon_am.gpkg\")\nscooters_raw = bind_rows(link_data, lime_data, spin_data) %&gt;%\n  st_transform(crs = 3857)\nif(!file.exists(\"../results/scooters_raw.gpkg\")){\n  st_write(scooters_raw, dsn = \"../results/scooters_raw.gpkg\")\n}\n\nFirst, we’ll filter out the disabled bike observations, and split the bikes into their own lists in order to apply a function over each bike.\n\nscooters_split &lt;- scooters_raw %&gt;%\n  distinct() %&gt;%\n  filter(is_disabled == 0) %&gt;%\n  group_by(vendor, bike_id) %&gt;%\n  group_split()\n\nThis function defines what constitutes a trip. It is slow. When it’s done, each list element, representing a scooter, will have a field delineating if a given time-interval was part of a trip.\n\nscooters_trip &lt;- lapply(scooters_split, function(df){\n  df %&gt;% mutate(dist_prev = units::drop_units(st_distance(geom, lag(geom), by_element = TRUE)),\n                dist_next = units::drop_units(st_distance(geom, lead(geom), by_element = TRUE)),\n                time_id = row_number(), #this is what allows us to order points for QGIS analysis\n                movement_id = paste(bike_id, \"_\", row_number(), sep = \"\"), #perhaps redundant, but easy solution for moving between R and QGIS\n                trip = case_when(\n                  dist_prev &gt; 50 | dist_next &gt; 50 ~ 1, #define trip based on distance column\n                  TRUE ~ 0))\n})\n\nNote that this is some of the earliest R code I’d ever written. I’m leaving it alone for posterity’s sake, but I’m not sure why I split it into groups and re-bound it again.\n\ntrip_long &lt;- bind_rows(scooters_trip) %&gt;%\n  filter(trip == 1) #filter by only trip points\ntrip_split &lt;- trip_long %&gt;% #split again by trips\n  group_by(bike_id) %&gt;%\n  group_split()\n\n\n#trip_split_id &lt;- lapply(trip_split, function(df){\n#  df %&gt;% mutate(time_id = row_number())\n#})\ntrip_id_long &lt;-bind_rows(trip_split) #bind into trips\nif(!file.exists(\"../results/trip_id_long.gpkg\")){\n  st_write(trip_id_long, \"../results/trip_id_long.gpkg\", append = FALSE)\n}"
  },
  {
    "objectID": "posts/finalproj/src/hexagons.html",
    "href": "posts/finalproj/src/hexagons.html",
    "title": "Flow Mapping",
    "section": "",
    "text": "This script uses outputs from the previous 3 Rmd files to create a flow map based on the mapdeck package, and bins start and end points by hexes. This workflow is not currently automated for other datasets, but this is a minimal working example to show that it can be expanded to other metro areas.\n\nflow_lines &lt;- readRDS(\"G:/My Drive/GES486/final_proj/results/flow_lines.RDS\") %&gt;%\n  st_transform(crs = 3857)\nbaltimore_bound = counties(state = \"MD\", cb = TRUE) %&gt;%\n  filter(str_detect(GEOID, \"24510|24005\")) %&gt;%\n  st_transform(crs = 3857)\n\n\nbalt_hex &lt;- st_make_grid(baltimore_bound, #create 2000m hex grid\n                         cellsize = c(1600,1600),\n                         what = \"polygons\",\n                         square = FALSE,\n                         crs = 3857\n                         ) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(hex_id = paste(\"hex_\", row_number(), sep = \"\"))\nbalt_hex.intersects &lt;- st_intersects(st_union(baltimore_bound), balt_hex)\nbalt_hex.subset &lt;- balt_hex[balt_hex.intersects[[1]],]\nbalt_centroid &lt;- st_centroid(balt_hex.subset) %&gt;% st_transform(4326) #dataframe of centroids of each hex\nbaltimore_bound &lt;- baltimore_bound %&gt;% st_transform(4326)\ntm_shape(baltimore_bound)+\n  tm_polygons(col = \"#bdbdbd\", border.col = \"black\", lwd = 2)+\n  tm_shape(balt_hex)+\n  tm_borders()\n\nThis next block of code is a mess, but I couldn’t get dplyr and sf to cooperate with some of these dataframes, which contained multiple geometry columns. This can be cleaned up (and probably will be), but for now, this works for the data that I have.\n\nbalt_hex &lt;- balt_hex %&gt;% st_transform(4326)\nbalt_hex.subset &lt;- balt_hex.subset %&gt;% st_transform(4326)\nflow_lines_sf &lt;- st_as_sf(flow_lines)\nflow_lines_sf &lt;- st_transform(flow_lines_sf, crs = 4326)\nflow_lines_sf &lt;- st_set_geometry(flow_lines_sf, flow_lines_sf$start_geom) %&gt;% st_transform(4326)\nstart_points_join &lt;- st_join(flow_lines_sf, balt_hex.subset) #spatial join of start points (set start points as active geometry in previous step)\nflow_lines_sf &lt;- st_set_geometry(flow_lines_sf, flow_lines_sf$end_geom) %&gt;% st_transform(4326)\nend_points_join &lt;- st_join(flow_lines_sf, balt_hex.subset) #in similar fashion, spatial join of end points\nflow_lines_sf$start_hex_id &lt;- start_points_join$hex_id\nflow_lines_sf$end_hex_id &lt;- end_points_join$hex_id\nflow_lines_sf &lt;- left_join(st_drop_geometry(flow_lines_sf), balt_centroid, by = c(\"start_hex_id\" = \"hex_id\")) #join centroid of endpoint hex to row\nflow_lines_sf &lt;- rename(flow_lines_sf, start_centroid = x)\nflow_lines_sf &lt;- left_join(flow_lines_sf, balt_centroid, by = c(\"end_hex_id\" = \"hex_id\")) #likewise for end\nflow_lines_sf &lt;- rename(flow_lines_sf, end_centroid = x)\nflow_lines_sf &lt;- st_sf(flow_lines_sf) %&gt;% st_transform(4326)\nflow_lines_arc &lt;- rename(count(flow_lines_sf, start_hex_id, end_hex_id), wgt = n)\nflow_lines_arc &lt;- flow_lines_arc %&gt;% mutate(scale_weight = (3*wgt))\nflow_lines_arc &lt;- left_join(st_drop_geometry(flow_lines_arc), balt_centroid, by = c(\"end_hex_id\" = \"hex_id\"))\nflow_lines_arc &lt;- left_join(flow_lines_arc, balt_centroid, by = c(\"start_hex_id\" = \"hex_id\"))\nflow_lines_arc &lt;- flow_lines_arc %&gt;% rename(end_centroid = x.x, start_centroid = x.y) %&gt;% st_sf()\nflow_lines_arc &lt;- flow_lines_arc %&gt;% st_transform(4326)\n\nThe following methodology, to count start and end points within hexes, is adapted from Matt Herman’s blog post detailing the counting of trees within NYC census geographies.\n\nstart_hex_count &lt;- count(as_tibble(start_points_join), hex_id) %&gt;% rename(start_in_hex = n)\nend_hex_count &lt;- count(as_tibble(end_points_join), hex_id) %&gt;% rename(end_in_hex = n)\nbalt_hex.subset &lt;- left_join(balt_hex.subset, start_hex_count, by = c(\"hex_id\" = \"hex_id\"))\nbalt_hex.subset &lt;- left_join(balt_hex.subset, end_hex_count, by = c(\"hex_id\" = \"hex_id\"))\nbalt_hex.subset &lt;- balt_hex.subset %&gt;% replace(is.na(.), 0)\nbalt_hex.subset &lt;- balt_hex.subset %&gt;% mutate(total_endpoint = start_in_hex + end_in_hex) \nbalt_hex.subset &lt;- balt_hex.subset %&gt;% st_transform(4326)\n\nThree flow maps can be generated from this data: one symbolized with start points per hex, one with end points per hex, and one with total endpoints per hex.\n\nflow_lines_arc %&gt;%\n  mapdeck(token = Sys.getenv(\"MAPBOX_TOKEN\")) %&gt;%\n  add_arc(origin = \"start_centroid\",\n          destination = \"end_centroid\",\n          stroke_from = \"#000000\",\n          stroke_to = \"#000000\",\n          stroke_width = \"scale_weight\",\n          update_view = TRUE) %&gt;%\n  add_sf(data = balt_hex.subset,\n         fill_colour = \"start_in_hex\",\n         fill_opacity = 180,\n         legend = TRUE\n         )\n\n\nflow_lines_arc %&gt;%\n  mapdeck(token = Sys.getenv(\"MAPBOX_TOKEN\")) %&gt;%\n  add_arc(origin = \"start_centroid\",\n          destination = \"end_centroid\",\n          stroke_from = \"#000000\",\n          stroke_to = \"#000000\",\n          stroke_width = \"scale_weight\",\n          update_view = TRUE) %&gt;%\n  add_sf(data = balt_hex.subset,\n         fill_colour = \"end_in_hex\",\n         fill_opacity = 180,\n         legend = TRUE\n         )\n\n\nflow_lines_arc %&gt;%\n  mapdeck(token = Sys.getenv(\"MAPBOX_TOKEN\")) %&gt;%\n  add_arc(origin = \"start_centroid\",\n          destination = \"end_centroid\",\n          stroke_from = \"#000000\",\n          stroke_to = \"#000000\",\n          stroke_width = \"scale_weight\",\n          update_view = TRUE) %&gt;%\n  add_sf(data = balt_hex.subset,\n         fill_colour = \"total_endpoint\",\n         fill_opacity = 180,\n         legend = TRUE\n         )\n\n\nif(!file.exists(\"../results/flow_lines_arc.RDS\")){\n  saveRDS(flow_lines_arc, file = \"../results/flow_lines_arc.RDS\")\n}\nif(!file.exists(\"../results/balt_hex.RDS\")){\n  saveRDS(balt_hex, file = \"../results/balt_hex.RDS\")\n}"
  },
  {
    "objectID": "posts/finalproj/src/hexagons.html#setup",
    "href": "posts/finalproj/src/hexagons.html#setup",
    "title": "Flow Mapping",
    "section": "",
    "text": "This script uses outputs from the previous 3 Rmd files to create a flow map based on the mapdeck package, and bins start and end points by hexes. This workflow is not currently automated for other datasets, but this is a minimal working example to show that it can be expanded to other metro areas.\n\nflow_lines &lt;- readRDS(\"G:/My Drive/GES486/final_proj/results/flow_lines.RDS\") %&gt;%\n  st_transform(crs = 3857)\nbaltimore_bound = counties(state = \"MD\", cb = TRUE) %&gt;%\n  filter(str_detect(GEOID, \"24510|24005\")) %&gt;%\n  st_transform(crs = 3857)\n\n\nbalt_hex &lt;- st_make_grid(baltimore_bound, #create 2000m hex grid\n                         cellsize = c(1600,1600),\n                         what = \"polygons\",\n                         square = FALSE,\n                         crs = 3857\n                         ) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(hex_id = paste(\"hex_\", row_number(), sep = \"\"))\nbalt_hex.intersects &lt;- st_intersects(st_union(baltimore_bound), balt_hex)\nbalt_hex.subset &lt;- balt_hex[balt_hex.intersects[[1]],]\nbalt_centroid &lt;- st_centroid(balt_hex.subset) %&gt;% st_transform(4326) #dataframe of centroids of each hex\nbaltimore_bound &lt;- baltimore_bound %&gt;% st_transform(4326)\ntm_shape(baltimore_bound)+\n  tm_polygons(col = \"#bdbdbd\", border.col = \"black\", lwd = 2)+\n  tm_shape(balt_hex)+\n  tm_borders()\n\nThis next block of code is a mess, but I couldn’t get dplyr and sf to cooperate with some of these dataframes, which contained multiple geometry columns. This can be cleaned up (and probably will be), but for now, this works for the data that I have.\n\nbalt_hex &lt;- balt_hex %&gt;% st_transform(4326)\nbalt_hex.subset &lt;- balt_hex.subset %&gt;% st_transform(4326)\nflow_lines_sf &lt;- st_as_sf(flow_lines)\nflow_lines_sf &lt;- st_transform(flow_lines_sf, crs = 4326)\nflow_lines_sf &lt;- st_set_geometry(flow_lines_sf, flow_lines_sf$start_geom) %&gt;% st_transform(4326)\nstart_points_join &lt;- st_join(flow_lines_sf, balt_hex.subset) #spatial join of start points (set start points as active geometry in previous step)\nflow_lines_sf &lt;- st_set_geometry(flow_lines_sf, flow_lines_sf$end_geom) %&gt;% st_transform(4326)\nend_points_join &lt;- st_join(flow_lines_sf, balt_hex.subset) #in similar fashion, spatial join of end points\nflow_lines_sf$start_hex_id &lt;- start_points_join$hex_id\nflow_lines_sf$end_hex_id &lt;- end_points_join$hex_id\nflow_lines_sf &lt;- left_join(st_drop_geometry(flow_lines_sf), balt_centroid, by = c(\"start_hex_id\" = \"hex_id\")) #join centroid of endpoint hex to row\nflow_lines_sf &lt;- rename(flow_lines_sf, start_centroid = x)\nflow_lines_sf &lt;- left_join(flow_lines_sf, balt_centroid, by = c(\"end_hex_id\" = \"hex_id\")) #likewise for end\nflow_lines_sf &lt;- rename(flow_lines_sf, end_centroid = x)\nflow_lines_sf &lt;- st_sf(flow_lines_sf) %&gt;% st_transform(4326)\nflow_lines_arc &lt;- rename(count(flow_lines_sf, start_hex_id, end_hex_id), wgt = n)\nflow_lines_arc &lt;- flow_lines_arc %&gt;% mutate(scale_weight = (3*wgt))\nflow_lines_arc &lt;- left_join(st_drop_geometry(flow_lines_arc), balt_centroid, by = c(\"end_hex_id\" = \"hex_id\"))\nflow_lines_arc &lt;- left_join(flow_lines_arc, balt_centroid, by = c(\"start_hex_id\" = \"hex_id\"))\nflow_lines_arc &lt;- flow_lines_arc %&gt;% rename(end_centroid = x.x, start_centroid = x.y) %&gt;% st_sf()\nflow_lines_arc &lt;- flow_lines_arc %&gt;% st_transform(4326)\n\nThe following methodology, to count start and end points within hexes, is adapted from Matt Herman’s blog post detailing the counting of trees within NYC census geographies.\n\nstart_hex_count &lt;- count(as_tibble(start_points_join), hex_id) %&gt;% rename(start_in_hex = n)\nend_hex_count &lt;- count(as_tibble(end_points_join), hex_id) %&gt;% rename(end_in_hex = n)\nbalt_hex.subset &lt;- left_join(balt_hex.subset, start_hex_count, by = c(\"hex_id\" = \"hex_id\"))\nbalt_hex.subset &lt;- left_join(balt_hex.subset, end_hex_count, by = c(\"hex_id\" = \"hex_id\"))\nbalt_hex.subset &lt;- balt_hex.subset %&gt;% replace(is.na(.), 0)\nbalt_hex.subset &lt;- balt_hex.subset %&gt;% mutate(total_endpoint = start_in_hex + end_in_hex) \nbalt_hex.subset &lt;- balt_hex.subset %&gt;% st_transform(4326)\n\nThree flow maps can be generated from this data: one symbolized with start points per hex, one with end points per hex, and one with total endpoints per hex.\n\nflow_lines_arc %&gt;%\n  mapdeck(token = Sys.getenv(\"MAPBOX_TOKEN\")) %&gt;%\n  add_arc(origin = \"start_centroid\",\n          destination = \"end_centroid\",\n          stroke_from = \"#000000\",\n          stroke_to = \"#000000\",\n          stroke_width = \"scale_weight\",\n          update_view = TRUE) %&gt;%\n  add_sf(data = balt_hex.subset,\n         fill_colour = \"start_in_hex\",\n         fill_opacity = 180,\n         legend = TRUE\n         )\n\n\nflow_lines_arc %&gt;%\n  mapdeck(token = Sys.getenv(\"MAPBOX_TOKEN\")) %&gt;%\n  add_arc(origin = \"start_centroid\",\n          destination = \"end_centroid\",\n          stroke_from = \"#000000\",\n          stroke_to = \"#000000\",\n          stroke_width = \"scale_weight\",\n          update_view = TRUE) %&gt;%\n  add_sf(data = balt_hex.subset,\n         fill_colour = \"end_in_hex\",\n         fill_opacity = 180,\n         legend = TRUE\n         )\n\n\nflow_lines_arc %&gt;%\n  mapdeck(token = Sys.getenv(\"MAPBOX_TOKEN\")) %&gt;%\n  add_arc(origin = \"start_centroid\",\n          destination = \"end_centroid\",\n          stroke_from = \"#000000\",\n          stroke_to = \"#000000\",\n          stroke_width = \"scale_weight\",\n          update_view = TRUE) %&gt;%\n  add_sf(data = balt_hex.subset,\n         fill_colour = \"total_endpoint\",\n         fill_opacity = 180,\n         legend = TRUE\n         )\n\n\nif(!file.exists(\"../results/flow_lines_arc.RDS\")){\n  saveRDS(flow_lines_arc, file = \"../results/flow_lines_arc.RDS\")\n}\nif(!file.exists(\"../results/balt_hex.RDS\")){\n  saveRDS(balt_hex, file = \"../results/balt_hex.RDS\")\n}"
  },
  {
    "objectID": "posts/finalproj/index.html",
    "href": "posts/finalproj/index.html",
    "title": "Micromobility in Baltimore",
    "section": "",
    "text": "In 2019, Baltimore City officially adopted its Dockless Vehicle Program, granting permits to micromobility companies Link, Lime, and Spin. The goal of this program was to supplement existing public transit networks, to provide a sustainable alternative for small-scale commuting, and, according to the Baltimore DOT, to be ridden just “for fun!”\nAs the ‘dockless’ name implies, these scooters have nowhere to call home: they’re placed down by local employees of the scooter vendor, remain out for up to weeks at a time, then recollected for maintenance and recharging before being placed out again. Baltimore City lays out clear Deployment Zones and Deployment Equity Zones, in which a certain amount of scooters must remain for the vendors to continue operation in the city.\nThese zones are relatively small compared to the size of the city boundary: do they make scooter distribution truly equitable? Using the vendors’ public API endpoints (another requirement for operation within the city), data was (and is being) collected on scooter locations every fifteen minutes. For this project, I only analyzed times between 6:00 am and 10:00 am for the week of May 1, 2022 and May 7, 2022; but the potential is there for much more detailed analysis.\nThe python script which queries the API endpoints for all three vendors is currently running on Mapping Capital, and all other analysis was done using R statistical software. The scripts I used to manipulate the data collected can be found here.\n\n\n\nFor this analysis, I’ve coined a new unit: people-points. Represented by the total number of jobs as represented in LEHD data added with the total population in a given area, people-points can be used to find centers of transport and human activity as people commute to and from work and home. I’ll be symbolizing my maps based on the number of scooters per person-point (in this case, per 1,000 people-points for better scaling) in order to pick out locations where the number of scooters is not proportional to the number of jobs and residents in an area.\nHere are the resulting maps, made in ggplot2 and arranged using patchwork:\n \nWe can see that, during a week of morning commuting (Monday - Monday), the areas with the most scooter trips per person-point are in Locust Point and the Inner Harbor. More specifically, the two yellowest polygons, which represent the hexes with the highest proportion of trips per person-point, contain the Under Armour main campus. Notably, areas with high percentages of BIPOC individuals are almost all dark purple, showing few numbers of scooter trips per person-point.\nAccording to Spin, they are “committed to being the best possible partner for cities while building the safest, most equitable, and most sustainable mobility solution for the communities [they] serve” (emphasis added). I’d say that, whether intentional or not, the stark contrast of micromobility in marginalized communities compared to whiter communities demonstrates a veritable lack of equitable access, at least in Baltimore. Whether the vehicles are purposely dropped in whiter areas after charging, if they’re used to commute from these areas to Downtown but not back, or any other reason, calling the distribution of scooter trips and availability “equitable” is downright laughable as things stand at the moment.\n\n\n\nAn important reason that cities (including Baltimore) adopt these platforms is to supplement existing transit networks by providing a method for an individual to quickly get to a transit stop before switching to the bus, train, or other method of transit. One interesting area of research in this regard could be to examine how many scooter trips have start and end points that mirror existing transit routes. Could people be using scooters instead of public transit? If so, why? And, as always, more research could be carried out with more data. Since the script collecting our data runs every 15 minutes until the server runs out of storage or the inevitable heat death of the universe (whichever comes first), we could theoretically run this same analysis for time periods of months or even years, given powerful enough hardware."
  },
  {
    "objectID": "posts/finalproj/index.html#introduction",
    "href": "posts/finalproj/index.html#introduction",
    "title": "Micromobility in Baltimore",
    "section": "",
    "text": "In 2019, Baltimore City officially adopted its Dockless Vehicle Program, granting permits to micromobility companies Link, Lime, and Spin. The goal of this program was to supplement existing public transit networks, to provide a sustainable alternative for small-scale commuting, and, according to the Baltimore DOT, to be ridden just “for fun!”\nAs the ‘dockless’ name implies, these scooters have nowhere to call home: they’re placed down by local employees of the scooter vendor, remain out for up to weeks at a time, then recollected for maintenance and recharging before being placed out again. Baltimore City lays out clear Deployment Zones and Deployment Equity Zones, in which a certain amount of scooters must remain for the vendors to continue operation in the city.\nThese zones are relatively small compared to the size of the city boundary: do they make scooter distribution truly equitable? Using the vendors’ public API endpoints (another requirement for operation within the city), data was (and is being) collected on scooter locations every fifteen minutes. For this project, I only analyzed times between 6:00 am and 10:00 am for the week of May 1, 2022 and May 7, 2022; but the potential is there for much more detailed analysis.\nThe python script which queries the API endpoints for all three vendors is currently running on Mapping Capital, and all other analysis was done using R statistical software. The scripts I used to manipulate the data collected can be found here."
  },
  {
    "objectID": "posts/finalproj/index.html#results",
    "href": "posts/finalproj/index.html#results",
    "title": "Micromobility in Baltimore",
    "section": "",
    "text": "For this analysis, I’ve coined a new unit: people-points. Represented by the total number of jobs as represented in LEHD data added with the total population in a given area, people-points can be used to find centers of transport and human activity as people commute to and from work and home. I’ll be symbolizing my maps based on the number of scooters per person-point (in this case, per 1,000 people-points for better scaling) in order to pick out locations where the number of scooters is not proportional to the number of jobs and residents in an area.\nHere are the resulting maps, made in ggplot2 and arranged using patchwork:\n \nWe can see that, during a week of morning commuting (Monday - Monday), the areas with the most scooter trips per person-point are in Locust Point and the Inner Harbor. More specifically, the two yellowest polygons, which represent the hexes with the highest proportion of trips per person-point, contain the Under Armour main campus. Notably, areas with high percentages of BIPOC individuals are almost all dark purple, showing few numbers of scooter trips per person-point.\nAccording to Spin, they are “committed to being the best possible partner for cities while building the safest, most equitable, and most sustainable mobility solution for the communities [they] serve” (emphasis added). I’d say that, whether intentional or not, the stark contrast of micromobility in marginalized communities compared to whiter communities demonstrates a veritable lack of equitable access, at least in Baltimore. Whether the vehicles are purposely dropped in whiter areas after charging, if they’re used to commute from these areas to Downtown but not back, or any other reason, calling the distribution of scooter trips and availability “equitable” is downright laughable as things stand at the moment."
  },
  {
    "objectID": "posts/finalproj/index.html#further-research",
    "href": "posts/finalproj/index.html#further-research",
    "title": "Micromobility in Baltimore",
    "section": "",
    "text": "An important reason that cities (including Baltimore) adopt these platforms is to supplement existing transit networks by providing a method for an individual to quickly get to a transit stop before switching to the bus, train, or other method of transit. One interesting area of research in this regard could be to examine how many scooter trips have start and end points that mirror existing transit routes. Could people be using scooters instead of public transit? If so, why? And, as always, more research could be carried out with more data. Since the script collecting our data runs every 15 minutes until the server runs out of storage or the inevitable heat death of the universe (whichever comes first), we could theoretically run this same analysis for time periods of months or even years, given powerful enough hardware."
  },
  {
    "objectID": "posts/center_of_pop/index.html",
    "href": "posts/center_of_pop/index.html",
    "title": "Center of Population by Ethnicity",
    "section": "",
    "text": "Census data is a big part of what I do and what I’m interested in. Particularly, the Mean Center of Population is something that’s always intrigued me. Consolidating every person in the U.S. to one point is perhaps the peak of descriptive statistics: squishing all of the countless personalities, names, and relationships that make up the nation into a singular point located near the unassuming town of Hartville, Missouri. In fact, if you asked Americans what they think of as the “center of America,” I’d wager many guesses wouldn’t be too far from there.\nObviously, though, that isn’t everyone’s center of the country. There are myriad ways to slice a population; only some of which are captured in the Census: age, gender, income, marital status, race and ethnicity – just to name a few. However, the latter of these categories is often the one which defines a person’s experience in the U.S., for better or for worse – and these experiences often start with the distribution of a group across space.\nWhether discussing the enslavement of African American communities, largely in the Southeast; the immigration of Hispanic families, many of whom end up in Texas, California, Arizona, and New Mexico; or the “Manifest Destiny” of the White man moving westward – all of these happened somewhere, and impact the distribution of descendants today.\nThis project seeks to investigate and elaborate on artifacts of these histories by investigating the Mean Center of Population, split out by summarized race and ethnicity categories collected in the decennial Census. Though this example will only focus on one survey question, this methodology can easily be applied to any of the other ways the Census Bureau stratifies the American population, across any smaller geography."
  },
  {
    "objectID": "posts/center_of_pop/index.html#introduction",
    "href": "posts/center_of_pop/index.html#introduction",
    "title": "Center of Population by Ethnicity",
    "section": "",
    "text": "Census data is a big part of what I do and what I’m interested in. Particularly, the Mean Center of Population is something that’s always intrigued me. Consolidating every person in the U.S. to one point is perhaps the peak of descriptive statistics: squishing all of the countless personalities, names, and relationships that make up the nation into a singular point located near the unassuming town of Hartville, Missouri. In fact, if you asked Americans what they think of as the “center of America,” I’d wager many guesses wouldn’t be too far from there.\nObviously, though, that isn’t everyone’s center of the country. There are myriad ways to slice a population; only some of which are captured in the Census: age, gender, income, marital status, race and ethnicity – just to name a few. However, the latter of these categories is often the one which defines a person’s experience in the U.S., for better or for worse – and these experiences often start with the distribution of a group across space.\nWhether discussing the enslavement of African American communities, largely in the Southeast; the immigration of Hispanic families, many of whom end up in Texas, California, Arizona, and New Mexico; or the “Manifest Destiny” of the White man moving westward – all of these happened somewhere, and impact the distribution of descendants today.\nThis project seeks to investigate and elaborate on artifacts of these histories by investigating the Mean Center of Population, split out by summarized race and ethnicity categories collected in the decennial Census. Though this example will only focus on one survey question, this methodology can easily be applied to any of the other ways the Census Bureau stratifies the American population, across any smaller geography."
  },
  {
    "objectID": "posts/center_of_pop/index.html#methods",
    "href": "posts/center_of_pop/index.html#methods",
    "title": "Center of Population by Ethnicity",
    "section": "Methods",
    "text": "Methods\nLuckily, the Census Bureau is quite transparent with the methods used to calculate the Mean Center: there’s a whole PDF explaining the method from start to finish, including changes over time. The information is dense, so here’s the important stuff.\nFrom 1960 onward, the formula by which the Mean Center is calculated is as follows:\n\\[\n\\bar{\\phi} = \\frac{\\Sigma(w_i\\phi_i)}{\\Sigma w_i}\n\\]\nwhere \\(\\bar{\\phi}\\) is equal to the mean latitude of population\n\\[\n\\bar{\\lambda}=\\frac{\\Sigma w_i\\lambda_i cos(\\phi_i(\\frac{\\pi}{180}))}{\\Sigma w_i cos(\\phi_i(\\frac{\\pi}{180}))}\n\\]\nand \\(\\bar{\\lambda}\\) is the mean longitude.\n\\(\\phi_i\\), \\(\\lambda_i\\), and \\(w_i\\) are the individual latitude, longitude, and weight (in this case, population) of each of the small units used in computation, respectively.\nNow that the mathematical basis for our computations has been established, analysis can begin. Here are the packages we’ll need, and some handy options:\n\nlibrary(tidyverse)\nlibrary(tidycensus) \nlibrary(tigris) \nlibrary(sf) \nlibrary(tmap) \nlibrary(tmaptools) \nlibrary(showtext) \nlibrary(furrr) \nlibrary(data.table) \nlibrary(dtplyr)\noptions(tigris_use_cache = TRUE) \noptions(scipen = 999) \nextrafont::loadfonts(device = \"win\", quiet = T)\n\n\nParallelization\nOperating on Census blocks nationally is no small feat. There are over 11,000,000 blocks; multiply that by 8 population categories and we’re operating on nearly 90 million rows! Thus, parallelization is a must-use in this situation.\n\nfips_list &lt;- unique(fips_codes$state)[1:51] # create list to apply over\nnum_cores &lt;- availableCores()-1 # keep one core free while running to avoid crashes\nplan(multisession, workers = num_cores)\n\n\n\nPopulation groupings\nThe variable codes of interest can change as different summary files are released, but the population categories should be identical across decades. As this analysis was completed before the 2020 Census tabulation was completed, the preliminary redistricting information was used for population counts, as shown below:\n\n# use PL 94-171 data for recent vintages\npop_vars &lt;- c(white = \"P2_005N\",\n              black = \"P2_006N\",\n              aian = \"P2_007N\",\n              asian = \"P2_008N\",\n              hipi = \"P2_009N\",\n              other = \"P2_010N\",\n              two_p = \"P2_011N\",\n              hisp = \"P2_002N\")\n\nThe groupings I’m using here are tabulated below:\n\n\n\n\n\n\n\n\nGroup\nVariable\nCensus Sub-groups\n\n\n\n\nHispanic\nP2_002N\n“Hispanic or Latino”\n\n\nNH White\nP2_005N\n“Not Hispanic or Latino; White alone”\n\n\nNH African-American\nP2_006N\n“Not Hispanic or Latino; Black or African-American alone”\n\n\nNH American and Alaskan Native\nP2_007N\n“Not Hispanic or Latino; American Indian and Alaska Native alone”\n\n\nNH Asian\nP2_008N\n“Not Hispanic or Latino; Asian alone”\n\n\nNH Hawaiian and Pacific Islander\nP2_009N\n“Not Hispanic or Latino; Native Hawaiian and Other Pacific Islander alone”\n\n\nNH Other + NH Two or more\nP2_010N + P2_011N\n“Not Hispanic or Latino; Some Other Race alone” + “Not Hispanic or Latino; Two or more races”\n\n\n\n\n\nData collection\nNow, things start to get real. This looks tricky, but isn’t too hard to understand once you get past the density of it:\n\nb &lt;- reduce(future_map(fips_list, function(x){\n  get_decennial(geography = \"block\", \n                           variables = pop_vars,\n                           year = yr,\n                           state = x,\n                           output = \"tidy\",\n                           geometry = FALSE)}, .progress = TRUE), rbind)\n\nc &lt;- reduce(future_map(fips_list, function(x){blocks(state = x, year = yr) %&gt;%\n     st_drop_geometry() %&gt;%\n     mutate(across(.cols = starts_with(\"INTPTLON\"), .fns = as.numeric, .names = \"lon\")) %&gt;%\n     mutate(across(.cols = starts_with(\"INTPTLAT\"), .fns = as.numeric, .names = \"lat\")) %&gt;%\n     select(c(5,last_col(offset = 1), last_col()))}, .progress = TRUE), rbind)\n\nfuture:::ClusterRegistry(\"stop\")\n\nfwrite(as.data.table(b), paste0(\"../data/pop_data_2020.csv\"))\nfwrite(as.data.table(c), paste0(\"../data/latlon_2020.csv\"))\n\n\nIterate get_decennial over each of our states in fips_list, using future_map() to take advantage of multiple CPU cores\n\nCollect population data for each of our pop_vars defined earlier, and keep the data in tidy format for grouping and summarization\n\nreduce() the resulting set of dataframes into one long dataframe using rbind as the .f argument\nIterate blocks() over each of our states in fips_list, again using future_map()\nClean up the internal point coordinates with as.numeric(), and select only the important columns\nAgain, reduce() the set of dataframes into one, with an eighth as many rows as b.\n\nFinally, we can stop our cluster and write the two objects to disk, using data.table’s fwrite(). These can be read for subsequent analyses, or chunked to avoid memory issues in the computation steps.\n\n\nComputation\nNow, we need to join our tables in order to weight the internal point of each block with its respective population. Note the usage of {dtplyr} here: it allows us to keep writing in tidyverse syntax while gaining the performance of {data.table}.\nSince block-level population centers aren’t available through {tigris}, we will use the “internal point” latitude and longitude. The internal point of a geography is often the centroid: if a geography is shaped such that the centroid would fall outside its boundary, the internal point is placed as close to the internal centroid of the geography as possible, preferably on land. We’ll be using the formulas from earlier, after translating them from mathematician to R.\n\nb &lt;- lazy_dt(fread(paste0(\"../data/pop_data_\", yr, \".csv\")))\nc &lt;- lazy_dt(fread(paste0(\"../data/latlon_\", yr, \".csv\")))\no &lt;- b %&gt;%\n  pivot_wider(names_from = variable, values_from = value) %&gt;%\n  mutate(other = other + two_p) %&gt;%\n  select(-c(two_p)) %&gt;%\n  pivot_longer(cols = 3:9, names_to = \"variable\")\n\nj &lt;- left_join(o, c, by = c(\"GEOID\" = paste0(\"GEOID\", str_sub(yr, 3,4)))) %&gt;%\n  mutate(elat = value * lat,\n         elon = value * lon * cos(lat * (pi/180)),\n         denom = value*cos(lat * (pi/180)))\n\n\ng &lt;- j %&gt;% \n  group_by(variable) %&gt;%\n  summarize(tlat = sum(elat, na.rm = TRUE)/sum(value, na.rm = TRUE),\n            tlon = sum(elon, na.rm = TRUE)/sum(denom, na.rm = TRUE),\n            pop = sum(value))\n\ncc &lt;- as.data.table(j) %&gt;%\n  summarize(tlat = sum(elat, na.rm = TRUE)/sum(value, na.rm = TRUE),\n            tlon = sum(elon, na.rm = TRUE)/sum(denom, na.rm = TRUE),\n            pop = sum(value))\n\ng &lt;- as.data.table(g)\n\nI chose to read b and c back in from disk, so that I could skip querying the Census API in the future. If you still have b and c in your environment, and have enough memory to continue to use them, that’s also an option. Otherwise, you’ll have to get creative with chunking your files on disc to allow for smaller object sizes in memory. Note that because of how the Mean Center formula is written, it can nest again and again – one way to do this with less memory could be to compute the center of each state at a time, then aggregate those into regions, then aggregate those to the national level. Luckily for me, 32GB of RAM was just about enough to hold all of the objects in-memory.\nThis code chunk, thanks to the magic of dtplyr, is a bit easier to read than the previous, while allowing the performance gains of data.table and the readability of dplyr. The steps are, generally, as follows:\n\nPivot b wider so that each population group has its own column, and sum other and two_p, then drop two_p, and pivot longer again to create o\nJoin o and c by their GEOIDs, then calculate their individual components of weight to create j\nGroup j by each category variable, then summarize to tlat and tlon (total mean lat and long, respectively)\ncreate cc based on j, but without grouping by variables. This should yield the total Mean Center of Population\nCoerce g to a data.table, which computes the steps on the lazy_dt object and creates a memory object\n\nThese two objects, g and cc, contain the coordinates of the Mean Center of Population for each ethnicity category we’ve defined. Now, there’s only one thing left to do before we can map them – we need to make them into sf objects.\n\n\nCreating spatial data\n\ns &lt;- g %&gt;%\n  st_as_sf(coords = c(\"tlon\",\"tlat\")) %&gt;% st_set_crs(4326) %&gt;% st_transform(6350)\n\ncc &lt;- cc %&gt;%\n  st_as_sf(coords = c(\"tlon\", \"tlat\")) %&gt;% st_set_crs(4326) %&gt;% st_transform(6350)\n\nlso &lt;- vector(mode = 'list', length = nrow(s))\n\nfor (row in 1:nrow(s)){\n  cs &lt;- data.frame(st_coordinates(cc)[1], st_coordinates(cc)[2]) %&gt;%\n  rename(p_x = 1,\n         p_y = 2) %&gt;%\n  mutate(seq = 1)\n  cs &lt;- cbind(g[,c(1,4)],cs)\n  \n  ss &lt;- s[row,] %&gt;%\n    rowwise() %&gt;%\n    mutate(p_x = unlist(geometry)[1],\n           p_y = unlist(geometry)[2]) %&gt;%\n    mutate(seq = 2) %&gt;%\n    st_drop_geometry() %&gt;%\n    ungroup()\n  \n  ap &lt;- rbind(ss,cs[row,]) %&gt;%\n    arrange(variable, seq)\n  \n  m &lt;- as.matrix(ap[order(ap$seq),\n                    c(\"p_x\",\"p_y\")])\n  \n  ls &lt;- st_linestring(m) %&gt;%\n    st_sfc() %&gt;%\n    st_sf() %&gt;%\n    mutate(variable = ap$variable[1],\n           pop = ap$pop[1]) %&gt;%\n    bind_cols()\n  \n  lso &lt;- lso %&gt;% bind_rows(ls)\n}\n\nlsp &lt;- lso %&gt;% st_set_crs(6350)\n\nI really wanted to have lines connecting each of my points (in g) to the total Mean Center of Population (cc). To do so, I had to create a LINESTRING for each point, with the start point being at cc and the endpoint being at each point in g. After converting g to an sf object (s), I created a list the same length as s. By creating an empty object with the same length as the object I’m iterating over, I remove all headroom R needs to allocate more space to the incoming objects. Josiah Parry has an awesome YouTube video discussing this topic, for those who think looping is slow in R (like I used to!).\n\n\nPlotting\nFinally, it’s time to plot our information. Now that everything is an sf object, this is pretty trivial – I chose tmap for my plotting, but you could just as easily use ggplot2 for static maps, or, if you prefer interactive, mapgl or mapview.\nI chose to symbolize my points by color, representing a population group; and by size, representing the population of that group. However, the values for the minimum population and the maximum population are really far apart! The points barely show up on the low end. So, first, I’ll scale the population values in s to reduce the magnitude of the difference between the most- and least-populous groups.\n\ns &lt;- s %&gt;% mutate(pop_pct = pop/sum(pop),\n                  log_pct = abs(log(pop_pct)),\n                  normalized_log_pct = 0.1 + (log_pct - max(log_pct)) / (min(log_pct) - max(log_pct)) * (0.7 - 0.1))\n\nLet’s put it on a map!\n\nilh &lt;- palette(c(rgb(114,153,67, maxColorValue = 255),\n                 rgb(148,79,161, maxColorValue = 255),\n                 rgb(76,196,144, maxColorValue = 255),\n                 rgb(185,74,115, maxColorValue = 255),\n                 rgb(193,158,60, maxColorValue = 255),\n                 rgb(104,123,210, maxColorValue = 255),\n                 rgb(185,85,61, maxColorValue = 255)))\n\nplot_fips &lt;- unique(fips_codes$state_code)[1:51]\nplot_fips &lt;- plot_fips[!plot_fips %in% c(\"02\", \"15\", \"72\", \"78\")]\n\nus &lt;- states(cb = TRUE, year = yr) %&gt;% filter(if (yr == 2010) STATE %in% plot_fips else\n                                                 STATEFP %in% plot_fips) %&gt;%\n  st_transform(6350) #weird artifact in tigris means that column names don't match\n\nt &lt;- tm_shape(us, bbox = bb(us, ext = 1.1))+\n  tm_polygons(border.col = \"#aaaaaa\", lwd = 0.75, col = \"#5b5b5b\")+\n  tm_shape(lsp)+\n  tm_lines(col = \"variable\", palette = ilh, legend.col.show = FALSE, lwd = 1.5, legend.lwd.show = FALSE)+\n  tm_shape(s)+\n  tm_symbols(col = \"variable\", title.col = \"Race/Ethnicity\", size = \"normalized_log_pct\", border.col = \"#bdbdbd\", palette = ilh, border.lwd = 1, legend.col.show = FALSE, legend.size.show = FALSE)+\n  tm_shape(cc)+\n  tm_dots(col = \"#1286c4\", shape = 24, title = \"Total center of population\", size = 0.6, legend.show = FALSE, border.lwd = 1, border.col = \"#bdbdbd\")+\n  tm_add_legend(type = \"symbol\", \n    labels = c(\"American/Alaskan Native\", \"Asian\", \"African-American\", \"Hawaiian/Pacific Islander\", \"Hispanic\", \"Other/Two or more\", \"White\"),\n    col = ilh,\n    border.col = \"#bdbdbd\",\n    title = \"Ethnicity\",\n    size = 0.4)+\n  tm_add_legend(type = \"symbol\",\n                shape = 24,\n                col = \"#1286c4\",\n                size = 0.6,\n                border.col = \"#bdbdbd\",\n                labels = \"Total center of population\")+\n  tm_layout(main.title = \"Center of population by race/ethnicity\",\n            main.title.fontfamily = \"Manrope\",\n            main.title.fontface = 2,\n            main.title.size = 2,\n            bg.color = \"#3b3b3b\",\n            legend.outside = TRUE,\n            legend.text.color = \"#bdbdbd\", \n            legend.text.fontfamily = \"Manrope\", \n            legend.title.color = \"#bdbdbd\",\n            legend.title.fontface = 2,\n            legend.title.size = 1.5,\n            legend.title.fontfamily = \"Manrope\", \n            legend.text.size = 0.75,\n            legend.position = c(0,0.25),\n            outer.bg.color = \"#3b3b3b\",\n            frame = FALSE,\n            main.title.color = \"#bdbdbd\")+\n  tm_credits(paste0(\"Decennial census data, \", yr, \"\\nGraphic by Harrison DeFord (@oonuttlet)\"),\n             position = c(0,0.08),\n             col = \"#bdbdbd\",\n             fontfamily = \"Open Sans\",\n             size = 0.62)\n\nt\n\nif (!file.exists(paste0(\"../bin/center_of_pop_natl_\",yr,\"_intpt.png\"))){\n  tmap_save(t, paste0(\"../bin/center_of_pop_natl_\",yr,\"_intpt.png\"), dpi = 1200)\n}"
  },
  {
    "objectID": "posts/center_of_pop/index.html#discussion",
    "href": "posts/center_of_pop/index.html#discussion",
    "title": "Center of Population by Ethnicity",
    "section": "Discussion",
    "text": "Discussion\nThe completed map is visible below. Some trends are immediately apparent: the African-American point is dragged to the southeast, and the Hawaiian/Pacific Islander point is in the Pacific. I invite you to look over the map and think about how the historic distribution of these populations could shape their modern-day Mean Center of Population.\n\n\n\nThe final product.\n\n\nFinally, I’d like to investigate the change in Mean Centers from 2010-2020. I’ve computed this analysis only for 2010 and 2020, though in theory, it would be trivial to calculate for earlier years (at least since 1960). The gif shown below demonstrates the shift in Mean Centers between 2010 and 2020. Much of the shift is probably systemic shifts – 2020 was the first year in which the decennial census was administered mostly online. Again, I invite you to think about what other variables might be leading to the shift between decades. Of note: COVID-19 didn’t impact the U.S. until around March 12, 2020 – the Census Bureau aims to count every person on or around April 1 of the given vintage. Thus, COVID-19 may have affected the Census response rates, but is unlikely to have had an impact on net gain or loss of a given area that recently after its first impacts in the U.S.\n\n\n\nComparison of 2010 and 2020 Mean Centers"
  },
  {
    "objectID": "posts/668_project_proposal/index.html",
    "href": "posts/668_project_proposal/index.html",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "",
    "text": "Wildland fire and its relationship with land management is a contentious topic, particularly in a post-Industrial-Revolution climate. Since the 1935 advent of the Forest Service’s “ten-A.M. policy,” fire suppression has been the standard in land management: protection of economic interests including timber stands (for logging) and grasslands (for grazing) has been priority one (Westover 2023). However, this philosophy has shifted slowly as the ecological benefits of wildland fire have shown themselves in various ecosystems – lodgepole pines need the heat to open their cones, wild lupine needs fire to clear mid-canopy plants that outcompete it for sunlight, and the clearing of ladder fuels prevent future catastrophic crowning fires. Today, the Forest Service has a new position on wildland fire – when possible, let it burn.\nHowever, this is often impossible when fires burn into areas occupied by humans; protection of life, property, and prosperity becomes priority one. The groups most vulnerable to wildfire impacts are those who are marginalized in spaces across America: Davies et al. (2018) found that census tracts with majority Black, Hispanic, and Native American populations were about twice as vulnerable to wildfire impacts when compared to other census tracts."
  },
  {
    "objectID": "posts/668_project_proposal/index.html#introduction",
    "href": "posts/668_project_proposal/index.html#introduction",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "",
    "text": "Wildland fire and its relationship with land management is a contentious topic, particularly in a post-Industrial-Revolution climate. Since the 1935 advent of the Forest Service’s “ten-A.M. policy,” fire suppression has been the standard in land management: protection of economic interests including timber stands (for logging) and grasslands (for grazing) has been priority one (Westover 2023). However, this philosophy has shifted slowly as the ecological benefits of wildland fire have shown themselves in various ecosystems – lodgepole pines need the heat to open their cones, wild lupine needs fire to clear mid-canopy plants that outcompete it for sunlight, and the clearing of ladder fuels prevent future catastrophic crowning fires. Today, the Forest Service has a new position on wildland fire – when possible, let it burn.\nHowever, this is often impossible when fires burn into areas occupied by humans; protection of life, property, and prosperity becomes priority one. The groups most vulnerable to wildfire impacts are those who are marginalized in spaces across America: Davies et al. (2018) found that census tracts with majority Black, Hispanic, and Native American populations were about twice as vulnerable to wildfire impacts when compared to other census tracts."
  },
  {
    "objectID": "posts/668_project_proposal/index.html#goals",
    "href": "posts/668_project_proposal/index.html#goals",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Goals",
    "text": "Goals\nMy goals for this project are to:\n\nFacilitate easier gathering of data related to wildland fire\nInvestigate the historic relationships between wildland fire and people by incorporating Census demographic data\nImprove my functional and literate programming skills in R\n\nHopefully, anyone who is affected by historic wildfires would find this research useful; whether that be individuals, jurisdictions, land stewards, or conservationists. I don’t plan on computing an index; doing so would require a discussion of what a “negative social characteristic” is. The output object will contain the change in social variables between pre- and post-event years, but a single “impact index” will not be calculated."
  },
  {
    "objectID": "posts/668_project_proposal/index.html#data",
    "href": "posts/668_project_proposal/index.html#data",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Data",
    "text": "Data\nThere are two types of data I’m planning to use as proofs of concept for this project. The first is historic federal interagency wildland fire data, which is maintained by the National Interagency Fire Center (NIFC) and distributed via ArcGIS Online products including hosted feature layers and an Esri Hub site. The bulk of the data can be accessed from within R by using the arcgislayers package to query the NIFC ArcGIS REST server (link).\nIn particular, the Interagency Fire Perimeter History layer will be used to access historic wildland fire perimeters. The earliest recorded fire in this dataset started in 708 A.D., determined by carbon dating – however, a more realistic subset of wildland fires will be selected based on overlapping temporal ranges with Census data. Full documentation and metadata for the layer can be found via its NIFC ArcGIS Hub page, which provides a more readable interface for traditional GIS users.\nThe second data repository I’ll use to support my work is the Census Bureau’s ACS 5-year estimates, which contain annual estimates of demographic information from 2009 onward. ACS information will be accessed using tidycensus and tigris, which will allow for spatial operations including spatial filtering and population-weighted interpolation. Information regarding ACS data can be found at the Census Bureau’s ACS homepage.\nThe more unfamiliar and nonstandard of these sources is likely the NIFC perimeter data. It can be collected in the field, drawn in a desktop GIS, or derived from imagery, depending on the size of the fire, terrain, and available staffing, among other considerations. Luckily, it contains a field describing the method of collection for each fire; this can be used to determine which perimeters are more accurate for our uses. In addition, each row contains information corresponding to the responsible agency, event names, and unique event IDs.\nAs all of these sources are maintained and distributed by the federal government, they have generally permissive licensing:\n\nNIFC: https://data-nifc.opendata.arcgis.com/datasets/nifc::interagencyfireperimeterhistory-all-years-view/about (click on “View license details” on the right side of the page)\nCensus Bureau API: https://www.census.gov/data/developers/about/terms-of-service.html\n\nMy usage of these data sources is without warranty, and my work is not endorsed nor certified by either the U.S. Census Bureau or the National Interagency Fire Center.\nAs a demonstration, I’ll use the ten largest New Mexican wildfires in the NIFC historic perimeter database between 2010 and 2020.\n\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(arcgislayers)\nlibrary(tidyverse)\nlibrary(patchwork)\noptions(tigris_use_cache = T)\n\n\n# Example of tigris:\nnm &lt;- tigris::states() |&gt;\n  dplyr::filter(STUSPS == \"NM\") |&gt;\n  st_as_sfc()\n# Example of arcgislayers:\nnm_fires &lt;- arcgislayers::arc_read(r\"(https://services3.arcgis.com/T4QMspbfLg3qTGWY/ArcGIS/rest/services/InterAgencyFirePerimeterHistory_All_Years_View/FeatureServer/0)\",\n  where = \"FIRE_YEAR BETWEEN 2010 AND 2020\",\n  filter_geom = nm\n) |&gt;\n  st_make_valid() |&gt;\n  distinct(toupper(INCIDENT), .keep_all = T) |&gt;\n  slice_max(\n    order_by = Shape__Area,\n    n = 10\n  ) |&gt;\n  select(-`toupper(INCIDENT)`) |&gt;\n  mutate(COMMENTS = str_trunc(COMMENTS, 35))\nmapview::mapview(nm_fires)"
  },
  {
    "objectID": "posts/668_project_proposal/index.html#planned-approach",
    "href": "posts/668_project_proposal/index.html#planned-approach",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Planned approach",
    "text": "Planned approach\nMy goal is to create a function which will show the impact of nonspecific disasters on social and economic variables by extracting Census data via tidycensus. The Census Bureau has released OnTheMap for Emergency Management, which gathers relevant social variables for ongoing events. However, I don’t believe there exists a version which shows the values before and after historic events.\nMy approach is to allow a user to input an sf object and a year, and get back ACS estimates on variables including race/ethnicity, total population, and household income, by default. The variables option will allow a user to input their own variables, in tidycensus syntax, which will be summarized accordingly. By naming the variables in the variables option, the user can specify the summarization operation to be used: for example, mean_household_size is aggregated using mean(), and median_household_income is aggregated using median(). Variable names that do not follow these string patterns will be treated as extensive, and will be aggregated using sum().\nThe exciting thing about this approach is that a user can input a novel sf object with a field containing a year, and compute the demographics for all tracts it intersected with for a given range before and after that year. So, a polygon detailing a disaster could be drawn using mapedit, for example, and its effect on the surrounding demographics would be computed.\nUsing the nm_fires object created earlier, here is a brief demonstration of how the function works. The output object is an sf object, which contains all of the original incident fields – however, to the right side of this table, the requested demographic info is appended.\nWhen output = \"tidy\" (the default), differences are not computed and the information is returned in a long-format table, with a row per point in time (e.g., before and after the event) and per variable. When output = \"wide\" is specified, the absolute differences between the two time points are computed, and the original variables are not returned to aid with GIS and cartography.\n\nnm_fires_impact &lt;- get_incident_impacts(\n  data = nm_fires,\n  id_col = OBJECTID,\n  year = FIRE_YEAR\n)\n\nggplot(filter(nm_fires_impact, str_starts(variable, \"pop\"))) +\n  geom_line(aes(x = fct_rev(pt_in_time), y = computed, group = variable, color = variable)) +\n  xlab(\"Point in time\") +\n  ylab(\"Estimated population\") +\n  labs(title = \"Impacts of wildfires on population data\") +\n  scale_color_discrete(\"Race/Ethnicity Category\", labels = c(\n    \"Hispanic/Latino\",\n    \"Non-Hispanic American Native\",\n    \"Non-Hispanic Asian\",\n    \"Non-Hispanic African-American\",\n    \"Non-Hispanic Pacific Islander\",\n    \"Non-Hispanic Other\",\n    \"Non-Hispanic Two or More\",\n    \"Non-Hispanic White\",\n    \"Total Population\"\n  )) +\n  facet_wrap(~INCIDENT, scales = \"free_y\", nrow = 5, ncol = 2)\n\n\n\n\n\n\n\n\n\nnm_fires_impact_wide &lt;- get_incident_impacts(\n  data = nm_fires,\n  id_col = OBJECTID,\n  year = FIRE_YEAR,\n  output = \"wide\"\n)\n\nmapview::mapview(nm_fires_impact_wide,\n  zcol = \"median_hhi\"\n)\n\n\n\n\n\nA presentation with more details, given in class, can be found here."
  },
  {
    "objectID": "posts/381_proj/src/ipums_gsl_tract.html",
    "href": "posts/381_proj/src/ipums_gsl_tract.html",
    "title": "IPUMS data collection and processing",
    "section": "",
    "text": "This document describes the process through which population estimates were made and related to water area values extracted from Google Earth.\n\nlibrary(ipumsr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidycensus)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(tidyverse)\n\nFirst, a CSV containing information from the 1980, 1990, 2000, and 2010 censuses, all normalized to 2010 geographies, is loaded using read_nhgis() from {ipumsr}, and only counties bordering the Great Salt Lake were kept.\n\nut_ts &lt;- read_nhgis(\"../data/nhgis0005_ts_geog2010_tract.csv\") %&gt;%\n  filter(STATE == \"Utah\") %&gt;%\n  filter(str_detect(COUNTY, \"Utah County|Salt Lake County|Davis|Weber|Box Elder|Tooele\")) %&gt;%\n  select(c(1,6,8:17)) %&gt;%\n  pivot_longer(cols = tidyselect::starts_with(\"CL8AA\"))%&gt;%\n  filter(name %in% c(\"CL8AA1990\",\"CL8AA2000\",\"CL8AA2010\",\"CL8AA2020\"))\n\nA linear regression model was generated for each census tract. This allows for the interpolation of population data between the 10-year census recurrence interval, before the ACS was begun in 2009.\nThe estimated annual populations of each tract were summarized to the level of the entire geography, then ACS 5-year data was used from 2009 onwards. These data sources were combined to yield continuous annual population estimates from 1986 to 2020.\n\nut_ts.pred &lt;- list()\nfor (i in 1:length(ut_model)){\n  ut_ts.pred[[i]] = data.frame(year = c(1986:2009), \n                               GISJOIN = first(ut_ts.split[[i]]$GISJOIN), \n                               COUNTYA = first(ut_ts.split[[i]]$COUNTYA))\n  ut_ts.pred[[i]]$tpop = round(predict(ut_model[[i]], newdata = ut_ts.pred[[i]]))\n} \nut_ts.pred &lt;- bind_rows(ut_ts.pred)\n\nut_ts.sum &lt;- ut_ts.pred %&gt;%\n  group_by(year)%&gt;%\n  mutate(tpop = sum(tpop)) %&gt;%\n  ungroup()%&gt;%\n  distinct(year, .keep_all = TRUE) %&gt;%\n  select(year, tpop)\n\nut_acs.list &lt;- lapply(c(2010:2020), function(y) get_acs(geography = \"county\", state = \"UT\", year = y, survey = \"acs5\", variables = \"B01001_001\", cache_table = TRUE) %&gt;% mutate(year = y)) %&gt;%\n  bind_rows()\nut_acs.filtered &lt;- ut_acs.list %&gt;% filter(str_detect(NAME, \"Utah County|Salt Lake County|Davis|Weber|Box Elder|Tooele\")) %&gt;%\n  group_by(year) %&gt;%\n  mutate(tpop = sum(estimate))%&gt;%\n  ungroup()%&gt;%\n  distinct(year, .keep_all = TRUE) %&gt;%\n  select(year, tpop)\n\nut_tpop &lt;- bind_rows(ut_ts.sum, ut_acs.filtered)\n\nFinally, the population estimates were joined to the water area values obtained from Google Earth Engine, and written to a .csv for use in further processing.\n\nyear_area_gsl &lt;- read_csv(\"../data/year_area_gsl.csv\")\nvars_joined &lt;- left_join(ut_tpop, year_area_gsl) %&gt;% mutate(area_km = (area/1e6), tpop_mn = (tpop/1e6))\nvars_joined &lt;- na.omit(vars_joined)\nwrite_csv(vars_joined, file = \"../data/vars_joined.csv\")"
  },
  {
    "objectID": "posts/381_proj/index.html",
    "href": "posts/381_proj/index.html",
    "title": "The Great Shrinking Lake",
    "section": "",
    "text": "This presentation was given as my final project for my remote sensing course at UMBC. It uses R and Google Earth Engine to analyze the relationship between the changing water area of the Great Salt Lake since 1986 and the growing population of the Wasatch Front and the Utah Valley. You can view the animated timeseries of the lake here.\nThe report was limited in scope due to limited time to work: obviously, there are more factors contributing to a shrinking lake area than just population growth. However, in USGS’s 2015 Circular 1441 ranked Utah second of all states in per capita domestic water usage, so to say that the population of the Valley is not a major reason the Lake is losing water yearly would be undeniably false.\nLinear correlation tests run in R yielded a significantly negative correlation of y ~ 0.0016x and an R-squared value of 0.78 (p &lt;&lt; 0.001). In context, according to this model, for every 100,000 people that move to the Salt Lake area, 160 km2 of water surface are lost.\nThe R scripts for this report are available here and the Earth Engine code is available here\nNOTE: Google has removed Landsat Collection 1 data from GEE; this script will need to be modified before use (see this GEE article for more details)."
  },
  {
    "objectID": "posts/381_proj/index.html#final-project-for-ges-381-remote-sensing",
    "href": "posts/381_proj/index.html#final-project-for-ges-381-remote-sensing",
    "title": "The Great Shrinking Lake",
    "section": "",
    "text": "This presentation was given as my final project for my remote sensing course at UMBC. It uses R and Google Earth Engine to analyze the relationship between the changing water area of the Great Salt Lake since 1986 and the growing population of the Wasatch Front and the Utah Valley. You can view the animated timeseries of the lake here.\nThe report was limited in scope due to limited time to work: obviously, there are more factors contributing to a shrinking lake area than just population growth. However, in USGS’s 2015 Circular 1441 ranked Utah second of all states in per capita domestic water usage, so to say that the population of the Valley is not a major reason the Lake is losing water yearly would be undeniably false.\nLinear correlation tests run in R yielded a significantly negative correlation of y ~ 0.0016x and an R-squared value of 0.78 (p &lt;&lt; 0.001). In context, according to this model, for every 100,000 people that move to the Salt Lake area, 160 km2 of water surface are lost.\nThe R scripts for this report are available here and the Earth Engine code is available here\nNOTE: Google has removed Landsat Collection 1 data from GEE; this script will need to be modified before use (see this GEE article for more details)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "harrison deford",
    "section": "",
    "text": "forklift certified; aspiring geographer\n📍 baltimore\n\nPORTFOLIO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCenter of Population by Ethnicity\n\n\n\n\n\n\npersonal\n\n\nhuman geography\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicromobility in Baltimore\n\n\n\n\n\n\numbc\n\n\ntransportation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying the local impact of wildfire on demographic factors\n\n\n\n\n\n\n\n\n\n\n\nHarrison DeFord\n\n\n\n\n\n\n\n\n\n\n\n\nThe Great Shrinking Lake\n\n\n\n\n\n\numbc\n\n\nremote sensing\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about harrison",
    "section": "",
    "text": "My name is Harrison DeFord, and this website serves as a collection of documents showing my journey as I learn more about space, place, and what we can learn from them.\nI primarily write code in R, which I began learning in early 2022. I’m a geographer by training, but am also particularly interested in riding my bike (slowly), transportation and development in urban areas, and how geography shapes these topics.\nI’m currently a senior data analyst as part of the BCSTAT team in the Baltimore County Executive Office, where I serve as a spatial data guru, leading the development of internal and external tools, including the Social Determinants of Health (SDOH) web experience and the Baltimore County Food Pantry Locator. Working in the public sector is immensely fulfilling, and BCSTAT’s unique position in the government structure means I have experience with all sorts of topics, including animal services, code enforcement, fire/EMS, tax credits, transportation, and more.\nI graduated from UMBC with a BS in Geography and Environmental Systems in December 2022, and have since returned as a graduate student pursuing my MS, again in Geography and Environmental Systems. At UMBC I’ve had the opportunity to work with several professors in a lab environment.\n\nThe UMBC labs I’ve worked with\n\n\n\n\n\n\n\n\nProfessor\nLaboratory\nResearch Topics\nDates\n\n\n\n\nDr. Dillon Mahmoudi\nMapping Capital\nAir quality, Census data, micromobility, critical geography\nAug 2021 - Present\n\n\nDr. Matthew Fagan\nEarth from Above\nRemote Sensing, Machine Learning, Reforestation\nAug 2022 - Mar 2022\n\n\n\nDuring my time at UMBC, I’ve also had the chance to work with other organizations on various projects, including the Partnership for a Healthier America and UMBC’s IS department as a Data Science Scholar.\nIf you’re interested in working with me on a project, please reach out at hdeford1 [at] umbc [dot] edu. I currently don’t have much bandwidth, but am always willing to listen to new research ideas!"
  },
  {
    "objectID": "posts/381_proj/src/index.html",
    "href": "posts/381_proj/src/index.html",
    "title": "harrison deford",
    "section": "",
    "text": "Download the R script used to calculate yearly population estimates.\nDownload the R script used to visualize the area and population estimates and generate the linear regression."
  },
  {
    "objectID": "posts/381_proj/src/ipums_gsl_tract_display.html",
    "href": "posts/381_proj/src/ipums_gsl_tract_display.html",
    "title": "GSL data display",
    "section": "",
    "text": "library(dplyr) \n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2) \nlibrary(gganimate) \nlibrary(stringr) \nlibrary(ggpubr)\n\nThis document handles the charts used in the original presentation. First, it reads data created in the previous script:\n\nvars_joined &lt;- readr::read_csv(\"../data/vars_joined.csv\") \n\nRows: 34 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): year, tpop, area, area_km, tpop_mn\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvars_joined.playa &lt;- vars_joined %&gt;% \n  mutate(new_playa = lag(area_km)-area_km)\n\nIt creates some labels, in a very inefficient manner – if I were to rewrite this code today, I’d use something like format(x, big.mark = \",\").\n\nvars_joined.label &lt;- vars_joined.playa %&gt;%\n  mutate(tpop_lab = paste0(\"Total population: \", str_sub(tpop, 1,1), \",\", str_sub(tpop, 2,4), \",\", str_sub(tpop, 5,7)), \n         area_lab = paste0(\"Water area: \", str_sub(round(area_km), 1,1), \",\", str_sub(round(area_km), 2,4), \" sq. km\"), \n         playa_lab = paste0(\"New playa area: \", round(new_playa), \" sq. km\"))\n\nFirst, we set up an empty set of axes on which we’ll plot our data:\n\np1 &lt;- ggplot(data = vars_joined, aes(x = year, y = tpop_mn))+ \n  labs(title = \"Population of counties bordering the Great Salt Lake, Utah, 1986-2021\",\n       x = \"Year\", y = \"Population (millions)\")+\n  theme(panel.background = element_blank(),\n        axis.title.x = element_text(size = 16), \n        axis.title.y = element_text(size = 16, hjust = 0.4, vjust = 1), \n        axis.text.x = element_text(color = \"black\", size = 14), \n        axis.text.y = element_text(color = \"black\", size = 14), \n        plot.title = element_text(hjust = 0, vjust = 5, size = 18, face = \"bold\"), \n        legend.key = element_blank(), \n        legend.position = \"top\", \n        plot.margin = margin(1, 5, 0.1,0.1, 'cm')) \n\np1\n\n\n\n\n\n\n\n\nThen, we’ll use those axes to create an animated chart showing change in population over time.\n\np2 &lt;- p1 + \n  geom_point(data = vars_joined.label, aes(x = year, y = tpop_mn), color = \"#ffb303\", size = 3)+ \n  geom_line(data = vars_joined.label, aes(x = year, y = tpop_mn), color = \"#ffb303\", size = 1)+ \n  geom_text(data = vars_joined.label, aes(x = year + .1, y = tpop_mn), \n            label = vars_joined.label$tpop_lab, \n            hjust = -0.1, \n            vjust = -1, \n            size = 4, \n            show.legend = FALSE)+\n  transition_reveal(year)+ \n  ease_aes('linear')+ \n  coord_cartesian(clip = 'off') \n\nanimate(p2, start_pause = 24, end_pause = 24)\n\n\nJust like before, we’ll create an empty set of axes, this time for water area:\n\np3 &lt;- ggplot(data = vars_joined, aes(x = year, y = area_km))+\n  labs(title = \"Water area of the Great Salt Lake, 1986-2021\", x = \"Year\", y = \"Water area,\"~km^2)+\n  theme(panel.background = element_blank(), \n        axis.title.x = element_text(size = 16), \n        axis.title.y = element_text(size = 16, hjust = 0.4, vjust = 1), \n        axis.text.x = element_text(color = \"black\", size = 14), \n        axis.text.y = element_text(color = \"black\", size = 14), \n        plot.title = element_text(hjust = 0, vjust = 5, size = 18, face = \"bold\"), \n        legend.key = element_blank(), \n        legend.position = \"top\", \n        plot.margin = margin(1, 5, 0.1,0.1, 'cm')) \n\np3\n\n\n\n\n\n\n\n\nAnd, again like before, we’ll animate over those axes.\n\np4 &lt;- p3 + \n  geom_point(data = vars_joined, aes(x = year, y = area_km), color = \"#03afff\", size = 3)+ \n  geom_line(data = vars_joined, aes(x = year, y = area_km), color = \"#03afff\", size = 1)+ \n  geom_text(data = vars_joined.label, aes(x = year + .1, y = area_km), \n            label = vars_joined.label$area_lab, \n            hjust = -0.1, \n            vjust = -1, \n            size = 4, \n            show.legend = FALSE)+\n  transition_reveal(year)+\n  ease_aes('linear')+\n  transition_reveal(year)+\n  coord_cartesian(clip = 'off')\n\nanimate(p4, start_pause = 24, end_pause = 24)\n\nif (!file.exists(\"../images/utah_gsl_area.gif\")){\n  anim_save(filename = \"../images/utah_gsl_area.gif\", \n            fps = 24, nframes = 207, \n            animation = p4, \n            start_pause = 24, \n            end_pause = 24, \n            height = 1080, \n            width = 1920, \n            res = 180, \n            renderer = gifski_renderer())\n}\n\n\nOne last time, this time creating a linear regression model:\n\np8 &lt;- ggplot(data = vars_joined.label, aes(x = tpop, y = area_km))+ \n  labs(title = \"Water area of the Great Salt Lake compared with population, 1986-2021\", x = \"Population of counties bordering the Great Salt Lake\", y = \"Water area,\"~km^2)+ \n  theme(panel.background = element_blank(), \n        axis.title.x = element_text(size = 16), \n        axis.title.y = element_text(size = 16, hjust = 0.5, vjust = 1), \n        axis.text.x = element_text(color = \"black\", size = 14), \n        axis.text.y = element_text(color = \"black\", size = 14), \n        plot.title = element_text(hjust = 0, vjust = 5, size = 18, face = \"bold\"), \n        legend.key = element_blank(), \n        legend.position = \"top\", \n        plot.margin = margin(1, 0.1, 0.1,0.1, 'cm')) \n\np8\n\n\n\n\n\n\n\n\n\np7 &lt;- p8+ geom_point(data = vars_joined, aes(x = tpop, y = area_km), color = \"black\", size = 2)+ \n  geom_smooth(method=lm, se = FALSE, color = \"#a82727\")+ \n  stat_regline_equation(label.x = 2300000, label.y = 6500, aes(label = ..eq.label..))+ \n  stat_regline_equation(label.x = 2400000, label.y = 6400, aes(label = ..rr.label..)) \n\np7\n\nif (!file.exists(\"../images/pop_vs_area.png\")){\n  ggsave(p7, filename = \"../images/pop_vs_area.png\", width = 1920, height = 1080, dpi = 180, units = 'px')\n}\n\n\n\nlinreg &lt;- lm(area_km ~ tpop, data = vars_joined)\n\nsummary(linreg)\n\n\nCall:\nlm(formula = area_km ~ tpop, data = vars_joined)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-719.1 -243.1  -26.2  262.2  494.2 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.879e+03  2.847e+02   27.67  &lt; 2e-16 ***\ntpop        -1.601e-03  1.489e-04  -10.75 3.73e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 322.5 on 32 degrees of freedom\nMultiple R-squared:  0.7832,    Adjusted R-squared:  0.7764 \nF-statistic: 115.6 on 1 and 32 DF,  p-value: 3.731e-12"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#why-this-is-important",
    "href": "posts/668_project_proposal/presentation/index.html#why-this-is-important",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Why this is important",
    "text": "Why this is important\n\nSince the advent of aircraft, land management policy has been to put out wildfires immediately, even in the face of ecosystem needs\n\nProtection of people, property, and prosperity (Westover 2023)\nSpecies such as lodgepole pine (P. contorta) and wild lupine (L. perennis) require fire"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#why-this-is-important-1",
    "href": "posts/668_project_proposal/presentation/index.html#why-this-is-important-1",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Why this is important",
    "text": "Why this is important\n\nDavies et al. (2018) found that majority Black, Hispanic, and Native American areas were disproportionately impacted by wildfire\n\nAbout twice as vulnerable to wildfire impacts when compared to other census tracts"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#initial-goals",
    "href": "posts/668_project_proposal/presentation/index.html#initial-goals",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Initial goals",
    "text": "Initial goals\nMy initial goals for this project were to:\n\nIdentify sources of spatial data related to historic wildfire perimeters\nUse ACS data to estimate impacts of disasters between Decennial Census years\nCreate functions generic enough to apply to any disaster (or other) boundary"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#wildfire-perimeters",
    "href": "posts/668_project_proposal/presentation/index.html#wildfire-perimeters",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Wildfire perimeters",
    "text": "Wildfire perimeters\nHistoric wildfire GIS information is maintained by the National Interagency Fire Center (NIFC).\nNIFC data are distributed through ArcGIS Online products:\n\nHosted feature layers\nEsri Hub (“Open Data”) site\n\nThese sources can be queried using the {arcgislayers} package (Parry 2024)."
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#census-information",
    "href": "posts/668_project_proposal/presentation/index.html#census-information",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Census information",
    "text": "Census information\nThe functions I’ve written for this project lean heavily on Kyle Walker’s {tigris} (2024) and {tidycensus} (2024) packages.\n\nIncrease performance by only downloading required counties and tracts\nProvide an easy spatial filter for NIFC data, by using the filter_geom argument in arc_read()"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#define-the-new-mexico-boundary",
    "href": "posts/668_project_proposal/presentation/index.html#define-the-new-mexico-boundary",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Define the New Mexico boundary",
    "text": "Define the New Mexico boundary\n\nnm &lt;- tigris::states() |&gt;\n  filter(STUSPS == \"NM\")"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#get-the-wildfire-perimeters",
    "href": "posts/668_project_proposal/presentation/index.html#get-the-wildfire-perimeters",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Get the wildfire perimeters",
    "text": "Get the wildfire perimeters\n\nnm_fires &lt;- arcgislayers::arc_read(r\"(https://services3.arcgis.com/T4QMspbfLg3qTGWY/ArcGIS/rest/services/InterAgencyFirePerimeterHistory_All_Years_View/FeatureServer/0)\",\n  where = \"FIRE_YEAR BETWEEN 2010 AND 2020\",\n  filter_geom = st_as_sfc(nm)\n) |&gt;\n  st_make_valid(geos_method = \"valid_linework\") |&gt;\n  distinct(toupper(INCIDENT), .keep_all = T) |&gt;\n  slice_max(\n    order_by = Shape__Area,\n    n = 10\n  ) |&gt;\n  select(-`toupper(INCIDENT)`) |&gt;\n  mutate(COMMENTS = str_trunc(COMMENTS, 35))\n\n\n\n\n\n\n\nNote\n\n\nThe NIFC data is much less clean than the Census data; we have to validate geometry and de-duplicate incidents. In addition, many of the comment-writers had lots to say – we’ll truncate the COMMENTS field to 35 characters."
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#put-them-together",
    "href": "posts/668_project_proposal/presentation/index.html#put-them-together",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Put them together",
    "text": "Put them together\n\n\n\n\n\n\nThe cumulative area of these fires is 3368.7 square miles. However, there are other impacts that are not apparent by only looking at the burned land area."
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#determining-a-temporal-range",
    "href": "posts/668_project_proposal/presentation/index.html#determining-a-temporal-range",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Determining a temporal range",
    "text": "Determining a temporal range\n\nAn analyst must define what is meant by “before” and “after” an event, particularly when using 5-year ACS estimates\n\nDefault setting is 2 years before, 3 years after\nCan be changed using years_before and years_after parameters, but must still be &gt;= 5 years"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#finding-intersecting-counties-and-tracts",
    "href": "posts/668_project_proposal/presentation/index.html#finding-intersecting-counties-and-tracts",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Finding intersecting counties and tracts",
    "text": "Finding intersecting counties and tracts\n\nTo calculate the difference in demographic characteristics before and after an event, we’ll use census tracts\n\nDetermine which tracts an event boundary intersects with before and after it occurs\nget_counties_all_years() and get_tracts_all_years()\n\nUse st_filter() to determine which tracts an event boundary intersects with"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#relating-tract-data-to-events",
    "href": "posts/668_project_proposal/presentation/index.html#relating-tract-data-to-events",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Relating tract data to events",
    "text": "Relating tract data to events\n\nEach event in the input sf object is spatially joined to the tracts it intersects with before and after\n\nA set of unique state, county, and year combinations is generated to improve performance\nmap_data() is applied to iterate get_acs() across these combinations\n\nCommon variables as well as user-defined variables"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#summarize-acs-data-and-join-to-events",
    "href": "posts/668_project_proposal/presentation/index.html#summarize-acs-data-and-join-to-events",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Summarize ACS data and join to events",
    "text": "Summarize ACS data and join to events\n\nMost impactful events are large enough to affect more than one Census tract\n\nEstimates are aggregated across event boundaries\n\nSupport for extensive and intensive variables is included"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#acs-variables",
    "href": "posts/668_project_proposal/presentation/index.html#acs-variables",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "ACS Variables",
    "text": "ACS Variables\n\n\n\nVariable starts with…\nFunction to be computed\n\n\n\n\nmean\nmean()\n\n\nmedian\nmedian()\n\n\nAll other strings\nsum()\n\n\n\nDefault variables include:\n\nc(\n      \"pop_total\" = \"B03002_001\", # total population\n      \"pop_nh_white\" = \"B03002_003\", # non-Hispanic White\n      \"pop_nh_black\" = \"B03002_004\", # non-Hispanic Black\n      \"pop_nh_aian\" = \"B03002_005\", # non-Hispanic American Indian/Alaskan Native\n      \"pop_nh_asian\" = \"B03002_006\", # non-Hispanic Asian\n      \"pop_nh_hipi\" = \"B03002_007\", # non-Hispanic Native Hawaiian And Other Pacific Islander\n      \"pop_nh_other\" = \"B03002_008\", # non-Hispanic Some Other Race Alone\n      \"pop_nh_two\" = \"B03002_009\", # non-Hispanic Two or More Races\n      \"pop_hisp\" = \"B03002_012\", # Hispanic or Latino\n      \"median_hhi\" = \"B19013_001\", # median household income\n      \"mean_hh_size\" = \"B25010_001\", # household size\n      \"sch_enrollment\" = \"B14001_002\" # school enrollment\n    )"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#calculating-difference",
    "href": "posts/668_project_proposal/presentation/index.html#calculating-difference",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Calculating difference",
    "text": "Calculating difference\nThe final output object is an sf object, returned in either \"tidy\" (the default) or \"wide\" format.\n\n\n\n\n\n\n\noutput = \"tidy\"\noutput = \"wide\"\n\n\n\n\nTwice as long as nrow(data) * length(variables), with one field containing the point-in-time, one containing the corresponding ACS vintage year, one containing the variable, and one containing the estimate\nSame length as nrow(data), with a column returned for each variable requested and one denoting the years of the ACS vintages used\n\n\nNo differences are calculated; the user must group and perform calculations\nAbsolute differences are calculated; this avoids divide by zero error\n\n\nCan be used for side-by-side comparison maps (such as a slider map) comparing an event to itself\nUseful for single choropleth maps comparing events to one another"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#tidy-output",
    "href": "posts/668_project_proposal/presentation/index.html#tidy-output",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "“tidy” output",
    "text": "“tidy” output\n\nnm_fires_impact &lt;- get_incident_impacts(\n  data = nm_fires,\n  id_col = OBJECTID,\n  year = FIRE_YEAR\n)\n\n\n\n\n\n\n\n\n\nOBJECTID\nMAP_METHOD\nDATE_CUR\nCOMMENTS\nGEO_ID\nIRWINID\nUNQE_FIRE_ID\nFIRE_YEAR\nLOCAL_NUM\nINCIDENT\nUNIT_ID\nPOO_RESP_I\nFEATURE_CA\nGIS_ACRES\nUSER_NAME\nSOURCE\nAGENCY\nFIRE_YEAR_INT\nShape__Area\nShape__Length\nFORID\nOTHERID\npt_in_time\ndata_yr\nvariable\ncomputed\n\n\n\n\n24800\nDigitized From Hardcopy Orthophoto quad\n201202230000\nNA\n{066DE785-7868-44DC-B1FF-AD0600D05F53}\nNA\n2011-AZASF-000020\n2011\n2011-020\nWallow\nNA\nNA\nWildfire Final Fire Perimeter\n538051.6\nNA\nUSFS\nUSFS\n2011\n3161860839\n725941.9\nNA\nNA\nbefore\n2009\nmean_hh_size\n2.83\n\n\n24800\nDigitized From Hardcopy Orthophoto quad\n201202230000\nNA\n{066DE785-7868-44DC-B1FF-AD0600D05F53}\nNA\n2011-AZASF-000020\n2011\n2011-020\nWallow\nNA\nNA\nWildfire Final Fire Perimeter\n538051.6\nNA\nUSFS\nUSFS\n2011\n3161860839\n725941.9\nNA\nNA\nbefore\n2009\nmedian_hhi\n33802.00\n\n\n24800\nDigitized From Hardcopy Orthophoto quad\n201202230000\nNA\n{066DE785-7868-44DC-B1FF-AD0600D05F53}\nNA\n2011-AZASF-000020\n2011\n2011-020\nWallow\nNA\nNA\nWildfire Final Fire Perimeter\n538051.6\nNA\nUSFS\nUSFS\n2011\n3161860839\n725941.9\nNA\nNA\nbefore\n2009\npop_hisp\n3446.00\n\n\n24800\nDigitized From Hardcopy Orthophoto quad\n201202230000\nNA\n{066DE785-7868-44DC-B1FF-AD0600D05F53}\nNA\n2011-AZASF-000020\n2011\n2011-020\nWallow\nNA\nNA\nWildfire Final Fire Perimeter\n538051.6\nNA\nUSFS\nUSFS\n2011\n3161860839\n725941.9\nNA\nNA\nbefore\n2009\npop_nh_aian\n4764.00\n\n\n24800\nDigitized From Hardcopy Orthophoto quad\n201202230000\nNA\n{066DE785-7868-44DC-B1FF-AD0600D05F53}\nNA\n2011-AZASF-000020\n2011\n2011-020\nWallow\nNA\nNA\nWildfire Final Fire Perimeter\n538051.6\nNA\nUSFS\nUSFS\n2011\n3161860839\n725941.9\nNA\nNA\nbefore\n2009\npop_nh_asian\n69.00"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#wide-output",
    "href": "posts/668_project_proposal/presentation/index.html#wide-output",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "“wide” output",
    "text": "“wide” output\n\nnm_fires_impact_wide &lt;- get_incident_impacts(\n  data = nm_fires,\n  id_col = OBJECTID,\n  year = FIRE_YEAR,\n  output = \"wide\"\n)\n\n\n\n\n\n\n\n\n\nOBJECTID\nMAP_METHOD\nDATE_CUR\nCOMMENTS\nGEO_ID\nIRWINID\nUNQE_FIRE_ID\nFIRE_YEAR\nLOCAL_NUM\nINCIDENT\nUNIT_ID\nPOO_RESP_I\nFEATURE_CA\nGIS_ACRES\nUSER_NAME\nSOURCE\nAGENCY\nFIRE_YEAR_INT\nShape__Area\nShape__Length\nFORID\nOTHERID\nacs_vintage\nmean_hh_size\nmedian_hhi\npop_hisp\npop_nh_aian\npop_nh_asian\npop_nh_black\npop_nh_hipi\npop_nh_other\npop_nh_two\npop_nh_white\npop_total\nsch_enrollment\n\n\n\n\n24800\nDigitized From Hardcopy Orthophoto quad\n201202230000\nNA\n{066DE785-7868-44DC-B1FF-AD0600D05F53}\nNA\n2011-AZASF-000020\n2011\n2011-020\nWallow\nNA\nNA\nWildfire Final Fire Perimeter\n538051.56\nNA\nUSFS\nUSFS\n2011\n3161860839\n725941.9\nNA\nNA\n2009 to 2014\n0.1250000\n7718\n-434\n392\n-31\n98\n2\n2\n-101\n-4266\n-4338\n-1479\n\n\n29377\nInfrared Interpretation\n201212310000\nNA\n{D542FB27-0045-4CF3-A49F-A5913F690B67}\nNA\n2012-NMGNF-000200\n2012\n000200\nWhitewater-Baldy Complex\nNMGNF\nNMGNF\nWildfire Final Fire Perimeter\n297801.12\nNA\nUSFS\nUSFS\n2012\n1730419827\n1116513.1\nNA\nNA\n2010 to 2015\n0.1466667\n-1097\n-236\n-101\n12\n20\n0\n0\n50\n-290\n-545\n215\n\n\n13348\nOther\n201107210000\nNA\n{7BF09AF5-9A22-4A7A-B64B-2C959495DAE4}\nNA\n2011-NMSNF-000099\n2011\n000099\nLas Conchas\nNMSNF\nNMSNF\nWildfire Final Fire Perimeter\n156593.62\nNA\nUSFS\nUSFS\n2011\n967018464\n440306.1\nNA\nNA\n2009 to 2014\n-0.0175000\n-4409\n60\n1068\n-33\n-3\n19\n108\n-75\n-27\n1117\n551\n\n\n29401\nInfrared Interpretation\n201307112315\nKCFAST Report Incorrectly Has Si...\n{DF4D9D65-A856-472F-A827-60C4E2268D61}\nNA\n2013-NMGNF-000008\n2013\n000008\nSilver\nNMGNF\nNMGNF\nWildfire Final Fire Perimeter\n138705.45\nNA\nUSFS\nUSFS\n2013\n799345275\n375737.0\nNA\nNA\n2011 to 2016\n-0.1866667\n-15806\n52\n-25\n20\n27\n0\n19\n151\n-960\n-716\n-266\n\n\n46314\nGPS - Uncorrected Data\n201202010000\nData received from fire team member\n{77E85B4F-7FFB-44F5-B0DC-B95568BCDD16}\nNA\n2011-NMLNF-000425\n2011\n000425\nDonaldson\nNMN5S\nNMN5S\nWildfire Final Fire Perimeter\n96129.21\nNA\nUSFS\nUSFS\n2011\n558682949\n206076.9\nNA\nNA\n2009 to 2014\n0.6400000\n-167\n737\n12\n-8\n-26\n-5\n0\n-114\n1211\n1807\n670"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#and-some-maps",
    "href": "posts/668_project_proposal/presentation/index.html#and-some-maps",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "And some maps!",
    "text": "And some maps!\n\n\n\n\n\n\nUsing output = \"tidy\" allows us to compare one variable easily over time."
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#and-more-maps",
    "href": "posts/668_project_proposal/presentation/index.html#and-more-maps",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "And more maps!",
    "text": "And more maps!\n\n\n\n\n\n\nUsing output = \"wide\" allows us to compare differences in one variable easily across multiple events."
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#findings",
    "href": "posts/668_project_proposal/presentation/index.html#findings",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Findings",
    "text": "Findings\nThe Wallow Fire (2011) had the most severe impact on the Hispanic and Latino populations based on this methodology.\n\nHispanic/Latino population decreased by 442 individuals between 2009 and 2014\nTotal population decreased by over 4,300 people\nSchool enrollment decreased by 1,479 pupils."
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#further-applications",
    "href": "posts/668_project_proposal/presentation/index.html#further-applications",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Further applications",
    "text": "Further applications\nThe way that I’ve constructed this project allows it to be easily extended to other topics, not just wildfire:\n\nOther urban disasters, like flooding\nPositive events (like a new transit station opening)\nA novel polygon, to see change in a region over time"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#difficulties-and-limitations",
    "href": "posts/668_project_proposal/presentation/index.html#difficulties-and-limitations",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Difficulties and limitations",
    "text": "Difficulties and limitations\n\nData is returned by tigris with different column names depending on the vintage used\nACS estimates are only available from 2009, so impacts can only be calculated for events occurring in 2010 onwards\nCalculated differences (when output = \"wide\") can only be absolute\n\nIf an initial state is 0, then a relative difference cannot be calculated"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#what-the-project-does-well",
    "href": "posts/668_project_proposal/presentation/index.html#what-the-project-does-well",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "What the project does well",
    "text": "What the project does well\n\nWritten in such a way it can be converted to a package relatively easily\nProvides a good counterpart to existing disaster planning tools\n\nOnTheMap for Emergency Management\n\nDoes not calculate an impact index\n\nWhat is a negative social characteristic?"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#one-last-example",
    "href": "posts/668_project_proposal/presentation/index.html#one-last-example",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "One last example",
    "text": "One last example\nThe Washington Metropolitan Area Transit Authority (WMATA) opened a series of metro rail stations through Northern Virginia on July 26, 2014, in the Phase 1 extension of the Silver Line (“Dulles Corridor Metrorail Project” 2015). These stations include Greensboro, McLean, Spring Hill, Tysons, and Wiehle–Reston East.\nUsing the above methodology, we can investigate the question: what was the impact of opening these stations on the surrounding areas?"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#using-transit-data",
    "href": "posts/668_project_proposal/presentation/index.html#using-transit-data",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Using transit data",
    "text": "Using transit data\nAccording to the U.S. Department of Transportation, most pedestrians are willing to walk a quarter to a half a mile to a transit stop (2008). We can compute a polygon for the upper bound of this estimate using the mapboxapi package.\n\nwmata_stops &lt;- st_read(\"data/wmata_silver_p1.gpkg\", layer = \"wmata_silver_p1\")\nhf_mi_stops &lt;- mb_isochrone(wmata_stops, \n                            profile = \"walking\", \n                            distance = c(805), \n                            id_column = \"stop_id\") |&gt;\n  mutate(stop_yr = 2014) |&gt;\n  left_join(st_drop_geometry(select(wmata_stops, c(stop_id, stop_name))),\n            by = join_by(id == stop_id))"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#using-transit-data-1",
    "href": "posts/668_project_proposal/presentation/index.html#using-transit-data-1",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Using transit data",
    "text": "Using transit data"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#calculating-differences-over-time",
    "href": "posts/668_project_proposal/presentation/index.html#calculating-differences-over-time",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Calculating differences over time",
    "text": "Calculating differences over time\n\nwmata_impact &lt;- get_incident_impacts(hf_mi_stops,\n                                     id,\n                                     stop_yr,\n                                     output = \"tidy\")"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#calculating-differences-over-time-1",
    "href": "posts/668_project_proposal/presentation/index.html#calculating-differences-over-time-1",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "Calculating differences over time",
    "text": "Calculating differences over time\n\nwmata_impact_wide &lt;- get_incident_impacts(hf_mi_stops,\n                                     id,\n                                     stop_yr,\n                                     output = \"wide\")"
  },
  {
    "objectID": "posts/668_project_proposal/presentation/index.html#references",
    "href": "posts/668_project_proposal/presentation/index.html#references",
    "title": "Quantifying the local impact of wildfire on demographic factors",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nDavies, Ian P., Ryan D. Haugo, James C. Robertson, and Phillip S. Levin. 2018. “The Unequal Vulnerability of Communities of Color to Wildfire.” Edited by Julia A. Jones. PLOS ONE 13 (11): e0205825. https://doi.org/10.1371/journal.pone.0205825.\n\n\n“Dulles Corridor Metrorail Project.” 2015. https://www.transit.dot.gov/sites/fta.dot.gov/files/docs/2015-03_-_Dulles_Phase_1_-_Comprehensive_Monthly_Report.pdf.\n\n\nNabors, Dan, Robert Schneider, Dalia Leven, Kimberly Lieberman, and Colleen Mitchell. 2008. “Pedestrian Safety Guide for Transit Agencies.” safety.fhwa.dot.gov; U.S. Department of Transportation. https://safety.fhwa.dot.gov/ped_bike/ped_transit/ped_transguide/ch4.cfm.\n\n\nParry, Josiah. 2024. “Arcgislayers: An Interface to ArcGIS Data Services.” https://CRAN.R-project.org/package=arcgislayers.\n\n\nWalker, Kyle. 2024. “Tigris: Load Census TIGER/Line Shapefiles.” https://CRAN.R-project.org/package=tigris.\n\n\nWalker, Kyle, and Matt Herman. 2024. “Tidycensus: Load US Census Boundary and Attribute Data as ’Tidyverse’ and ’Sf’-Ready Data Frames.” https://CRAN.R-project.org/package=tidycensus.\n\n\nWestover, Robert Hudson. 2023. “Only You | US Forest Service.” US Forest Service. https://www.fs.usda.gov/about-agency/features/only-you."
  },
  {
    "objectID": "posts/center_of_pop/src/index.html",
    "href": "posts/center_of_pop/src/index.html",
    "title": "Centers of Population by Race/Ethnicity",
    "section": "",
    "text": "Every decennial census, the U.S. Census Bureau releases an updated version of the Mean Center of Population, which can be interpreted as the “center of gravity” of the U.S. population. The goal of this project is to calculate the mean center of population per ethnic group captured in the decennial census.\nFor simplification, the groups of interest in this analysis are:\n\n\n\n\n\n\n\nGroup\nCensus Sub-groups\n\n\n\n\nHispanic\n“Hispanic or Latino”\n\n\nNH White\n“Not Hispanic or Latino; White alone”\n\n\nNH African-American\n“Not Hispanic or Latino; Black or African-American alone”\n\n\nNH American and Alaskan Native\n“Not Hispanic or Latino; American Indian and Alaska Native alone”\n\n\nNH Asian\n“Not Hispanic or Latino; Asian alone”\n\n\nNH Hawaiian and Pacific Islander\n“Not Hispanic or Latino; Native Hawaiian and Other Pacific Islander alone”\n\n\nNH Other\n“Not Hispanic or Latino; Some Other Race alone”\n\n\nNH Two or more\n“Not Hispanic or Latino; Two or more races”\n\n\n\n\nLibraries\nFirst, we’ll load required libraries:\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(showtext)\nlibrary(furrr)\nlibrary(data.table)\nlibrary(dtplyr)\noptions(tigris_use_cache = TRUE)\noptions(scipen = 999)\nextrafont::loadfonts(device = \"win\", quiet = T)\n\nrm(list = ls())\nyr = 2010\n\n\n\nParallelization\nSince we’re computing at the block level (11 million blocks) across 7 groups, we’ll have a dataframe with over 70 million rows at its highest. I actually had to upgrade to 32GB of memory to run this analysis: we’re going to create a {furrr} multisession using all but one of our CPU threads. In my case, that’s 11 threads.\n\nfips_list &lt;- unique(fips_codes$state)[1:51]\nnum_cores &lt;- availableCores()-1\nplan(multisession, workers = num_cores)\n\nNow that our multisession is planned, we can start gathering our census data. The {tidycensus} package provides a fantastic interface to the USCB API, but at the block level we can only collect data one state at a time. However, by using future_map() to iterate over our list of state FIPS codes (fips_list) created earlier, we can gather 11 states worth of data simultaneously until we’ve obtained population counts for all 50 states, then reduce() them into one very long dataframe.\n\n\nCensus Data Collection\nNote that we gather tabular data (b) and spatial data (c) separately, to be stored in two different files on our machine. This code block takes my machine 20-30 minutes to run – luckily, once you’ve run it once, you shouldn’t have to run it again thanks to the power of fwrite()!\n\n## Not run:\n\nif (yr &lt; 2020){\n  pop_vars &lt;- c(white = \"P005003\",\n                black = \"P005004\",\n                aian = \"P005005\",\n                asian = \"P005006\",\n                hipi = \"P005007\",\n                other = \"P005008\",\n                two_p = \"P005009\",\n                hisp = \"P005010\")\n} else {\n  pop_vars &lt;- c(white = \"P2_005N\",\n                black = \"P2_006N\",\n                aian = \"P2_007N\",\n                asian = \"P2_008N\",\n                hipi = \"P2_009N\",\n                other = \"P2_010N\",\n                two_p = \"P2_011N\",\n                hisp = \"P2_002N\")\n}\n\nb &lt;- reduce(future_map(fips_list, function(x){\n  get_decennial(geography = \"block\", \n                           variables = pop_vars,\n                           year = yr,\n                           state = x,\n                           output = \"tidy\",\n                           geometry = FALSE)}, .progress = TRUE), rbind)\n\nc &lt;- reduce(future_map(fips_list, function(x){blocks(state = x, year = yr) %&gt;%\n     st_drop_geometry() %&gt;%\n     mutate(across(.cols = starts_with(\"INTPTLON\"), .fns = as.numeric, .names = \"lon\")) %&gt;%\n     mutate(across(.cols = starts_with(\"INTPTLAT\"), .fns = as.numeric, .names = \"lat\")) %&gt;%\n     select(c(5,last_col(offset = 1), last_col()))}, .progress = TRUE), rbind)\n\nfuture:::ClusterRegistry(\"stop\")\n\nfwrite(as.data.table(b), paste0(\"../data/pop_data_\", yr, \".csv\"))\nfwrite(as.data.table(c), paste0(\"../data/latlon_\", yr, \".csv\"))\n\n## End(Not run)\n\n\n\nCalculations\nNow, we need to join our tables in order to weight the internal point of each block with its respective population. Note the usage of {dtplyr} here: I wanted to keep writing in tidyverse syntax while gaining the performance of {data.table}. What a fantastic package!\nThe formula used is derived from this document. Since block-level population centers aren’t available through {tigris}, we will use the “internal point” latitude and longitude. The internal point of a geography is often the centroid: if a geography is shaped such that the centroid would fall outside its boundary, the internal point is placed as close to the internal centroid of the geography as possible, preferably on land.\n\nb &lt;- lazy_dt(fread(paste0(\"../data/pop_data_\", yr, \".csv\")))\nc &lt;- lazy_dt(fread(paste0(\"../data/latlon_\", yr, \".csv\")))\no &lt;- b %&gt;%\n  pivot_wider(names_from = variable, values_from = value) %&gt;%\n  mutate(other = other + two_p) %&gt;%\n  select(-c(two_p)) %&gt;%\n  pivot_longer(cols = 3:9, names_to = \"variable\")\n\nj &lt;- left_join(o, c, by = c(\"GEOID\" = paste0(\"GEOID\", str_sub(yr, 3,4)))) %&gt;%\n  mutate(elat = value * lat,\n         elon = value * lon * cos(lat * (pi/180)),\n         denom = value*cos(lat * (pi/180)))\n\n\ng &lt;- j %&gt;% \n  group_by(variable) %&gt;%\n  summarize(tlat = sum(elat, na.rm = TRUE)/sum(value, na.rm = TRUE),\n            tlon = sum(elon, na.rm = TRUE)/sum(denom, na.rm = TRUE),\n            pop = sum(value))\n\ncc &lt;- as.data.table(j) %&gt;%\n  summarize(tlat = sum(elat, na.rm = TRUE)/sum(value, na.rm = TRUE),\n            tlon = sum(elon, na.rm = TRUE)/sum(denom, na.rm = TRUE),\n            pop = sum(value))\n\ng &lt;- as.data.table(g)\n\n\ns &lt;- g %&gt;%\n  st_as_sf(coords = c(\"tlon\",\"tlat\")) %&gt;% st_set_crs(4326) %&gt;% st_transform(6350)\n\ncc &lt;- cc %&gt;%\n  st_as_sf(coords = c(\"tlon\", \"tlat\")) %&gt;% st_set_crs(4326) %&gt;% st_transform(6350)\n\nlso &lt;- vector(mode = 'list', length = nrow(s))\n\nfor (row in 1:nrow(s)){\n  cs &lt;- data.frame(st_coordinates(cc)[1], st_coordinates(cc)[2]) %&gt;%\n  rename(p_x = 1,\n         p_y = 2) %&gt;%\n  mutate(seq = 1)\n  cs &lt;- cbind(g[,c(1,4)],cs)\n  \n  ss &lt;- s[row,] %&gt;%\n    rowwise() %&gt;%\n    mutate(p_x = unlist(geometry)[1],\n           p_y = unlist(geometry)[2]) %&gt;%\n    mutate(seq = 2) %&gt;%\n    st_drop_geometry() %&gt;%\n    ungroup()\n  \n  ap &lt;- rbind(ss,cs[row,]) %&gt;%\n    arrange(variable, seq)\n  \n  m &lt;- as.matrix(ap[order(ap$seq),\n                    c(\"p_x\",\"p_y\")])\n  \n  ls &lt;- st_linestring(m) %&gt;%\n    st_sfc() %&gt;%\n    st_sf() %&gt;%\n    mutate(variable = ap$variable[1],\n           pop = ap$pop[1]) %&gt;%\n    bind_cols()\n  \n  lso &lt;- lso %&gt;% bind_rows(ls)\n}\n\nlsp &lt;- lso %&gt;% st_set_crs(6350)\n\n\ns &lt;- s %&gt;% mutate(pop_pct = pop/sum(pop),\n                  log_pct = abs(log(pop_pct)),\n                  normalized_log_pct = 0.1 + (log_pct - max(log_pct)) / (min(log_pct) - max(log_pct)) * (0.7 - 0.1))\n\nplot(lsp[\"variable\"])\n\n\n\nPlotting\nFinally, all we have to do is plot our points! While I don’t have them sized relative to their populations, it would be very easy to do: all I’d have to do is replace the numeric value in tm_symbols() with the column name I wanted to graduate on.\n\nilh &lt;- palette(c(rgb(114,153,67, maxColorValue = 255),\n                 rgb(148,79,161, maxColorValue = 255),\n                 rgb(76,196,144, maxColorValue = 255),\n                 rgb(185,74,115, maxColorValue = 255),\n                 rgb(193,158,60, maxColorValue = 255),\n                 rgb(104,123,210, maxColorValue = 255),\n                 rgb(185,85,61, maxColorValue = 255)))\n\nplot_fips &lt;- unique(fips_codes$state_code)[1:51]\nplot_fips &lt;- plot_fips[!plot_fips %in% c(\"02\", \"15\", \"72\", \"78\")]\n\nus &lt;- states(cb = TRUE, year = yr) %&gt;% filter(if (yr == 2010) STATE %in% plot_fips else\n                                                 STATEFP %in% plot_fips) %&gt;%\n  st_transform(6350) #weird artifact in tigris means that column names don't match\n\nt &lt;- tm_shape(us, bbox = bb(us, ext = 1.1))+\n  tm_polygons(border.col = \"#aaaaaa\", lwd = 0.75, col = \"#5b5b5b\")+\n  tm_shape(lsp)+\n  tm_lines(col = \"variable\", palette = ilh, legend.col.show = FALSE, lwd = 1.5, legend.lwd.show = FALSE)+\n  tm_shape(s)+\n  tm_symbols(col = \"variable\", title.col = \"Race/Ethnicity\", size = \"normalized_log_pct\", border.col = \"#bdbdbd\", palette = ilh, border.lwd = 1, legend.col.show = FALSE, legend.size.show = FALSE)+\n  tm_shape(cc)+\n  tm_dots(col = \"#1286c4\", shape = 24, title = \"Total center of population\", size = 0.6, legend.show = FALSE, border.lwd = 1, border.col = \"#bdbdbd\")+\n  tm_add_legend(type = \"symbol\", \n    labels = c(\"American/Alaskan Native\", \"Asian\", \"African-American\", \"Hawaiian/Pacific Islander\", \"Hispanic\", \"Other/Two or more\", \"White\"),\n    col = ilh,\n    border.col = \"#bdbdbd\",\n    title = \"Ethnicity\",\n    size = 0.4)+\n  tm_add_legend(type = \"symbol\",\n                shape = 24,\n                col = \"#1286c4\",\n                size = 0.6,\n                border.col = \"#bdbdbd\",\n                labels = \"Total center of population\")+\n  tm_layout(main.title = \"Center of population by race/ethnicity\",\n            main.title.fontfamily = \"Manrope\",\n            main.title.fontface = 2,\n            main.title.size = 2,\n            bg.color = \"#3b3b3b\",\n            legend.outside = TRUE,\n            legend.text.color = \"#bdbdbd\", \n            legend.text.fontfamily = \"Manrope\", \n            legend.title.color = \"#bdbdbd\",\n            legend.title.fontface = 2,\n            legend.title.size = 1.5,\n            legend.title.fontfamily = \"Manrope\", \n            legend.text.size = 0.75,\n            legend.position = c(0,0.25),\n            outer.bg.color = \"#3b3b3b\",\n            frame = FALSE,\n            main.title.color = \"#bdbdbd\")+\n  tm_credits(paste0(\"Decennial census data, \", yr, \"\\nGraphic by Harrison DeFord (@oonuttlet)\"),\n             position = c(0,0.08),\n             col = \"#bdbdbd\",\n             fontfamily = \"Open Sans\",\n             size = 0.62)\n\nt\n\nif (!file.exists(paste0(\"../bin/center_of_pop_natl_\",yr,\"_intpt.png\"))){\n  tmap_save(t, paste0(\"../bin/center_of_pop_natl_\",yr,\"_intpt.png\"), dpi = 1200)\n}"
  },
  {
    "objectID": "posts/finalproj/src/flow_diagram.html",
    "href": "posts/finalproj/src/flow_diagram.html",
    "title": "Generating Flow Lines",
    "section": "",
    "text": "This script is meant to be run after QGIS 2.x.x is used to generate paths from the timeseries data generated in the previous script. The older version of QGIS is necessary to use the Points to Paths plugin, which allows for separate lines per vertex (important because we define each 15-minute interval as a distinct trip, even if movement was detected over several consecutive intervals). Within the plugin, bike IDs and row numbers were concatenated using field calculator in order to create a movement ID, which was used to define a trip as a movement of over 50 meters within 15 minutes to account for GPS variability on scooters.\n\nflow_lines &lt;- st_read(\"../results/trip_id_p2p.shp\")\ntrip_id_long &lt;- st_read(\"../results/trip_id_long.gpkg\")\nflow_lines_proj &lt;- flow_lines %&gt;% st_transform(4326)\n\nWe use lwgeom to define the start and endpoints of each of our flow lines, so we have a directionality for our trips\n\nflow_lines$start_geom &lt;- st_startpoint(flow_lines)\nflow_lines$end_geom &lt;- st_endpoint(flow_lines)\nflow_lines_proj$start_geom &lt;- st_startpoint(flow_lines_proj)\nflow_lines_proj$end_geom &lt;- st_endpoint(flow_lines_proj)\n\nSince almost all spatial file formats require only one geometry column, we write to an RDS file to preserve our geometries (for starts and ends, and in the next script for hexagon data as well).\n\nflow_lines &lt;- flow_lines %&gt;% mutate(dist = st_length(geometry))\nsummarise(flow_lines, mean = mean(dist))\nif(!file.exists(\"../results/flow_lines.RDS\")){\n  saveRDS(object = flow_lines, file = \"../results/flow_lines.RDS\")\n}"
  },
  {
    "objectID": "posts/finalproj/src/flow_diagram.html#setup",
    "href": "posts/finalproj/src/flow_diagram.html#setup",
    "title": "Generating Flow Lines",
    "section": "",
    "text": "This script is meant to be run after QGIS 2.x.x is used to generate paths from the timeseries data generated in the previous script. The older version of QGIS is necessary to use the Points to Paths plugin, which allows for separate lines per vertex (important because we define each 15-minute interval as a distinct trip, even if movement was detected over several consecutive intervals). Within the plugin, bike IDs and row numbers were concatenated using field calculator in order to create a movement ID, which was used to define a trip as a movement of over 50 meters within 15 minutes to account for GPS variability on scooters.\n\nflow_lines &lt;- st_read(\"../results/trip_id_p2p.shp\")\ntrip_id_long &lt;- st_read(\"../results/trip_id_long.gpkg\")\nflow_lines_proj &lt;- flow_lines %&gt;% st_transform(4326)\n\nWe use lwgeom to define the start and endpoints of each of our flow lines, so we have a directionality for our trips\n\nflow_lines$start_geom &lt;- st_startpoint(flow_lines)\nflow_lines$end_geom &lt;- st_endpoint(flow_lines)\nflow_lines_proj$start_geom &lt;- st_startpoint(flow_lines_proj)\nflow_lines_proj$end_geom &lt;- st_endpoint(flow_lines_proj)\n\nSince almost all spatial file formats require only one geometry column, we write to an RDS file to preserve our geometries (for starts and ends, and in the next script for hexagon data as well).\n\nflow_lines &lt;- flow_lines %&gt;% mutate(dist = st_length(geometry))\nsummarise(flow_lines, mean = mean(dist))\nif(!file.exists(\"../results/flow_lines.RDS\")){\n  saveRDS(object = flow_lines, file = \"../results/flow_lines.RDS\")\n}"
  },
  {
    "objectID": "posts/finalproj/src/index.html",
    "href": "posts/finalproj/src/index.html",
    "title": "harrison deford",
    "section": "",
    "text": "These three scripts are data preparation and manipulation scripts:\nScript 1\nScript 2\nScript 3\nThis final script produces HTML widgets, which are interactive in 3 dimensions, to smybolize scooter flows. It only uses data collected on May 1, 2022 (due to hardware restraints).\nScript 4\nThis HTML is what I presented in class, and incorporates elements from all of the above scripts.\nIt uses data collected the week of 05/01/2022 through 05/07/2022 to output maps on scooter locations compared to BIPOC population and job and population location."
  },
  {
    "objectID": "posts/finalproj/src/index.html#ges-486-final-project-scripts",
    "href": "posts/finalproj/src/index.html#ges-486-final-project-scripts",
    "title": "harrison deford",
    "section": "",
    "text": "These three scripts are data preparation and manipulation scripts:\nScript 1\nScript 2\nScript 3\nThis final script produces HTML widgets, which are interactive in 3 dimensions, to smybolize scooter flows. It only uses data collected on May 1, 2022 (due to hardware restraints).\nScript 4\nThis HTML is what I presented in class, and incorporates elements from all of the above scripts.\nIt uses data collected the week of 05/01/2022 through 05/07/2022 to output maps on scooter locations compared to BIPOC population and job and population location."
  },
  {
    "objectID": "posts/finalproj/src/scooter_locations.html",
    "href": "posts/finalproj/src/scooter_locations.html",
    "title": "Micromobility in Baltimore",
    "section": "",
    "text": "flow_lines_arc &lt;- st_sf(readRDS(\"../results/flow_lines_arc.RDS\"))\nbalt_hex &lt;- st_sf(readRDS(\"../results/balt_hex.RDS\"))\notm_lehd &lt;- st_read(\"../results/otm_5b749ca22bf64d9abe899ef50a619131/points_2019.shp\") %&gt;% st_transform(3857) %&gt;%\n  select(id, c000, geometry)\nscooters_raw &lt;- st_read(\"../results/scooters_raw.gpkg\") %&gt;% filter(is_disabled == 0)\n\n\nbaltimore_bg_income &lt;- get_acs(geography = \"block group\", \n                           variables = c(\"pop\" = \"B03002_001\", # Total\n                                         \"pop_nhwhite\" = \"B03002_003\", # NH White\n                                         \"pop_nhblack\" = \"B03002_004\", # NH Black\n                                         \"pop_nhamind\" = \"B03002_005\", # NH Am Ind\n                                         \"pop_nhasian\" = \"B03002_006\", # NH Asian\n                                         \"pop_nhhwnpi\" = \"B03002_007\", # NH Hawaiin/PI\n                                         \"pop_nhother\" = \"B03002_008\", # One Other\n                                         \"pop_nhtwomr\" = \"B03002_009\", # Two+\n                                         \"pop_hispltx\" = \"B03002_012\", # Hispanic/Latinx\n                                         \"hu_total\"  = \"B25001_001\", # Housing Units\n                                         \"hu_totocc\" = \"B25003_001\", # Housing Units - Occ\n                                         \"hu_totown\" = \"B25003_002\", # Housing Units - Owner Occ,\n                                         \"hu_totrnt\" = \"B25003_003\", # Housing Units - Renter Occ,\n                                         \"mhhi\" = \"B19013_001\"), #median household income\n                           \n                           year = 2019,\n                           survey = \"acs5\",\n                           state = c(24), \n                           county = c(510), \n                           geometry = TRUE, \n                           output = \"wide\") %&gt;% st_transform(3857)\n\n\n# Computes the NH Asian Population\nbaltimore_bg_income$pop_nhasianXE &lt;- baltimore_bg_income$pop_nhasianE + baltimore_bg_income$pop_nhhwnpiE\n\n# Computes the NH \"Other\" Population\nbaltimore_bg_income$pop_nhotherXE &lt;- baltimore_bg_income$pop_nhamindE + baltimore_bg_income$pop_nhotherE + baltimore_bg_income$pop_nhtwomrE\n\n\nst_erase &lt;- function(x, y) {\n  st_difference(x, st_make_valid(st_union(y)))}\nbmore_water &lt;- area_water(\"MD\", c(510,005), class = \"sf\") %&gt;% st_transform(3857) %&gt;% filter(AWATER &gt; 20000)\nbmore_water &lt;- st_make_valid(st_buffer(bmore_water, 0)) # Fix topology\n\n\nbaltimore_bg_income &lt;- baltimore_bg_income %&gt;% st_transform(3857)\nbaltimore_bg_income &lt;- st_erase(baltimore_bg_income, bmore_water)\nbalt_hex &lt;- balt_hex %&gt;% st_transform(3857)\nbalt_bg_emp &lt;- st_join(otm_lehd, balt_hex)\nbalt_bg_emp &lt;- balt_bg_emp %&gt;% group_by(hex_id) %&gt;% summarise(sum_c000 = sum(c000))\nbalt_hex.intersects &lt;- st_intersects(st_union(baltimore_bg_income), balt_hex)\nbalt_hex.subset &lt;- balt_hex[balt_hex.intersects[[1]],]\nbmore_scooters_in_hex &lt;- st_join(balt_hex.subset, scooters_raw)\nbmore_scooter_hex_count &lt;- count(as_tibble(bmore_scooters_in_hex), hex_id)\nbalt_hex.subset &lt;- left_join(balt_hex.subset, bmore_scooter_hex_count)\n\n\nar_validate(source = baltimore_bg_income, target = balt_hex.subset, varList = \"popE\", method = \"aw\", verbose = TRUE)\n\nbmore_grid_income &lt;- aw_interpolate(balt_hex.subset, tid = hex_id, source = baltimore_bg_income, sid = \"GEOID\", weight = \"sum\", output = \"sf\", extensive = c(\"popE\",\"hu_totalE\", \"pop_nhwhiteE\"))\nbmore_grid_emp_income &lt;- left_join(bmore_grid_income, st_drop_geometry(balt_bg_emp))\nbmore_grid_emp_income[is.na(bmore_grid_emp_income)] &lt;- 0\nbmore_grid_emp_income &lt;- bmore_grid_emp_income %&gt;% rename(scooter_count = n)%&gt;% mutate(people_pt = popE + sum_c000, scooters_per_pt = (1000*scooter_count/18)/people_pt, pct_bipoc = (popE-pop_nhwhiteE)/popE) %&gt;% filter(people_pt &gt; 10)\nbmore_grid_emp_income[is.na(bmore_grid_emp_income)] &lt;- 0\nbmore_grid_emp_income &lt;- bmore_grid_emp_income %&gt;% st_transform(3857)\n\n\nbbox_new &lt;- st_bbox(bmore_grid_emp_income) # current bounding box\n\nxrange &lt;- bbox_new$xmax - bbox_new$xmin # range of x values\nyrange &lt;- bbox_new$ymax - bbox_new$ymin # range of y values\n\nbbox_new &lt;- bbox_new %&gt;%  # take the bounding box ...\n  st_as_sfc() # ... and make it a sf polygon\n\n\nggplot.pts &lt;- ggplot()+\n  geom_sf(balt_hex, fill = NA, mapping = aes())+\n  geom_sf(bmore_grid_emp_income, color = NA, mapping = aes(fill = people_pt))+\n  labs(title = \"Total person-points\", fill = \"Person-points\")+\n  geom_sf(st_union(baltimore_bg_income), fill = NA, color = \"black\", size = 0.8, mapping = aes())+\n  scale_fill_viridis_c(breaks = c(0, 20000, 40000))+\n  coord_sf(xlim = st_coordinates(bbox_new)[c(1,2),1], \n           ylim = st_coordinates(bbox_new)[c(2,3),2]) + \n  theme_void()+\n  theme(plot.title = element_text(hjust = 0.5), panel.border = element_rect(color = \"black\", fill = NA), legend.position = \"bottom\", plot.margin = margin(0,5,0,5))\n\nggplot.pct_bipoc &lt;- ggplot()+\n  geom_sf(balt_hex, fill = NA, mapping = aes())+\n  geom_sf(bmore_grid_emp_income, color = NA, mapping = aes(fill = pct_bipoc))+\n  labs(title = \"Percentage of BIPOC residents\", fill = \"% BIPOC\")+\n  geom_sf(st_union(baltimore_bg_income), fill = NA, color = \"black\", size = 0.8, mapping = aes())+\n  scale_fill_viridis_c()+\n  coord_sf(xlim = st_coordinates(bbox_new)[c(1,2),1], \n           ylim = st_coordinates(bbox_new)[c(2,3),2]) + \n  theme_void()+\n  theme(plot.title = element_text(hjust = 0.5), panel.border = element_rect(color = \"black\", fill = NA), legend.position = \"bottom\", plot.margin = margin(0,5,0,5))\n\nggplot.scooters_pt &lt;- ggplot()+\n  geom_sf(balt_hex, fill = NA, mapping = aes())+\n  geom_sf(bmore_grid_emp_income, color = NA, mapping = aes(fill = scooters_per_pt))+\n  labs(title = \"Scooters per 1000 person-points\", fill = \"Scooters per 1000 \\nperson-points\")+\n  geom_sf(st_union(baltimore_bg_income), fill = NA, color = \"black\", size = 0.8, mapping = aes())+\n  scale_fill_viridis_c()+\n  coord_sf(xlim = st_coordinates(bbox_new)[c(1,2),1], \n           ylim = st_coordinates(bbox_new)[c(2,3),2]) + \n  theme_void()+\n  theme(plot.title = element_text(hjust = 0.5), panel.border = element_rect(color = \"black\", fill = NA), legend.position = \"bottom\", plot.margin = margin(0,5,0,5))\n\n\nlayout1 &lt;- ggplot.scooters_pt|ggplot.pct_bipoc\nlayout1\n\n\nlayout2 &lt;- ggplot.scooters_pt | ggplot.pts\nlayout2"
  }
]