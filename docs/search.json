[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PORTFOLIO",
    "section": "",
    "text": "forklift certified; aspiring geographer\nüìç baltimore"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#umbc",
    "href": "index.html#umbc",
    "title": "PORTFOLIO",
    "section": "UMBC",
    "text": "UMBC\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Great Shrinking Lake\n\n\n\n\n\n\n\n\n\n\n\nMicromobility in Baltimore"
  },
  {
    "objectID": "index.html#mapping-capital",
    "href": "index.html#mapping-capital",
    "title": "PORTFOLIO",
    "section": "Mapping Capital",
    "text": "Mapping Capital"
  },
  {
    "objectID": "posts/381_proj/index.html",
    "href": "posts/381_proj/index.html",
    "title": "The Great Shrinking Lake",
    "section": "",
    "text": "This presentation was given as my final project for my remote sensing course at UMBC. It uses R and Google Earth Engine to analyze the relationship between the changing water area of the Great Salt Lake since 1986 and the growing population of the Wasatch Front and the Utah Valley. You can view the animated timeseries of the lake here.\nThe report was limited in scope due to limited time to work: obviously, there are more factors contributing to a shrinking lake area than just population growth. However, in USGS‚Äôs 2015 Circular 1441 ranked Utah second of all states in per capita domestic water usage, so to say that the population of the Valley is not a major reason the Lake is losing water yearly would be undeniably false.\nLinear correlation tests run in R yielded a significantly negative correlation of y ~ 0.0016x and an R-squared value of 0.78 (p &lt;&lt; 0.001). In context, according to this model, for every 100,000 people that move to the Salt Lake area, 160 km2 of water surface are lost.\nThe R scripts for this report are available here and the Earth Engine code is available here\nNOTE: Google has removed Landsat Collection 1 data from GEE; this script will need to be modified before use (see this GEE article for more details)."
  },
  {
    "objectID": "posts/381_proj/index.html#final-project-for-ges-381-remote-sensing",
    "href": "posts/381_proj/index.html#final-project-for-ges-381-remote-sensing",
    "title": "The Great Shrinking Lake",
    "section": "",
    "text": "This presentation was given as my final project for my remote sensing course at UMBC. It uses R and Google Earth Engine to analyze the relationship between the changing water area of the Great Salt Lake since 1986 and the growing population of the Wasatch Front and the Utah Valley. You can view the animated timeseries of the lake here.\nThe report was limited in scope due to limited time to work: obviously, there are more factors contributing to a shrinking lake area than just population growth. However, in USGS‚Äôs 2015 Circular 1441 ranked Utah second of all states in per capita domestic water usage, so to say that the population of the Valley is not a major reason the Lake is losing water yearly would be undeniably false.\nLinear correlation tests run in R yielded a significantly negative correlation of y ~ 0.0016x and an R-squared value of 0.78 (p &lt;&lt; 0.001). In context, according to this model, for every 100,000 people that move to the Salt Lake area, 160 km2 of water surface are lost.\nThe R scripts for this report are available here and the Earth Engine code is available here\nNOTE: Google has removed Landsat Collection 1 data from GEE; this script will need to be modified before use (see this GEE article for more details)."
  },
  {
    "objectID": "posts/finalproj/scripts.html",
    "href": "posts/finalproj/scripts.html",
    "title": "harrison deford",
    "section": "",
    "text": "These three scripts are data preparation and manipulation scripts:\nScript 1\nScript 2\nScript 3\nThis final script produces HTML widgets, which are interactive in 3 dimensions, to smybolize scooter flows. It only uses data collected on May 1, 2022 (due to hardware restraints).\nScript 4\nThis HTML is what I presented in class, and incorporates elements from all of the above scripts.\nIt uses data collected the week of 05/01/2022 through 05/07/2022 to output maps on scooter locations compared to BIPOC population and job and population location."
  },
  {
    "objectID": "posts/finalproj/scripts.html#ges-486-final-project-scripts",
    "href": "posts/finalproj/scripts.html#ges-486-final-project-scripts",
    "title": "harrison deford",
    "section": "",
    "text": "These three scripts are data preparation and manipulation scripts:\nScript 1\nScript 2\nScript 3\nThis final script produces HTML widgets, which are interactive in 3 dimensions, to smybolize scooter flows. It only uses data collected on May 1, 2022 (due to hardware restraints).\nScript 4\nThis HTML is what I presented in class, and incorporates elements from all of the above scripts.\nIt uses data collected the week of 05/01/2022 through 05/07/2022 to output maps on scooter locations compared to BIPOC population and job and population location."
  },
  {
    "objectID": "posts/finalproj/index.html",
    "href": "posts/finalproj/index.html",
    "title": "Equitable Access to Dockless Vehicles in Baltimore City",
    "section": "",
    "text": "In 2019, Baltimore City officially adopted its Dockless Vehicle Program, granting permits to micromobility companies Link, Lime, and Spin. The goal of this program was to supplement existing public transit networks, to provide a sustainable alternative for small-scale commuting, and, according to the Baltimore DOT, to be ridden just ‚Äúfor fun!‚Äù\nAs the ‚Äòdockless‚Äô name implies, these scooters have nowhere to call home: they‚Äôre placed down by local employees of the scooter vendor, remain out for up to weeks at a time, then recollected for maintenance and recharging before being placed out again. Baltimore City lays out clear Deployment Zones and Deployment Equity Zones, in which a certain amount of scooters must remain for the vendors to continue operation in the city.\nThese zones are relatively small compared to the size of the city boundary: do they make scooter distribution truly equitable? Using the vendors‚Äô public API endpoints (another requirement for operation within the city), data was (and is being) collected on scooter locations every fifteen minutes. For this project, I only analyzed times between 6:00 am and 10:00 am for the week of May 1, 2022 and May 7, 2022; but the potential is there for much more detailed analysis.\nThe python script which queries the API endpoints for all three vendors is currently running on Mapping Capital, and all other analysis was done using R statistical software. The scripts I used to manipulate the data collected can be found here.\n\n\n\nFor this analysis, I‚Äôve coined a new unit: people-points. Represented by the total number of jobs as represented in LEHD data added with the total population in a given area, people-points can be used to find centers of transport and human activity as people commute to and from work and home. I‚Äôll be symbolizing my maps based on the number of scooters per person-point (in this case, per 1,000 people-points for better scaling) in order to pick out locations where the number of scooters is not proportional to the number of jobs and residents in an area.\nHere are the resulting maps, made in ggplot2 and arranged using patchwork:\n \nWe can see that, during a week of morning commuting (Monday - Monday), the areas with the most scooter trips per person-point are in Locust Point and the Inner Harbor. More specifically, the two yellowest polygons, which represent the hexes with the highest proportion of trips per person-point, contain the Under Armour main campus. Notably, areas with high percentages of BIPOC individuals are almost all dark purple, showing few numbers of scooter trips per person-point.\nAccording to Spin, they are ‚Äúcommitted to being the best possible partner for cities while building the safest, most equitable, and most sustainable mobility solution for the communities [they] serve‚Äù (emphasis added). I‚Äôd say that, whether intentional or not, the stark contrast of micromobility in marginalized communities compared to whiter communities demonstrates a veritable lack of equitable access, at least in Baltimore. Whether the vehicles are purposely dropped in whiter areas after charging, if they‚Äôre used to commute from these areas to Downtown but not back, or any other reason, calling the distribution of scooter trips and availability ‚Äúequitable‚Äù is downright laughable as things stand at the moment.\n\n\n\nAn important reason that cities (including Baltimore) adopt these platforms is to supplement existing transit networks by providing a method for an individual to quickly get to a transit stop before switching to the bus, train, or other method of transit. One interesting area of research in this regard could be to examine how many scooter trips have start and end points that mirror existing transit routes. Could people be using scooters instead of public transit? If so, why? And, as always, more research could be carried out with more data. Since the script collecting our data runs every 15 minutes until the server runs out of storage or the inevitable heat death of the universe (whichever comes first), we could theoretically run this same analysis for time periods of months or even years, given powerful enough hardware."
  },
  {
    "objectID": "posts/finalproj/index.html#introduction",
    "href": "posts/finalproj/index.html#introduction",
    "title": "Equitable Access to Dockless Vehicles in Baltimore City",
    "section": "",
    "text": "In 2019, Baltimore City officially adopted its Dockless Vehicle Program, granting permits to micromobility companies Link, Lime, and Spin. The goal of this program was to supplement existing public transit networks, to provide a sustainable alternative for small-scale commuting, and, according to the Baltimore DOT, to be ridden just ‚Äúfor fun!‚Äù\nAs the ‚Äòdockless‚Äô name implies, these scooters have nowhere to call home: they‚Äôre placed down by local employees of the scooter vendor, remain out for up to weeks at a time, then recollected for maintenance and recharging before being placed out again. Baltimore City lays out clear Deployment Zones and Deployment Equity Zones, in which a certain amount of scooters must remain for the vendors to continue operation in the city.\nThese zones are relatively small compared to the size of the city boundary: do they make scooter distribution truly equitable? Using the vendors‚Äô public API endpoints (another requirement for operation within the city), data was (and is being) collected on scooter locations every fifteen minutes. For this project, I only analyzed times between 6:00 am and 10:00 am for the week of May 1, 2022 and May 7, 2022; but the potential is there for much more detailed analysis.\nThe python script which queries the API endpoints for all three vendors is currently running on Mapping Capital, and all other analysis was done using R statistical software. The scripts I used to manipulate the data collected can be found here."
  },
  {
    "objectID": "posts/finalproj/index.html#results",
    "href": "posts/finalproj/index.html#results",
    "title": "Equitable Access to Dockless Vehicles in Baltimore City",
    "section": "",
    "text": "For this analysis, I‚Äôve coined a new unit: people-points. Represented by the total number of jobs as represented in LEHD data added with the total population in a given area, people-points can be used to find centers of transport and human activity as people commute to and from work and home. I‚Äôll be symbolizing my maps based on the number of scooters per person-point (in this case, per 1,000 people-points for better scaling) in order to pick out locations where the number of scooters is not proportional to the number of jobs and residents in an area.\nHere are the resulting maps, made in ggplot2 and arranged using patchwork:\n \nWe can see that, during a week of morning commuting (Monday - Monday), the areas with the most scooter trips per person-point are in Locust Point and the Inner Harbor. More specifically, the two yellowest polygons, which represent the hexes with the highest proportion of trips per person-point, contain the Under Armour main campus. Notably, areas with high percentages of BIPOC individuals are almost all dark purple, showing few numbers of scooter trips per person-point.\nAccording to Spin, they are ‚Äúcommitted to being the best possible partner for cities while building the safest, most equitable, and most sustainable mobility solution for the communities [they] serve‚Äù (emphasis added). I‚Äôd say that, whether intentional or not, the stark contrast of micromobility in marginalized communities compared to whiter communities demonstrates a veritable lack of equitable access, at least in Baltimore. Whether the vehicles are purposely dropped in whiter areas after charging, if they‚Äôre used to commute from these areas to Downtown but not back, or any other reason, calling the distribution of scooter trips and availability ‚Äúequitable‚Äù is downright laughable as things stand at the moment."
  },
  {
    "objectID": "posts/finalproj/index.html#further-research",
    "href": "posts/finalproj/index.html#further-research",
    "title": "Equitable Access to Dockless Vehicles in Baltimore City",
    "section": "",
    "text": "An important reason that cities (including Baltimore) adopt these platforms is to supplement existing transit networks by providing a method for an individual to quickly get to a transit stop before switching to the bus, train, or other method of transit. One interesting area of research in this regard could be to examine how many scooter trips have start and end points that mirror existing transit routes. Could people be using scooters instead of public transit? If so, why? And, as always, more research could be carried out with more data. Since the script collecting our data runs every 15 minutes until the server runs out of storage or the inevitable heat death of the universe (whichever comes first), we could theoretically run this same analysis for time periods of months or even years, given powerful enough hardware."
  },
  {
    "objectID": "posts/381_proj/src/index.html",
    "href": "posts/381_proj/src/index.html",
    "title": "harrison deford",
    "section": "",
    "text": "Download the R script used to calculate yearly population estimates.\nDownload the R script used to visualize the area and population estimates and generate the linear regression."
  },
  {
    "objectID": "posts/finalproj/scripts/index.html",
    "href": "posts/finalproj/scripts/index.html",
    "title": "harrison deford",
    "section": "",
    "text": "These three scripts are data preparation and manipulation scripts:\nScript 1\nScript 2\nScript 3\nThis final script produces HTML widgets, which are interactive in 3 dimensions, to smybolize scooter flows. It only uses data collected on May 1, 2022 (due to hardware restraints).\nScript 4\nThis HTML is what I presented in class, and incorporates elements from all of the above scripts.\nIt uses data collected the week of 05/01/2022 through 05/07/2022 to output maps on scooter locations compared to BIPOC population and job and population location."
  },
  {
    "objectID": "posts/finalproj/scripts/index.html#ges-486-final-project-scripts",
    "href": "posts/finalproj/scripts/index.html#ges-486-final-project-scripts",
    "title": "harrison deford",
    "section": "",
    "text": "These three scripts are data preparation and manipulation scripts:\nScript 1\nScript 2\nScript 3\nThis final script produces HTML widgets, which are interactive in 3 dimensions, to smybolize scooter flows. It only uses data collected on May 1, 2022 (due to hardware restraints).\nScript 4\nThis HTML is what I presented in class, and incorporates elements from all of the above scripts.\nIt uses data collected the week of 05/01/2022 through 05/07/2022 to output maps on scooter locations compared to BIPOC population and job and population location."
  },
  {
    "objectID": "posts/finalproj/src/start_end_join.html",
    "href": "posts/finalproj/src/start_end_join.html",
    "title": "Data Gathering and Wrangling",
    "section": "",
    "text": "The purpose of this script is to define functions which convert data collected by the python script running on mapping.capital, which are returned as nested JSON files, to geoJSON which can be used for analysis. This file is set up for a dataset of all scooter locations, collected every 15 minutes, from 0600 Eastern to 1000 Eastern on May 1, 2022. The second function defined in this document creates a long dataframe which is a timeseries of scooter locations over time.\n\nvendors = list(\"link\",\"lime\",\"spin\")\n\nThis code block defines a method to convert the nested JSON returned by scooter vendors to a non-nested geoJSON which can be read by sf.\n\njson2geoJSON &lt;- function(vendor){\n  files &lt;- list.files(path = paste(\"../data/monday/\", vendor, \"/morning/json/\", sep = \"\"), pattern=\"*.json\", full.names=TRUE, recursive=FALSE) #list files in directory\n    lapply(files, function(x) {\n      current_data &lt;- fromJSON(txt = x) # load file\n      current_tibble &lt;- as_tibble(current_data$data$bikes) #convert to tibble\n      current_tibble$timestamp_utc &lt;- as_datetime(current_data$last_updated, tz = Sys.timezone()) #create timestamp column\n      current_sf &lt;- st_as_sf(current_tibble, coords = c(\"lon\",\"lat\"), crs = 4326) #coerce to sf\n      if (!file.exists(paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\",\n                                       current_data$last_updated, \"_\", vendor, \".geoJSON\"))){\n      st_write(current_sf, dsn = paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\",\n                                       current_data$last_updated, \"_\", vendor, \".geoJSON\", sep = \"\")\n               , append = FALSE) #write as geoJSON\n      }\n})\n}\n\n\nfor (v in vendors){\n  json2geoJSON(v)\n} #loop through each of link, lime, spin\n\nThis section of code defines a function which creates a timeseries for each scooter and adds a vendor column which can be grouped by in following scripts.\nNote 2024-10-11: I cannot emphasize enough how much you SHOULD NOT USE GLOBAL ASSIGNMENT (&lt;&lt;-) in a function. This was some of the first R code I‚Äôd ever written, and I‚Äôm leaving it for posterity. However, there are MUCH better ways to do this.\n\nload_timeseries &lt;- function(vendor){\n  files &lt;- list.files(path = paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\", sep = \"\"), pattern=\"*.geoJSON\", full.names=TRUE, recursive=FALSE) #load files from geoJSON directory\n  list_df &lt;&lt;- vector(mode = \"list\") #empty list\n  for(fn in files){\n    tmp &lt;- st_read(fn) #read each file in geoJSON dir\n    list_df[[which(fn == files)]] &lt;&lt;- tmp #append to list_df\n  }\n  test_sf &lt;&lt;- bind_rows(list_df) #make long df\n  test_sf$vendor &lt;&lt;- vendor #create vendor column\n  test_sf &lt;&lt;- distinct(test_sf) #script adds multiples, need to debug. hacky solution here\n}\n\n\nload_timeseries(\"link\")\nlink_data &lt;- test_sf\nif (!file.exists(\"../results/link_mon_am.gpkg\")){\n  st_write(link_data, dsn = paste0(\"../results/link_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}\n\n\nload_timeseries(\"lime\")\nlime_data &lt;- test_sf\nif (!file.exists(\"../results/lime_mon_am.gpkg\")){\n  st_write(lime_data, dsn = paste0(\"../results/lime_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}\n\n\nload_timeseries(\"spin\")\nspin_data &lt;- test_sf\nif (!file.exists(\"../results/spin_mon_am.gpkg\")){\n  st_write(spin_data, dsn = paste0(\"../results/spin_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}"
  },
  {
    "objectID": "posts/finalproj/src/start_end_join.html#setup",
    "href": "posts/finalproj/src/start_end_join.html#setup",
    "title": "Data Gathering and Wrangling",
    "section": "",
    "text": "The purpose of this script is to define functions which convert data collected by the python script running on mapping.capital, which are returned as nested JSON files, to geoJSON which can be used for analysis. This file is set up for a dataset of all scooter locations, collected every 15 minutes, from 0600 Eastern to 1000 Eastern on May 1, 2022. The second function defined in this document creates a long dataframe which is a timeseries of scooter locations over time.\n\nvendors = list(\"link\",\"lime\",\"spin\")\n\nThis code block defines a method to convert the nested JSON returned by scooter vendors to a non-nested geoJSON which can be read by sf.\n\njson2geoJSON &lt;- function(vendor){\n  files &lt;- list.files(path = paste(\"../data/monday/\", vendor, \"/morning/json/\", sep = \"\"), pattern=\"*.json\", full.names=TRUE, recursive=FALSE) #list files in directory\n    lapply(files, function(x) {\n      current_data &lt;- fromJSON(txt = x) # load file\n      current_tibble &lt;- as_tibble(current_data$data$bikes) #convert to tibble\n      current_tibble$timestamp_utc &lt;- as_datetime(current_data$last_updated, tz = Sys.timezone()) #create timestamp column\n      current_sf &lt;- st_as_sf(current_tibble, coords = c(\"lon\",\"lat\"), crs = 4326) #coerce to sf\n      if (!file.exists(paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\",\n                                       current_data$last_updated, \"_\", vendor, \".geoJSON\"))){\n      st_write(current_sf, dsn = paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\",\n                                       current_data$last_updated, \"_\", vendor, \".geoJSON\", sep = \"\")\n               , append = FALSE) #write as geoJSON\n      }\n})\n}\n\n\nfor (v in vendors){\n  json2geoJSON(v)\n} #loop through each of link, lime, spin\n\nThis section of code defines a function which creates a timeseries for each scooter and adds a vendor column which can be grouped by in following scripts.\nNote 2024-10-11: I cannot emphasize enough how much you SHOULD NOT USE GLOBAL ASSIGNMENT (&lt;&lt;-) in a function. This was some of the first R code I‚Äôd ever written, and I‚Äôm leaving it for posterity. However, there are MUCH better ways to do this.\n\nload_timeseries &lt;- function(vendor){\n  files &lt;- list.files(path = paste(\"../data/monday/\", vendor, \"/morning/geoJSON/\", sep = \"\"), pattern=\"*.geoJSON\", full.names=TRUE, recursive=FALSE) #load files from geoJSON directory\n  list_df &lt;&lt;- vector(mode = \"list\") #empty list\n  for(fn in files){\n    tmp &lt;- st_read(fn) #read each file in geoJSON dir\n    list_df[[which(fn == files)]] &lt;&lt;- tmp #append to list_df\n  }\n  test_sf &lt;&lt;- bind_rows(list_df) #make long df\n  test_sf$vendor &lt;&lt;- vendor #create vendor column\n  test_sf &lt;&lt;- distinct(test_sf) #script adds multiples, need to debug. hacky solution here\n}\n\n\nload_timeseries(\"link\")\nlink_data &lt;- test_sf\nif (!file.exists(\"../results/link_mon_am.gpkg\")){\n  st_write(link_data, dsn = paste0(\"../results/link_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}\n\n\nload_timeseries(\"lime\")\nlime_data &lt;- test_sf\nif (!file.exists(\"../results/lime_mon_am.gpkg\")){\n  st_write(lime_data, dsn = paste0(\"../results/lime_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}\n\n\nload_timeseries(\"spin\")\nspin_data &lt;- test_sf\nif (!file.exists(\"../results/spin_mon_am.gpkg\")){\n  st_write(spin_data, dsn = paste0(\"../results/spin_mon_am.gpkg\", sep = \"\"), append = FALSE)\n}"
  },
  {
    "objectID": "posts/finalproj/src/index.html",
    "href": "posts/finalproj/src/index.html",
    "title": "harrison deford",
    "section": "",
    "text": "These three scripts are data preparation and manipulation scripts:\nScript 1\nScript 2\nScript 3\nThis final script produces HTML widgets, which are interactive in 3 dimensions, to smybolize scooter flows. It only uses data collected on May 1, 2022 (due to hardware restraints).\nScript 4\nThis HTML is what I presented in class, and incorporates elements from all of the above scripts.\nIt uses data collected the week of 05/01/2022 through 05/07/2022 to output maps on scooter locations compared to BIPOC population and job and population location."
  },
  {
    "objectID": "posts/finalproj/src/index.html#ges-486-final-project-scripts",
    "href": "posts/finalproj/src/index.html#ges-486-final-project-scripts",
    "title": "harrison deford",
    "section": "",
    "text": "These three scripts are data preparation and manipulation scripts:\nScript 1\nScript 2\nScript 3\nThis final script produces HTML widgets, which are interactive in 3 dimensions, to smybolize scooter flows. It only uses data collected on May 1, 2022 (due to hardware restraints).\nScript 4\nThis HTML is what I presented in class, and incorporates elements from all of the above scripts.\nIt uses data collected the week of 05/01/2022 through 05/07/2022 to output maps on scooter locations compared to BIPOC population and job and population location."
  }
]